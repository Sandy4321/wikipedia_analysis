The [[existential risk of artificial general intelligence]] is the potential that unbridled progress in computer technology may lead to the [[technological singularity]], resulting in the development of [[synthetic intelligence]] that may bring about the end of human civilization or even [[human extinction|the extinction of the human race]].

The slow progress of biological evolution has given way to the rapid progress of [[technological revolution]] and [[accelerating change]], bringing us the [[emerging technology]] of [[artificial intelligence]], which may lead to the development of [[artificial general intelligence]] (strong AI).  Strong AI would be capable of [[recursive self-improvement]], and may result in an [[intelligence explosion]] and the emergence of [[superintelligence]]. Strong AI could also be self-replicating, producing many more of its own kind (computers and robots). Either of these outcomes may pose a [[global catastrophic risk]]. 

In his paper ''Ethical Issues in Advanced Artificial Intelligence'', the Oxford philosopher [[Nick Bostrom]] argues that artificial intelligence has the capability to bring about human extinction. Since artificial intellects need not share our human motivational tendencies, it would be up to the designers of an emergent superintelligence to specify its original motivations.  In theory, a superintelligent AI  would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its top goal, many uncontrolled unintended consequences could arise. It could kill off all other agents, persuade them to change their behavior, or block their attempts at interference.<ref name="Bostrom, Nick 2003">Bostrom, Nick. 2003. "Ethical Issues in Advanced Artificial Intelligence." In Cognitive, Emotive and Ethical Aspects of Decision Making in Humans and in Artificial Intelligence, edited by Iva Smit and George E. Lasker, 12<U+2013>17. Vol. 2. Windsor, ON: International Institute for Advanced Studies in Systems Research / Cybernetics.</ref>

[[Eliezer Yudkowsky]] put it this way:

<blockquote>"The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else." <ref>[[Eliezer Yudkowsky]] (2008) in ''[http://intelligence.org/files/AIPosNegFactor.pdf Artificial Intelligence as a Positive and Negative Factor in Global Risk]''</ref></blockquote>

==Dangers==
In January 2015, [[Nick Bostrom]] joined [[Stephen Hawking]], [[Max Tegmark]], [[Elon Musk]], Lord [[Martin Rees, Baron Rees of Ludlow|Martin Rees]], [[Jaan Tallinn]] among others, in signing the [[Future of Life Institute]]'s open letter warning of the potential dangers associated with [[artificial intelligence]]. The signatories "...believe that research on how to make AI systems robust and beneficial is both important and timely, and that there are concrete research directions that can be pursued today."<ref>{{cite web|url=http://futureoflife.org/misc/open_letter|title=The Future of Life Institute Open Letter|publisher= The Future of Life Institute |accessdate=4 March 2015}}</ref><ref>{{cite web|url=http://www.ft.com/cms/s/0/3d2c2f12-99e9-11e4-93c1-00144feabdc0.html#axzz3TNL9lxJV|title=Scientists and investors warn on AI|publisher= The Financial Times|accessdate=4 March 2015}}</ref>

[[Vernor Vinge]] has suggested that a moment may come when computers and robots are smarter than humans. He calls this "[[Technological singularity|the Singularity]]."<ref name="nytimes july09"/> He suggests that it may be somewhat or possibly very dangerous for humans.<ref>[http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html The Coming Technological Singularity: How to Survive in the Post-Human Era], by Vernor Vinge, Department of Mathematical Sciences, San Diego State University, (c) 1993 by Vernor Vinge.</ref> This is discussed by a philosophy called [[transhumanism]].

[[Future studies|Futurist]] and [[computer scientist]] [[Raymond Kurzweil]] has noted that "There are physical limits to computation, but they're not very limiting." If the current trend in computer computation improvement continues, and existing problems in creating artificial intelligence are overcome, sentient machines are likely to immediately hold an enormous advantage in at least some forms of mental capability, including the capacity of [[perfect recall]], a vastly superior knowledge base, and the ability to [[Human multitasking|multitask]] in ways not possible to biological entities. This may give them the opportunity to<U+2014> either as a single being or as a new [[species]] <U+2014> become much more powerful than humans, and to displace them.<ref>{{cite book |last=Warwick |first=Kevin |authorlink=Kevin Warwick |title=March of the Machines: The Breakthrough in Artificial Intelligence |publisher=University of Illinois Press |year=2004 |isbn=0-252-07223-5}}</ref>

[[Stephen Hawking]] said in 2014 that "Success in creating AI would be the biggest event in human history. Unfortunately, it might also be the last, unless we learn how to avoid the risks." Hawking believes that in the coming decades, AI could offer "incalculable benefits and risks" such as "technology outsmarting financial markets, out-inventing human researchers, out-manipulating human leaders, and developing weapons we cannot even understand." Hawking believes more should be done to prepare for the singularity:<ref>{{cite web |url=http://www.independent.co.uk/news/science/stephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence--but-are-we-taking-ai-seriously-enough-9313474.html |title=Stephen Hawking: 'Transcendence looks at the implications of artificial intelligence - but are we taking AI seriously enough?'  |work=[[The Independent]] |author=[[Stephen Hawking]] |date=1 May 2014 |accessdate=May 5, 2014}}</ref>{{quote|So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, "We'll arrive in a few decades," would we just reply, "OK, call us when you get here <U+2013> we'll leave the lights on"? Probably not <U+2013> but this is more or less what is happening with AI.}}

Physicist [[Stephen Hawking]], [[Microsoft]] founder [[Bill Gates]] and [[SpaceX]] founder [[Elon Musk]] have expressed concerns about the possibility that AI could evolve to the point that humans could not control it, with Hawking theorizing that this could "spell the end of the human race".<ref>{{cite web|last1=Rawlinson|first1=Kevin|title=Microsoft's Bill Gates insists AI is a threat|url=http://www.bbc.co.uk/news/31047780|publisher=[[BBC News]]|accessdate=30 January 2015}}</ref> In 2009, experts attended a conference hosted by the [[Association for the Advancement of Artificial Intelligence]] (AAAI) to discuss whether computers and robots might be able to acquire any sort of [[autonomy]], and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved "cockroach intelligence." They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.<ref name="nytimes july09">[http://www.nytimes.com/2009/07/26/science/26robot.html?_r=1&ref=todayspaper Scientists Worry Machines May Outsmart Man] By JOHN MARKOFF, NY Times, July 26, 2009.</ref>

Various media sources and scientific groups have noted separate trends in differing areas which might together result in greater robotic functionalities and autonomy, and which pose some inherent concerns.<ref>[http://www.slate.com/id/2218834/ Gaming the Robot Revolution: A military technology expert weighs in on Terminator: Salvation]., By P. W. Singer, slate.com Thursday, May 21, 2009.</ref><ref>[http://www.gyre.org/news/explore/robot-takeover Robot takeover], gyre.org.</ref><ref>[http://www.engadget.com/tag/robotapocalypse robot page], engadget.com.</ref> [[Eliezer Yudkowsky]] believes that risks from artificial intelligence are harder to predict than any other known risks. He also argues that research into artificial intelligence is biased by [[anthropomorphism]]. Since people base their judgments of artificial intelligence on their own experience, he claims that they underestimate the potential power of AI. He distinguishes between risks due to technical failure of AI, which means that flawed algorithms prevent the AI from carrying out its intended goals, and philosophical failure, which means that the AI is programmed to realize a flawed ideology.<ref name="Yudkowsky">{{cite web|last=Yudkowsky|first=Eliezer|title=Artificial Intelligence as a Positive and Negative Factor in Global Risk|url=http://yudkowsky.net/singularity/ai-risk|accessdate=26 July 2013}}</ref>

The creation of artificial general intelligence may have repercussions so great and so complex that it may not be possible to forecast what will come afterwards.  Thus the event in the hypothetical future of achieving strong AI is called the [[technological singularity]], because theoretically you cannot see past it.  But this has not stopped philosophers and researchers from guessing what the smart computers or robots of the future might do, including forming a utopia by [[friendly artificial intelligence|being our friends]] or overwhelming us in an [[AI takeover]]. The latter potentiality is particularly disturbing as it poses an existential risk for mankind, that is, it may lead to [[human extinction]].

=== Unfriendly AI ===

Strong AI may be inherently dangerous. Hyper-intelligent software may not necessarily decide to support the continued existence of mankind, and may be extremely difficult to stop.<ref>{{Harvtxt|Yudkowsky, Eliezer|2008}}</ref> This topic has also recently begun to be discussed in academic publications as a real source of [[risks to civilization, humans, and planet Earth]].

{{Harvtxt|Berglas|2008}} notes that there is no direct evolutionary motivation for an AI to be friendly to humans. Evolution has no inherent tendency to produce outcomes valued by humans, and there is little reason to expect an arbitrary optimisation process to promote an outcome desired by mankind, rather than inadvertently leading to an AI behaving in a way not intended by its creators (such as Nick Bostrom's whimsical example of an AI which was originally programmed with the goal of manufacturing paper clips, so that when it achieves superintelligence it decides to convert the entire planet into a paper clip manufacturing facility).<ref name="nickbostrom8">[http://www.nickbostrom.com/ethics/ai.html Ethical Issues in Advanced Artificial Intelligence, Nick Bostrom, in Cognitive, Emotive and Ethical Aspects of Decision Making in Humans and in Artificial Intelligence, Vol. 2, ed. I. Smit et al., Int. Institute of Advanced Studies in Systems Research and Cybernetics, 2003, pp. 12<U+2013>17]</ref><ref name="singinst">[[Eliezer Yudkowsky]]: [http://singinst.org/upload/artificial-intelligence-risk.pdf Artificial Intelligence as a Positive and Negative Factor in Global Risk]. Draft for a publication in ''Global Catastrophic Risk'' from August 31, 2006, retrieved July 18, 2011 (PDF file)</ref><ref name="singinst9">[http://www.singinst.org/blog/2007/06/11/the-stamp-collecting-device/ The Stamp Collecting Device, Nick Hay]</ref> [[Anders Sandberg]] has also elaborated on this scenario, addressing various common counter-arguments.<ref name="aleph">[http://www.aleph.se/andart/archives/2011/02/why_we_should_fear_the_paperclipper.html 'Why we should fear the Paperclipper'], 2011-02-14 entry of Sandberg's blog 'Andart'</ref>

AI researcher [[Hugo de Garis]] suggests that artificial intelligences may simply eliminate the human race for access to scarce resources,<ref name="selfawaresystems.com">[http://selfawaresystems.com/2007/10/05/paper-on-the-nature-of-self-improving-artificial-intelligence/ Omohundro, Stephen M., "The Nature of Self-Improving Artificial Intelligence." Self-Aware Systems. 21 Jan. 2008. Web. 07 Jan. 2010.]</ref><ref name="selfawaresystems10">[http://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/ Omohundro, Stephen M., "The Basic AI Drives." Artificial General Intelligence, 2008 proceedings of the First AGI Conference, eds. Pei Wang, Ben Goertzel, and Stan Franklin. Vol. 171. Amsterdam: IOS, 2008. ]</ref> and humans would be powerless to stop them.<ref name="forbes">de Garis, Hugo. [http://www.forbes.com/2009/06/18/cosmist-terran-cyborgist-opinions-contributors-artificial-intelligence-09-hugo-de-garis.html "The Coming Artilect War"], Forbes.com, 22 June 2009.</ref> Alternatively, AIs developed under evolutionary pressure to promote their own survival could outcompete humanity.<ref name="nickbostrom7">[http://www.nickbostrom.com/fut/evolution.html Bostrom, Nick, The Future of Human Evolution, Death and Anti-Death: Two Hundred Years After Kant, Fifty Years After Turing, ed. Charles Tandy, pp. 339<U+2013>371, 2004, Ria University Press.]</ref>

One proposal to deal with this likelihood is to make sure that the first generally intelligent AI is [[friendly AI]], that would then endeavor to ensure that subsequently developed AIs were also nice to us. But, friendly AI is harder to create than plain AGI, and therefore it is likely, in a race between the two, that non-friendly AI would be developed first.  Also, there is no guarantee that friendly AI would remain friendly, or that its progeny would also all be good.<ref>{{Harvtxt|Berglas|2008}}</ref>

It is a significant problem that unfriendly artificial intelligence is likely to be much easier to create than friendly AI. While both require large advances in recursive optimisation process design, friendly AI also requires goal structures that will remain unchanged by self-improvement (or the AI could transform itself into something unfriendly). These goals would need to align with human values so that AI does not automatically destroy the human race. An unfriendly AI, on the other hand, can optimize for an arbitrary goal structure, which does not need to be invariant under self-modification.<ref name="singinst12">[http://singinst.org/upload/CEV.html Coherent Extrapolated Volition, Eliezer S. Yudkowsky, May 2004 ]</ref>

The sheer complexity of human value systems makes it very difficult to make AI's motivations human-friendly.<ref name="Muehlhauser, Luke 2012">Muehlhauser, Luke, and Louie Helm. 2012. "Intelligence Explosion and Machine Ethics." In Singularity Hypotheses: A Scientific and Philosophical Assessment, edited by Amnon Eden, Johnny S<U+00F8>raker, James H. Moor, and Eric Steinhart. Berlin: Springer.</ref><ref name="Bostrom, Nick 2003">Bostrom, Nick. 2003. "Ethical Issues in Advanced Artificial Intelligence." In Cognitive, Emotive and Ethical Aspects of Decision Making in Humans and in Artificial Intelligence, edited by Iva Smit and George E. Lasker, 12<U+2013>17. Vol. 2. Windsor, ON: International Institute for Advanced Studies in Systems Research / Cybernetics.</ref> Unless moral philosophy provides us with a flawless ethical theory, an AI's utility function could allow for many potentially harmful scenarios that conform with a given ethical framework but not "common sense". According to [[Eliezer Yudkowsky]], there is little reason to suppose that an artificially designed mind would have such an adaptation.<ref>Yudkowsky, Eliezer. 2011. "Complex Value Systems in Friendly AI." In Schmidhuber, Th<U+00F3>risson, and Looks 2011, 388<U+2013>393.</ref>

[[Eliezer Yudkowsky]] proposed that research be undertaken to produce [[friendly artificial intelligence]] in order to address the dangers. He noted that the first real AI would have a head start on self-improvement and, if friendly, could prevent unfriendly AIs from developing, as well as providing enormous benefits to mankind.<ref name="ReferenceB">{{cite web|url=http://singinst.org/riskintro/index.html |title=Concise Summary &#124; Singularity Institute for Artificial Intelligence |publisher=Singinst.org |accessdate=2011-09-09}}</ref>

=== Self-replicating machines ===
Smart computers or robots would be able to produce copies of themselves.  They would be [[self-replicating machine]]s.  A growing population of intelligent robots could conceivably outcompete inferior humans in job markets, in business, in science, in politics (pursuing [[robot rights]]), and technologically, sociologically ([[Group mind (science fiction)|by acting as one]]), and militarily.  ''See also [[swarm intelligence]].''

They would also be able to be mass produced, in automated factories. While it takes many years for a human to acquire an adult-level education and skill set, all it takes for a new computer or robot to acquire fully developed software is [[installation (computer programs)|installation]] or [[file copying]]. So, once AI was developed, AI computers and robots would be fully capable upon coming off the production line, whether they were factory worker robots, [[driverless car]]s, or [[military robots|soldier bots]].

=== Emergent superintelligence ===
{{Main|Superintelligence}}

A superintelligence, hyperintelligence, or superhuman intelligence is a hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds. <U+2018><U+2018>Superintelligence<U+2019><U+2019> may also refer to the form or degree of intelligence possessed by such an agent.

Philosopher [[David Chalmers]] argues that generally intelligent AI &mdash; [[artificial general intelligence]] &mdash; is a very likely path to superhuman intelligence. Chalmers breaks this claim down into an argument that AI can achieve ''equivalence'' to human intelligence, that it can be ''extended'' to surpass human intelligence, and that it be further ''amplified'' to completely dominate humans across arbitrary tasks.{{sfn|Chalmers|2010|p=7}}

If research into strong AI produced sufficiently intelligent software, it would be able to reprogram and improve itself &ndash; a feature called "[[recursive self-improvement]]". It would then be even better at improving itself, and would probably continue doing so in a rapidly increasing cycle, leading to an [[intelligence explosion]] and the emergence of [[superintelligence]].  Such an intelligence would not have the limitations of human intellect, and may be able to invent or discover almost anything.

Many researchers have argued that, by way of an "intelligence explosion" sometime over the next century, a self-improving AI could become so vastly more powerful than humans that we would not be able to stop it from achieving its goals.<ref name="Muehlhauser, Luke 2012">Muehlhauser, Luke, and Louie Helm. 2012. "Intelligence Explosion and Machine Ethics." In Singularity Hypotheses: A Scientific and Philosophical Assessment, edited by Amnon Eden, Johnny S<U+00F8>raker, James H. Moor, and Eric Steinhart. Berlin: Springer.</ref>

It has been suggested that learning computers that rapidly become superintelligent may take unforeseen actions or that [[Robot#Social impact|robots]] would out-compete humanity (one [[technological singularity]] scenario).<ref name="billjoy">[[Bill Joy]], [http://www.wired.com/wired/archive/8.04/joy_pr.html Why the future doesn't need us]. In:[[Wired magazine]]. See also [[technological singularity]].[[Nick Bostrom]] 2002 Ethical Issues in Advanced Artificial Intelligence http://www.nickbostrom.com</ref>

Because of its exceptional scheduling and organizational capability and the range of novel technologies it could develop, it is possible that the first Earth superintelligence to emerge could rapidly become matchless and unrivaled: conceivably it would be able to bring about almost any possible outcome, and be able to foil virtually any attempt that threatened to prevent it achieving its objectives.<ref name="Bostrom">[[Nick Bostrom]] 2002 Ethical Issues in Advanced Artificial Intelligence http://www.nickbostrom.com</ref> It could eliminate, wiping out if it chose, any other challenging rival intellects; alternatively it might manipulate or persuade them to change their behavior towards its own interests, or it may merely obstruct their attempts at interference.<ref name="Bostrom" /> In Bostrom's book, ''[[Superintelligence: Paths, Dangers, Strategies]]'', he defines this as the control problem.<ref name="Bostrom2">''Superintelligence: Paths, Dangers, Strategies''</ref>

{{Harvtxt|Bostrom|2002}} discusses how this might come about:

{{quote|When we create the first superintelligent entity, we might make a mistake and give it goals that lead it to annihilate humankind, assuming its enormous intellectual advantage gives it the power to do so. For example, we could mistakenly elevate a subgoal to the status of a supergoal. We tell it to solve a mathematical problem, and it complies by turning all the matter in the solar system into a giant calculating device, in the process killing the person who asked the question.}}

== Prevention ==
[[Oxford]] philosopher [[Nick Bostrom]] argues in ''[[Superintelligence: Paths, Dangers, Strategies]]'' that insufficient research is being undertaken to mitigate the risks posed by artificial intelligence to our current society. To this end, Bostrom now heads The [[Future of Humanity Institute]]. [[Eliezer Yudkowsky]], and many others in the [[effective altruism]] are concerned about the risks a [[superintelligence|superintelligent]] system would pose to humanity.

In  February 2009, under the auspices of the [[Association for the Advancement of Artificial Intelligence]] (AAAI), [[Eric Horvitz]] chaired a meeting of leading computer scientists, artificial intelligence researchers and roboticists at Asilomar in Pacific Grove, California. The goal was to discuss the potential impact of the hypothetical possibility that robots could become self-sufficient and able to make their own decisions. They discussed the extent to which computers and robots might be able to acquire [[autonomy]], and to what degree they could use such abilities to pose threats or hazards.

In [[PBS]]'s [[Off Book (Web series)|Off Book]], [[Gary Marcus]] asks "what happens if (AIs) decide we are not useful anymore?" Marcus argues that AI cannot, and should not, be banned, and that "the sensible thing to do" is to "start thinking now" about AI ethics.<ref name=pbs_off_book_rise_of_ai>{{cite news|title=The Rise of Artificial Intelligence|url=http://www.youtube.com/watch?feature=player_detailpage&v=53K1dMyslJg#t=389|accessdate=24 October 2013|newspaper=[[PBS]] [[Off Book (Web series)|Off Book]]|date=11 July 2013|time=6:29-7:26|quote=...what happens if (AIs) decide we are not useful anymore? I think we do need to think about how to build machines that are ethical. The smarter the machines gets, the more important that is... [T]here are so many advantages to AI in terms of human health, in terms of education and so forth that I would be reluctant to stop it. But even if I did think we should stop it, I don't think it's possible... if, let's say, the US Government forbade development in kind of the way they did development of new stem cell lines, that would just mean that the research would go offshore, it wouldn't mean it would stop. The more sensible thing to do is start thinking now about these questions... I don't think we can simply ban it.}}</ref>

[[Isaac Asimov]]'s [[Three Laws of Robotics]] is one of the earliest examples of proposed safety measures for AI. The laws are intended to prevent artificially intelligent robots from harming humans. In Asimov<U+2019>s stories, any perceived problems with the laws tend to arise as a result of a misunderstanding on the part of some human operator; the robots themselves are merely acting to their best interpretation of their rules. In the [[2004 in film|2004]] film ''[[I, Robot (film)|I, Robot]]'', loosely based on Asimov's [[Robot series (Asimov)|''Robot'' stories]], an AI attempts to take complete control over humanity for the purpose of protecting humanity from itself due to [[Zeroth law of robotics|an extrapolation of the Three Laws]]. In 2004, the Singularity Institute launched an Internet campaign called ''3 Laws Unsafe'' to raise awareness of AI safety issues and the inadequacy of Asimov<U+2019>s laws in particular.<ref>{{Harv|Singularity Institute for Artificial Intelligence|2004}}</ref>

{{harvtxt|Hibbard|2014}} proposes an AI design that avoids several dangers including self-delusion,<ref name="JAGI2012">{{Citation| journal=Journal of Artificial General Intelligence| year=2012| volume=3| title=Model-Based Utility Functions| first=Bill| last=Hibbard| postscript=.| doi=10.2478/v10229-011-0013-5| page=1|arxiv = 1111.3934 |bibcode = 2012JAGI....3....1H }}</ref> unintended instrumental actions,<ref name="selfawaresystems">[http://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/ Omohundro, Stephen M., "The Basic AI Drives." Artificial General Intelligence, 2008 proceedings of the First AGI Conference, eds. Pei Wang, Ben Goertzel, and Stan Franklin. Vol. 171. Amsterdam: IOS, 2008 ]</ref><ref name="AGI-12a">[http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_56.pdf  Avoiding Unintended AI Behaviors.] Bill Hibbard. 2012 proceedings of the Fifth Conference on Artificial General Intelligence, eds. Joscha Bach, Ben Goertzel and Matthew Ikle. This paper won the Singularity Institute's 2012 Turing Prize for the Best AGI Safety Paper [http://intelligence.org/2012/12/19/december-2012-newsletter/]
.</ref> and corruption of the reward generator.<ref name="AGI-12a"/> [[Bill Hibbard|He]] also discusses social impacts of AI<ref name="JET2008">{{Citation| url=http://jetpress.org/v17/hibbard.htm| journal=Journal of Evolution and Technology| year=2008| volume=17| title=The Technology of Mind and a New Social Contract| first=Bill| last=Hibbard| postscript=.}}</ref> and testing AI.<ref name="AGI-12b">[http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_57.pdf  Decision Support for Safe AI Design|.] Bill Hibbard. 2012 proceedings of the Fifth Conference on Artificial General Intelligence, eds. Joscha Bach, Ben Goertzel and Matthew Ikle.</ref> His 2001 book ''[[Super-Intelligent Machines]]'' advocates the need for public education about AI and public control over AI. It also proposed a simple design that was vulnerable to some of these dangers.

One hypothetical approach towards attempting to control an artificial intelligence is an [[AI box]], where the artificial intelligence is kept constrained inside a [[Computer simulation|simulated world]] and not allowed to affect the external world. However, a sufficiently intelligent AI may simply be able to escape by outsmarting its less intelligent human captors.<ref name="positive-and-negative">{{Citation | last = Yudkowsky | first = Eliezer  | title = Artificial Intelligence as a Positive and Negative Factor in Global Risk | journal =  Global Catastrophic Risks | editor-last = Bostrom | editor-first = Nick | editor2-last = Cirkovic | editor2-first = Milan | publisher =  Oxford University Press | year = 2008 | url = http://singinst.org/AIRisk.pdf | bibcode = 2008gcr..book..303Y | isbn=978-0-19-857050-9 | page = 303}}</ref><ref name="berglas">[http://berglas.org/Articles/AIKillGrandchildren/AIKillGrandchildren.html#mozTocId817119 Artificial Intelligence Will Kill Our Grandchildren (Singularity), Dr Anthony Berglas]</ref><ref name="philosophical">The Singularity: A Philosophical Analysis David J. Chalmers</ref>

Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions.<ref>[http://news.bbc.co.uk/2/hi/technology/8182003.stm Call for debate on killer robots], By Jason Palmer, Science and technology reporter, BBC News, 8/3/09.</ref> There are also concerns about technology which might allow some armed robots to be controlled mainly by other robots.<ref>[http://www.wired.com/dangerroom/2009/08/robot-three-way-portends-autonomous-future/ Robot Three-Way Portends Autonomous Future], By David Axe wired.com, August 13, 2009.</ref> The US Navy has funded a report which indicates that as [[military robots]] become more complex, there should be greater attention to implications of their ability to make autonomous decisions.<ref>[http://www.dailytech.com/New%20Navyfunded%20Report%20Warns%20of%20War%20Robots%20Going%20Terminator/article14298.htm New Navy-funded Report Warns of War Robots Going "Terminator"], by Jason Mick (Blog), dailytech.com, February 17, 2009.</ref><ref>[http://www.engadget.com/2009/02/18/navy-report-warns-of-robot-uprising-suggests-a-strong-moral-com/ Navy report warns of robot uprising, suggests a strong moral compass], by Joseph L. Flatley engadget.com, Feb 18th 2009.</ref> One researcher states that autonomous robots might be more humane, as they could make decisions more effectively. However, other experts question this.<ref>[http://www.csmonitor.com/layout/set/print/content/view/print/279448 New role for robot warriors;] Drones are just part of a bid to automate combat. Can virtual ethics make machines decisionmakers?, by Gregory M. Lamb / Staff writer, Christian Science Monitor, February 17, 2010.</ref>

Some support the design of friendly artificial intelligence, meaning that the advances that are already occurring with AI should also include an effort to make AI intrinsically friendly and humane.<ref name="asimovlaws">[http://www.asimovlaws.com/articles/archives/2004/07/why_we_need_fri_1.html Article at Asimovlaws.com], July 2004, accessed 7/27/2009.</ref>  A "friendly" AI could help reduce existential risk by developing technological solutions to threats.<ref name="Yudkowsky"/>

== See also ==
{{div col||25em}}
* [[AI takeover]]
* [[Effective altruism#Far future and global catastrophic risks|Effective altruism, the far future and global catastrophic risks]]
* [[Future of Humanity Institute#Existential risk|Future of Humanity Institute on existential risk]]
* [[Global catastrophic risk]] (existential risk)
* [[Machine ethics]]
* [[Nick Bostrom#Existential risk|Nick Bostrom on existential risk]]
* [[Outline of artificial intelligence]]
* [[Outline of robotics]]
* [[Outline of transhumanism]]
* [[Superintelligence]]
* ''[[Superintelligence: Paths, Dangers, Strategies]]''
{{div col end}}

== References ==
{{Reflist|30em}}

== External links ==
* [http://cser.org/emerging-risks-from-technology/artificial-intelligence/ AI page of the Centre for the Study of Existential Risk]
* ''[http://www.nickbostrom.com/existential/risks.html Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards]'', by Nick Bostrom
* ''[https://intelligence.org/summary/ Reducing Long-Term Catastrophic Risks from Artificial Intelligence]'' &ndash; Machine Intelligence Research Institute

{{DEFAULTSORT:Artificial general intelligence}}
[[Category:Artificial intelligence]]
[[Category:Futurology]]
[[Category:Human extinction]]
[[Category:Hazards]]
[[Category:Risk analysis]]
[[Category:Doomsday scenarios]]
