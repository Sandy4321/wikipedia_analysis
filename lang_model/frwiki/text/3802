
L''''écart type''' (en anglais {{lang|en|''standard deviation''}}) est une notion mathématique définie en [[probabilité]]s et appliqué à la [[statistique]]. L'écart type est une mesure de la dispersion d'une [[variable aléatoire réelle]]. Il est défini comme la [[racine carrée]] de la [[Variance (statistiques et probabilités)|variance]]. Il a la même [[Dimension#Dimension d'une grandeur|dimension]] que la variable aléatoire dont on calcule la dispersion.

C'est une grandeur dont l'invention remonte à la période du {{s-|XX|e}} qui vit la statistique se développer au Royaume-Uni.

Les écarts types sont rencontrés dans tous les domaines où sont appliqués les probabilités et la statistique, en particulier dans le domaine des [[Sondage (statistique)|sondage]]s, en [[physique]], en [[biologie]] ou dans la [[finance]]. Ils permettent en général de synthétiser les résultats numériques d'une expérience répétée.

Quand l'écart type d'une population est inconnu, sa valeur est approchée à l'aide d'estimateurs.

Tant en probabilités qu'en statistique, il sert à l'expression d'autres notions importantes comme le [[coefficient de corrélation]], ou la répartition optimale de  [[Jerzy Neyman|Neyman]]. 

Sur de grands ensembles de données, l'écart type peut être calculé grâce à l'ordinateur mais il faut rechercher le bon algorithme.    

== Histoire ==
[[Fichier:Comparison standard deviations.svg|thumb|fig. 01 - Exemple de deux échantillons ayant la même moyenne mais des écarts types différents illustrant l'écart type comme mesure de la dispersion autour de la moyenne.]]
C'est à [[Abraham de Moivre]] qu'est attribuée la découverte du concept de mesure de la dispersion qui apparaît dans son ouvrage ''{{lang|en|The Doctrine of Chances}}'' en 1718<ref name=PBerstein group="b">{{Harvsp|Bernstein|1996|p=127}}</ref>. Mais le terme d'écart type ({{citation étrangère|lang=en|standard deviation}}) a été employé pour la première fois par [[Karl Pearson]] en 1893 devant la {{citation étrangère|lang=en|London Royal Society}}<ref name=Dodge506 group="b">{{Harvsp|Dodge|2010|p=506}}</ref>. C'est aussi Karl Pearson qui utilisa pour la première fois le symbole <math>\scriptstyle \sigma</math> pour représenter l'écart type<ref name=Dodge506 group="b"/>. En 1908, [[William Gosset]], plus connu sous le pseudonyme de Student, définit l'écart type empirique d'un [[Échantillon (statistiques)|échantillon]] et montre  qu'il est important de le distinguer de l'écart type d'une [[population]]<ref name=Dodge506 group="b"/>. La [[Variance (statistiques et probabilités)|variance]] est une notion qui apparut plus tard, en 1918, dans un texte de [[Ronald Aylmer Fisher|Ronald Fisher]] intitulé ''{{lang|en|The Correlation between Relatives on the Supposition of Mendelian Inheritance}}''<ref name=RAFisher group="i"> {{pdf}} {{Article|langue=en|prénom1= Ronald Aylmar|nom1= Fisher|lien auteur1=Ronald Aylmer Fisher | titre                  = The Correlation between Relatives on the Supposition of Mendelian Inheritance| périodique             = Philosophical Transactions of the Royal Society of Edinburgh| volume                 = 52| année                  = 1918| pages                  = 399–433| url texte              = http://digital.library.adelaide.edu.au/dspace/bitstream/2440/15097/1/9.pdf| consulté le            = 25 avril 2012}}</ref>.

== Contexte général ==

En [[statistique]] comme en [[Probabilité|probabilités]], on définit des critères de [[Tendance centrale|position]] ainsi que des critères de dispersion. Dans le domaine des probabilités, la dispersion d'une [[variable aléatoire réelle]] ''X'' autour de sa moyenne est mesurée par divers indicateurs comme l'[[écart interquartile]], l'[[Glossaire_des_statistiques#E|étendue]], la [[Variance (statistiques et probabilités)|variance]], l'écart type {{etc}}<ref name=Saporta25 group="b">{{Harvsp|Saporta|2006|p=25}}</ref>.

En [[statistique descriptive]], où l'étude porte sur une population finie parfaitement connue, la moyenne et la [[Médiane (statistiques)|médiane]] sont utilisées comme critères de [[Tendance centrale|position]] et l'écart type, l'[[écart moyen]], l'étendue {{etc}} comme critères de [[dispersion statistique|dispersion]]. Tous ces critères aident ensemble à résumer l'échantillon statistique<ref name=Saporta119 group="b">{{Harvsp|Saporta|2006|p=119}}</ref>.

Dans la pratique, on préfère l'écart type <math>\scriptstyle \sigma</math> (lettre grecque [[sigma]]) à la variance <math>\scriptstyle V = \sigma^2</math>, car l'écart type peut-être comparé à l'ordre de grandeur des valeurs, ce qui n'est pas le cas de la variance<ref name=Saporta121 group="b">{{Harvsp|Saporta|2006|p=121}}</ref>.

== Applications ==
[[Fichier:Standard deviation diagram.svg|thumb|fig. 02 - Représentation graphique d'une [[loi normale]]. Chaque bande colorée a la largeur d'un écart type.]]

L'écart type sert à mesurer la dispersion d'un ensemble de données. Plus il est faible, plus les valeurs sont regroupées autour de la moyenne. Par exemple pour la répartition des notes d'une classe, plus l'écart type est faible, plus la classe est homogène. À l'inverse, s’il est plus important, les notes sont moins resserrées. Dans le cas d'une notation de 0 à 20, l'écart type minimal est 0 (notes toutes identiques), et peut valoir jusqu'à 10 si la moitié de la classe a 0/20 et l'autre moitié 20/20<ref group="Note">Si n élèves ont 0/20 et n élèves ont 20/20, c'est-à-dire l'échantillon contient n fois la valeur 20 et n fois la valeur 0, la moyenne est <math>\scriptstyle \frac{n\times 20}{n+n}</math> ; soit <math>\scriptstyle\bar X = 10</math> et <math>\scriptstyle\bar X^2 = 100</math>.<br/> Les valeurs au carré, notées <math>\scriptstyle X^2</math>, sont n fois 400 et n fois 0. La moyenne de <math>\scriptstyle X^2</math> vaut donc <math>\scriptstyle\overline{X^2} = 200</math>. On en déduit que la variance vaut 100 et l'écart type 10.</ref>.

En [[Sciences humaines et sociales|sciences humaines]], il est fréquent de considérer que les valeurs se répartissent selon une [[Fonction gaussienne|courbe de Gauss]]. Dans ce cas, la moyenne et l'écart type permettent de déterminer un intervalle dans lequel on trouve une majorité de la population. En effet, si la moyenne est <math>\scriptstyle m</math> et l'écart type est <math> \scriptstyle \sigma</math>, on trouve 95 % de la population dans l'intervalle <math>\scriptstyle [m - 1,96 \,\sigma\, ;\, m + 1,96 \,\sigma]</math> et on trouve 68 % de la population dans l'intervalle <math>\scriptstyle [m - \,\sigma\, ;\, m + \,\sigma]</math><ref name=Saporta43 group="b">{{Harvsp|Saporta|2006|p=43-44}}</ref>.

L'écart type est aussi utilisé pour construire un [[intervalle de confiance]] attribuable à un échantillon. Si l'on se réfère à la figure ci-contre, on voit qu'un sigma d'écart de part et d'autre de la valeur moyenne recouvre 68.2% de la distribution, deux sigmas d'écart ([<math>\scriptstyle -2\sigma</math>, <math>\scriptstyle +2\sigma</math>], 13.6+34.1+34.1+13.6 =) 95.4%,  3 sigmas d'écart ([<math>\scriptstyle -3\sigma</math>, <math>\scriptstyle +3\sigma</math>], 2.1+13.6+34.1+34.1+13.6+2.1 =) 99.8% et ainsi de suite...
C'est l'usage notamment en [[physique des particules]], où la détection d'évènements est quantifiée en nombre de sigmas, et où un résultat notamment est considéré comme significatif par l'obtention de 5 sigmas, représentant une probabilité d'erreur inférieure à 0,00003 % (niveau de confiance de plus de 99.9999%)<ref name=RHeuer group="i"> {{article|prénom1=Rolf|nom1=Heuer|titre=Une fin d’année pleine de suspense|périodique=Bulletin Hebdomadaire du CERN|volume=2012|numéro=3|année=2012|url=http://cdsweb.cern.ch/record/1416005?ln=fr|consulté le=27 avril 2012}}</ref>.
[[Fichier:Bollinger-bands-example2.jpg|right|thumb|fig.03 - La moyenne mobile est en rouge et les bandes de Bollinger, calculées à l'aide de l'écart type, sont en bleu.]]
Dans le domaine de l'[[analyse technique]] des [[cours de la bourse]], l'écart type est une mesure de la [[Volatilité (finance)|volatilité]] des cours
<ref name=JPPetit group="b">
{{Ouvrage
 | prénom1                = Jean-Pierre
 | nom1                   = Petit
 | titre                  = La Bourse  
 | sous-titre             = Rupture et Renouveau
 | éditeur                = Odile Jacob economie
 | lieu                   = Paris 
 | année                  = 2003
 | pages totales          = 285
 | passage                = 36
 | isbn                   = 978-2738113382
 | consulté le            = 27 avril 2012
}}</ref>. Les [[bandes de Bollinger]] sont des outils facilitant l'analyse des prévisions boursières. [[John Bollinger]] a construit la courbe des [[Moyenne glissante|moyennes mobiles]] sur 20 jours et les courbes, de part et d'autre de cette courbe, situées à deux fois l'écart type sur ces 20 jours. John Bollinger a utilisé une définition adaptée de l'écart type<ref name=JBollinger group="i"> {{en}} {{Lien web
 | auteur                 = John Bollinger
 | lien auteur            = John Bollinger
 | url                    = http://www.bollingerbands.com/#
 | titre                  = Bollinger Bands Introduction
 | consulté le            = 27 avril 2012
}}</ref>. En outre, le risque d'un actif boursier et le risque associé au marché sont mesurés par l'écart type de la [[rentabilité]] attendue, dans le [[modèle d'évaluation des actifs financiers]] de [[Harry Markowitz]]<ref group="i">{{pdf}}{{article|prénom1=P|nom1=Fery|titre=Risque et calcul socioéconomique|périodique=Centre d'analyse stratégique|volume=|numéro=|année=2010|url=http://www.strategie.gouv.fr/content/rapport-le-calcul-du-risque-dans-les-investissements-publics (pour aller plus loin)|consulté le=8 avril 2012}}</ref>. 

Dans l'[[industrie]],  l'écart type intervient dans le calcul de l'indice de qualité des [[produit manufacturé]]s ou dans l'indice de fidélité d'un [[Instrument de mesure|appareil de mesure]]<ref name=PFerignac group="i">{{pdf}}{{article|prénom1=P|nom1=Ferignac|titre=Contrôle de réception quantitatif ou par mesure.|périodique=Revue de statistique appliquée |volume=7 |numéro=2 |mois=| année=1959| url=http://archive.numdam.org/ARCHIVE/RSA/RSA_1959__7_2/RSA_1959__7_2_27_0/RSA_1959__7_2_27_0.pdf|consulté le=26 mars 2012}}</ref>{{,}}<ref name=PFerignac2 group="i">{{pdf}}{{article|prénom1=P|nom1=Ferignac|titre=Erreurs de mesure et contrôle de la qualité.|périodique=Revue de statistique appliquée |volume=13 |numéro=2 |mois=| année=1965| url=http://archive.numdam.org/ARCHIVE/RSA/RSA_1965__13_2/RSA_1965__13_2_57_0/RSA_1965__13_2_57_0.pdf|consulté le=26 mars 2012}}</ref>.

Enfin, en [[mécanique quantique]], le [[principe d'incertitude]] d'[[Heisenberg]] exprime que le produit des écarts-types de la [[Vecteur position|position]] x et de l'[[Moment linéaire|impulsion]] p d'une [[Particule subatomique|particule]] est supérieur ou égal à la [[constante de Plank]] divisée par deux, soit <math>\scriptstyle \sigma_{x} \sigma_{p} \ge \frac{\hbar}{2}</math> <ref name=YMeyer group="i">{{pdf}}{{article|prénom1=Yves|nom1=Meyer|titre=Principe d'incertitude, bases hilbertiennes et algèbres d'opérateurs.|périodique=Séminaire Bourbaki |volume=662 |numéro= |mois=février| année=1986| url=http://archive.numdam.org/ARCHIVE/SB/SB_1985-1986__28_/SB_1985-1986__28__209_0/SB_1985-1986__28__209_0.pdf|consulté le=4 avril 2012}}</ref>.

{{clr|left}}

== En probabilité ==
Dans la formulation moderne des probabilités, suite aux travaux de [[Henri Léon Lebesgue|Henri Lebesgue]] et à la mise en place de l'[[Axiomes des probabilités|axiomatique de Kolmogorov]], une [[variable aléatoire]] ''X'' est une [[Application (mathématiques)|application]] à valeurs réelles, ou [[Espace vectoriel|vectorielles]]<ref group="Note">en fait, à valeurs dans des [[Espace de Banach|espaces de Banach]], mais cet article se limite au cas réel qui est le plus utilisé en pratique</ref>, suivant une loi de probabilité P. L'application ''X'' ne joue pas un rôle fondamental ; seule sa loi importe : l'image de P par ''X'', notée P<sub>''X''</sub>. Il s'agit d'une mesure sur <math>\scriptstyle \R</math> ou sur <math>\scriptstyle \R^n</math><ref name=Saporta16 group="b">{{Harvsp|Saporta|2006|p=16}}</ref>. 

=== Définition ===
Si X est une variable aléatoire de carré intégrable, appartenant donc à l'espace <math>\scriptstyle \mathcal{L}^2 \left(\Omega, \mathcal A, P\right)</math><ref group="Note">où <math>\scriptstyle \Omega</math> est un ensemble, <math>\scriptstyle \mathcal A</math> une [[Tribu (mathématiques)|tribu]] sur <math>\scriptstyle \Omega</math>, et <math>\scriptstyle P</math> une mesure sur <math>\scriptstyle \mathcal A</math>.</ref>, son écart type, généralement noté <math>\scriptstyle \sigma_X</math>, est défini comme la racine carrée de l'espérance mathématique de (''X''−E[''X''])² , soit:
<center><math>\sigma_X=\sqrt{ E[\left (X-E[X])^2 \right]}=\sqrt{ E[ X^2 ] - E[ X ]^2}</math><ref group="Note"> la première égalité définit <math>\scriptstyle {\sigma_X}^2</math>, la seconde est donnée par le [[Théorème de König-Huyghens]]</ref>{{,}}<ref name=SMéléard group="i"> {{pdf}} {{Lien web
 | auteur                 = Sylvie Méléard
 | url                    = http://catalogue.polytechnique.fr/site.php?id=54&fileid=288
 | titre                  = Aléatoire : Introduction à la théorie et au calcul des probabilités
 | page                   = 57,94
 | consulté le            = 07 mai 2012
}}</ref>.</center>

L'élévation au carré pour le membre de droite désigne implicitement la [[Norme (mathématiques)|norme euclidienne]] au carré dans le cas où ''X'' est à valeurs vectorielles.

Enfin, l'écart type élevé au carré est égal à la [[variance]].

=== Exemples ===
Cette identité se spécialise dans un grand nombre de cas particuliers, dont celui des variables aléatoires discrètes. Si la variable ''X'' prend un nombre fini de valeurs réelles ''x₁'', …, ''x''<sub>''n''</sub>, avec des probabilités respectives ''p₁'', …, ''p''<sub>''n''</sub>, l'écart type est donné par <math>\scriptstyle \sigma := \sqrt{\sum_{i=1}^n p_i (x_i-\overline{x})^2} = \sqrt{\left( \sum_{i=1}^n p_i {x_i}^2 \right) - \overline{x}^2 }</math>, où <math>\overline{x}</math> désigne la moyenne <math>\scriptstyle \sum_{i=1}^n p_i x_i</math>. En particulier, si la loi de ''X'' est [[Loi uniforme discrète|uniforme]] sur un ensemble fini de valeurs, c'est-à-dire si <math>\scriptstyle p_i = \frac{1}{n}, i=1,..n</math><ref name=Saporta30 group="b">{{Harvsp|Saporta|2006|p=30}}</ref>, alors <math>\scriptstyle \sigma_X:=\sqrt{ \frac{1}{n}\sum_{i=1}^n (x_i-\overline{x})^2} = \sqrt{ \frac{1}{n}\left( \sum_{i=1}^n {x_i}^2 \right) - \overline{x}^2 }</math>, où cette fois, <math>\scriptstyle \overline{x}=\frac{1}{n}\sum_{i=1}^n x_i</math>.
Ces formules se généralisent immédiatement en [[Dimension d'un espace vectoriel|dimension]] supérieure en remplaçant l'élévation au carré par la norme euclidienne au carré.


La [[loi de probabilité|loi]] P<sub>''X''</sub> est dite continue lorsque la probabilité que ''X'' appartienne au segment ]a,b[ est <math>\scriptstyle \mathbb P_X ]a,b[ \longmapsto \mathbb P\left(X\in ]a,b[\right)=\int_a^b f(x)\,\mathrm{d}x</math> où ''f'' est une [[fonction localement intégrable]], pour la [[mesure de Lebesgue]] par exemple, mais pas nécessairement une fonction continue<ref name=Rioul45 group="b">{{Harvsp|Rioul|2006|p=45}}</ref>. Cette fonction ''f'' s'appelle la [[densité de probabilité]] de la loi P<sub>''X''</sub>. L'écart type de ''X'' est défini par  <math>\scriptstyle \sigma_X:=\sqrt{\int_{\R} (x- \mu)^2 \mathrm{d}x}</math> où <math>\scriptstyle \mu =\sqrt{\int_{\R} xf(x)\,\mathrm{d}x}</math> et la formule <math>\scriptstyle \sigma_X:=\sqrt{\int_{\R} x^2 f(x)\mathrm{d}x-{\left(\int_{\R} x f(x) \,\mathrm{d}x\right)}^2}</math> est équivalente aussi.

Avec ces formules et la définition le calcul des écarts types pour les lois couramment rencontrées est aisée. Le tableau suivant donne les écarts types de quelques-unes de ces lois :
{| class="wikitable centre" 
|-align="center" 
! Nom de la loi
! Paramètre(s)
! Description
! Écart type
|-
| [[Loi de Bernoulli]]<ref group="b" name="Saporta30"/> || ''p'' || Loi discrète de valeurs 0 avec probabilité 1-''p'' et 1 avec probabilité ''p'' || <math>\scriptstyle \sigma=\sqrt{p(1-p)}</math>
|-
| [[Loi binomiale]]<ref name=Saporta31 group="b">{{Harvsp|Saporta|2006|p=31}}</ref> || ''p'' et <math>\scriptstyle n\in\N^*</math> || Loi de la somme de ''n'' variables indépendantes suivant la loi de Bernoulli de paramètre ''p'' || <math>\scriptstyle \sigma=\sqrt{n p (1-p)}</math>
|-
| [[Loi géométrique]]<ref name=Saporta38 group="b">{{Harvsp|Saporta|2006|p=38}}</ref> || ''p'' || Loi discrète sur <math>\N</math> telle que la probabilité d'obtenir l'entier ''n'' soit (1-''p'')''p''<sup>''n''</sup> || <math>\scriptstyle \sigma=\sqrt{\frac{1-p}{p^2}}</math>
|-
| Loi uniforme sur un segment<ref name=Saporta39 group="b">{{Harvsp|Saporta|2006|p=39}}</ref> || ''a''<''b'' || Loi uniformément continue sur <math>\R</math> de densité un multiple de la fonction indicatrice de [''a'';''b''] || <math>\scriptstyle \sigma=\frac{b-a}{\sqrt{12}}</math>
|-
| [[Loi exponentielle]]<ref name=Saporta39 group="b"/> || ''p'' ||Loi uniformément continue de [[support de mesure|support]] <math>\R_+</math> de densité la fonction <math>f\colon x \mapsto p \exp(-p x)</math> || <math>\scriptstyle \sigma=\frac{1}{p}</math>
|-
| [[Loi de Poisson]]<ref name=Saporta33 group="b">{{Harvsp|Saporta|2006|p=33}}</ref> || ''<math>\scriptstyle \lambda</math>'' ||Loi sur <math>\N</math>  de densité la fonction <math>\scriptstyle f\colon x \mapsto \exp(-\lambda)\frac{\lambda^x}{x!}</math> où <math>\scriptstyle \lambda \in \R_+</math> || <math>\scriptstyle \sigma=\sqrt{\lambda}</math>
|-
| [[Loi du χ²]]<ref name=Dodge71 group="b">{{Harvsp|Dodge|2010|p=71}}</ref> || ''<math>\scriptstyle n</math>'' ||Loi sur <math>\R^+</math>  de densité la fonction <math>\scriptstyle f\colon x \mapsto \frac{1}{2^\frac{n}{2}\Gamma(\frac{n}{2})} x^{\frac{n}{2} - 1} e^{-\frac{x}{2}}\,</math> pour tout ''x'' positif où <math>\scriptstyle \Gamma </math> est la fonction gamma || <math>\scriptstyle \sigma=\sqrt{2n}</math>
|-
|}
<br />


Si la variable X suit une [[loi log-normale]] alors <math>\scriptstyle \ln X</math> suit une [[loi normale]] et l'écart type de X est relié à l'[[écart type géométrique]]<ref name=WHFinlay group="b">{{Ouvrage
 | langue                 = en
 | prénom1                = Warren H.
 | nom1                   = Finlay
 | lien auteur1           = 
 | titre                  = {{lang|en|The Mechanics of Inhaled Pharmaceutical Aerosols: An Introduction }}
 | sous-titre             = 
 | lien titre             = 
 | numéro d'édition       = 
 | éditeur                = Academic Press Inc
 | lien éditeur           = 
 | lieu                   = San Diego
 | année                  = 2001
 | volume                 = 
 | tome                   = 
 | pages totales          = 320
 | passage                = 5
 | isbn                   = 978-0122569715
 | lire en ligne          = 
 | consulté le            = 15 avril 2012
 | présentation en ligne  = 
 | id                     = 
}}  </ref>.

Mais toutes les lois de probabilité n'admettent pas forcément un écart type fini : la [[loi de Cauchy]] (ou loi de Lorentz) n'a pas d'écart type, ni même d'espérance mathématique<ref name=Dodge60 group="b">{{Harvsp|Dodge|2010|p=60}}</ref>.

=== Propriétés ===
L'écart type est toujours positif ou nul, celui d'une constante est nul. L'écart type d'une variable aléatoire X à laquelle a été ajoutée une constante <ref group = "Note"><math>\scriptstyle Y = X + a </math></ref>est égal à l'écart type de la variable X. Cette propriété est nommée ''invariance par translation''. L'écart type d'une variable multipliée par une constante est égal à la [[valeur absolue]] de la constante multipliée par l'écart type de la variable. Cette propriété est nommée ''invariance par dilatation''<ref group="Note"> Toutes ces propriétés sont la conséquence directe du théorème de Huygens et des propriétés de l'espérance mathématique .</ref>{{,}}<ref name=Saporta2325 group="b">{{Harvsp|Saporta|2006|p=23-25}}</ref>. Ceci peut se résumer par <math>\scriptstyle \sigma_{cX+b}=|c|\sigma_{X}</math>. 

L'écart type de la somme algébrique de deux variables est égal à <math>\scriptstyle \sigma_{X\pm Y}=\sqrt{\sigma_X^2+\sigma_Y^2\pm 2\sigma_X \sigma_Y \rho(X,Y)}</math> où <math>\scriptstyle \rho(X,Y)</math> est le [[coefficient de corrélation]] entre les deux variables X et Y<ref name=Saporta26 group="b">{{Harvsp|Saporta|2006|p=26}}</ref>.

La fonction <math>\scriptstyle \R  \rightarrow  \R^+ : c \rightarrow \sqrt{(|X-c|^2)}</math> admet son [[Optimisation (mathématiques)|minimum]] au point <math>\scriptstyle c=E(X)</math> et prend donc pour valeur en ce point l'écart type de la variable  <math>\scriptstyle X</math><ref name=Rioul146 group="b">{{Harvsp|Rioul|2006|p=146}}</ref>.  

=== Usage ===

L'écart type intervient en probabilité pour comparer des variables ou des distributions entre elles.

==== Variable centrée réduite ====

[[Fichier:Skewness Statistics.svg|thumb|right|fig.04 - exemples de distributions asymériques]]
[[Fichier:Standard symmetric pdfs.png|thumb|right|fig.05 - exemples de distributions plus ou moins aplaties]]

Si X est une variable aléatoire d'écart type non nul, on peut lui faire correspondre la [[Variable centrée réduite|variable centrée et réduite]] Z définie par <math>\scriptstyle Z= \frac{X - \bar X}{\sigma}</math>. Deux variables aléatoires centrées et réduites <math>\scriptstyle Z_1</math> et <math>\scriptstyle Z_2</math>  sont aisées à comparer, puisque <math>\scriptstyle E(Z_i)=0</math> et <math>\scriptstyle \sigma_{Z_i}=1</math><ref name=Gautier387 group="b">{{Harvsp|Gautier|1975|p=387}}</ref>. Le [[théorème central limite]] a pour objet la limite d'une suite de variables aléatoires centrées réduites<ref name=Saporta66 group="b">{{Harvsp|Saporta|2006|p=66}}</ref>, les [[Asymétrie (statistique)|coefficients de dissymétrie]] et d'[[Kurtosis|aplatissement]] d'une densité de probabilité,  <math>\scriptstyle E[Z^3]~ {et} ~E[Z^4]</math>, permettent de comparer des distributions différentes<ref name=Rioul157 group="b">{{Harvsp|Rioul|2008|p=157}}</ref>.
{{clr|left}}

==== Coefficient de corrélation ====
{{Article détaillé|Corrélation (statistiques)}}
Le coefficient de corrélation est une autre application de l'écart type en probabilité. Si ''X'' et ''Y'' sont deux [[Variable aléatoire|variables aléatoires]], on appelle coefficient de corrélation le rapport <math>\scriptstyle \rho = \frac{\operatorname{cov}(X,Y)} {\sigma_X \sigma_Y} </math> où <math>\scriptstyle \operatorname{cov}(X,Y)= \mathbb{E}[(X - \mathbb{E}[X])\,(Y-\mathbb{E}[Y])] = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]</math> est la [[covariance]] des variables ''X'' et ''Y''. D'après l'[[inégalité de Cauchy-Schwarz]], <math>\scriptstyle |\operatorname{cov}(X,Y)| \le \sigma_X \sigma_Y</math> ; ce qui permet de dire que <math>\scriptstyle \rho</math> prend ses valeurs dans l'[[Intervalle (mathématiques)|intervalle]] <math>\scriptstyle [-1,+1]</math><ref name=Rioul175 group="b">{{Harvsp|Rioul|2008|p=175}}</ref>. Si <math>\scriptstyle \rho = 0</math> les deux variables ne sont pas corrélées, si <math>\scriptstyle \rho = \pm 1</math> les deux variables sont [[Indépendance linéaire|linéairement dépendantes]]<ref name=Rioul178 group="b">{{Harvsp|Rioul|2008|p=178}}</ref>.

==== Inégalité de Bienaymé-Tchebychev ====
{{Article détaillé|Inégalité de Bienaymé-Tchebychev}}
C'est grâce à l'[[inégalité de Bienaymé-Tchebychev]] que l'écart type apparaît comme une mesure de la dispersion autour de la moyenne. En effet, cette inégalité exprime que <math>\scriptstyle P(|X-E(X)|>k\sigma) \le \frac{1}{k^2}</math><ref group="b" name="Saporta25"/> et montre que la probabilité pour que X s'écarte de E(X) de plus de k fois l'écart type est inférieure à  <math>\scriptstyle \frac{1}{k^2}</math><ref name=AJacquard2829 group="b">{{Harvsp|Jacquard|1976|p=28-29}}</ref>.

== En statistique ==
[[Fichier:Fisher's Iris data with standard deviation error bars around means.png|thumb|fig.06 - L'écart type sert à indiquer, à l'aide de [[barre d'erreur|barres d'erreur]], l'[[Calcul d'incertitude|incertitude]] du positionnement du barycentre de chaque groupe d'iris de l'[[échantillon (statistiques)|échantillon]] le long de la direction indiquée par la largeur des sépales.]]

Pour une population finie - relativement faible en nombre - le calcul de l'écart type est purement algébrique, sans référence aux probabilités, et le [[statisticien]] emploie l'écart type empirique défini par <math>\scriptstyle s:=\sqrt{\frac{1}{n}\sum_{i=1}^n(x_i-\overline{x})^2}</math><ref name=Saporta279 group="b"/>.

Mais, en [[statistique]], la population étudiée est souvent très importante en nombre, et il n'est pas possible de connaître toutes les valeurs de la caractéristique considérée. Le statisticien procède par échantillonnage et estimation pour évaluer les grandeurs analysées telles que l'écart type.   

=== Estimateurs ===

Un [[Estimateur (statistique)|estimateur]] est une fonction permettant d'approcher un paramètre d'une population à l'aide d'un échantillon tiré au [[hasard]]<ref name=Saporta289 group="b">{{Harvsp|Saporta|2006|p=289}}</ref>.

Deux [[Estimateur (statistique)|estimateurs]] de l'écart type sont généralement utilisés. Ces estimateurs sont notés <math>\scriptstyle S_n</math> (ou ''S'') et <math>\scriptstyle {S_{n-1}}</math> (ou ''S′'' ) et s'expriment en fonction des valeurs de l'échantillon par <math>\scriptstyle S_n=\sqrt{\frac{1}{n}\sum_{i=1}^n(X_i-\overline{X})^2}</math> et <math>\scriptstyle S_{n-1}=\sqrt{\frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X})^2} = \sqrt{\frac{n}{n-1}}\cdot S_n</math>. <math>\scriptstyle {S_{n-1}}</math> est l'estimateur privilégié<ref name="tuff655" group="b">{{Harvsp|Tufféry|2010|p=655}}</ref>{{,}}<ref name=Saporta279 group="b">{{Harvsp|Saporta|2006|p=279-280}}</ref>.

=== Propriétés des estimateurs ===
Ainsi, deux propriétés importantes des estimateurs sont la [[Estimateur (statistique)#Convergence|convergence]] et l'absence de [[Biais (statistique)|biais]]<ref name=Saporta279 group="b"/>. 

Si <math>\scriptstyle \hat \theta</math> est un estimateur du paramètre <math>\scriptstyle \theta</math>, le biais est la quantité <math>\scriptstyle E[\hat \theta] - \theta</math>. Si cette quantité est différente de zéro, cela signifie que <math>\scriptstyle \hat \theta</math> se positionne autour de <math>\scriptstyle E[\hat \theta]</math> au lieu de se positionner autour de  <math>\scriptstyle \theta</math>. L'estimateur <math>\scriptstyle \hat \theta</math> est alors entaché d'erreur. Un bon estimateur n'a pas de biais<ref name=Saporta290 group="b">{{Harvsp|Saporta|2006|p=290}}</ref>. Un estimateur naturel de l'écart type est <math>\scriptstyle S_{n-1}</math> biaisé mais dont le biais est acceptable<ref name=Saporta284 group="b">{{Harvsp|Saporta|2006|p=284}}</ref>{{,}}<ref group="Note">d'après G. Saporta on a <math>\scriptstyle E(S_{n-1})=\sigma \sqrt{ \frac{2}{n-1}}\frac{\Gamma(\frac{n}{2})}{\Gamma(\frac{n-1}{2})}</math> qui tend vers <math>\scriptstyle \sigma</math> lorsque <math>\scriptstyle n \to \infty</math> où  <math>\scriptstyle S_{n-1}= \sqrt{\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X)^2}</math> et <math>\scriptstyle \Gamma</math> la [[Fonction gamma]]</ref>{{,}}<ref name=EGrenier group="i">{{pdf}}{{article|prénom1=Emmanuel|nom1=Grenier|titre=Quelle est la « bonne » formule de l’écart-type ?|périodique=La revue MODULAD|volume=|numéro=37|mois=décembre|année=2007|url=http://www.modulad.fr/archives/numero-37/Notule-Grenier-37/Notule-Grenier-37.pdf|consulté le=18 février 2012}}</ref>. 


Si  <math>\scriptstyle \lim\limits_{n \to \infty} (a_n)=a</math>, <math>\scriptstyle (a_n)</math> converge vers a quand n tend vers l'infini<ref group="Note">la convergence peut-être en loi, en moyenne, presque surement, en probabilité... </ref>. Or, <math>\scriptstyle {S_n}^2</math> et <math>\scriptstyle {S_{n-1}}^2</math> sont des estimateurs [[Estimateur (statistique)#Convergence|convergents]] de ''σ²'', ce qui reflète l'approximation de ''σ²'' par ces deux séries lorsque n devient de plus en plus grand<ref name=Saporta290 group="b"/>. Grace au [[Convergence de variables aléatoires#Convergence d'une fonction d'une variable aléatoire|théorème de continuité]], stipulant que si f est continue <math>\scriptstyle \lim\limits_{n \to \infty} f(X_n)=f(\lim\limits_{n \to \infty} X_n)</math>, la fonction [[racine carrée]] étant continue, les estimateurs <math>\scriptstyle S_n</math> et <math>\scriptstyle S_{n-1}</math> sont convergents eux aussi<ref group="Note">d'après le théorème de continuité on a : {{Théorème|Si {{math|''g''}} est continue, alors :
<math>\scriptstyle X_n\xrightarrow{\mathbb{P}}X \Longrightarrow g(X_n)\xrightarrow{\mathbb{P}}g(X)</math>}}. Comme la fonction [[racine carrée]] est une fonction [[Continuité|continue]], <math>\scriptstyle S_{n-1}</math> et <math>\scriptstyle S_{n}</math> sont des estimateurs convergents de l'écart type, autrement dit : <math>\scriptstyle S_{n-1} \xrightarrow{\mathbb{P}} \sigma \text{ et } S_{n} \xrightarrow{\mathbb{P}} \sigma</math></ref>{{,}}<ref name=Rioul253 group="b">{{Harvsp|Rioul|2008|p=253}}</ref>. Ce qui conforte le statisticien à utiliser ces estimateurs.

=== Écart type des moyennes ===
Pour estimer la précision de l'estimation de la moyenne d'une variable, la méthode du calcul de l'écart type de la distribution d'échantillonnage des moyennes est utilisée. Appelé aussi [[Erreur type|erreur type de la moyenne]] ({{Citation étrangère|lang=en|Standard error}}), noté <math>\scriptstyle \sigma_{\bar x} </math>, c'est l'écart type des moyennes des échantillons de tailles identiques d'une population. Si n est la taille des échantillons prélevés sur une population d'écart type <math>\scriptstyle \sigma</math>, et si N est la taille de la population, alors <math>\scriptstyle \sigma_{\bar x} = \frac{\sigma}{\sqrt{n}}\sqrt{\frac{N-n}{N-1}}</math><ref name=Dodge509 group="b">{{Harvsp|Dodge|2010|p=508-509}}</ref>. Lorsque l'écart type <math>\scriptstyle \sigma</math> de la population est inconnu, il peut être remplacé par l'estimateur <math>\scriptstyle S_{n-1}</math><ref name=Dodge509 group="b"/>. Quand n est suffisamment grand (<math>\scriptstyle n \ge 30</math>), la distribution d'échantillonnage suit approximativement une loi de Laplace-Gauss, ce qui permet de déduire un intervalle de confiance, fonction de <math>\scriptstyle \sigma_{\bar x} </math>, permettant de situer la moyenne de la population par rapport à la moyenne de l'échantillon<ref name=Dodge472 group="b">{{Harvsp|Dodge|2010|p=472}}</ref>{{,}}<ref name=AVessereau56 group="b">{{Harvsp|Vessereau|1976|p=56}}</ref>.

=== Écart type des écarts-types empiriques ===

En général, il est très difficile de calculer la loi de distribution des écarts-types empiriques. Mais si <math>\scriptstyle X_n</math> est une suite de variables aléatoires distribuées selon la loi normale <math>\scriptstyle \mathcal{N}(\mu,\sigma^2) </math>, alors <math>\scriptstyle n\frac{S_n^2}{\sigma^2}</math> suit une loi du <math>\scriptstyle \chi^2</math> à n [[Degré de liberté (statistiques)|degrés de liberté]]<ref group="b" name="Dodge71"/>{{,}}<ref group="Note">par définition de la [[Loi du χ²|loi du <math>\scriptstyle \chi^2</math>]]</ref>.
Cette loi a pour écart type <math>\scriptstyle \sqrt{2n}</math> et donc l'écart type de la distribution des écarts types de variables normales a pour expression <math>\scriptstyle \sigma_{S_n^2}=\sigma^2\sqrt{\frac{2}{n}}</math><ref name=Dodge71 group="b"/>.

=== Interprétation d'un écart type élevé ===
Trois raisons au moins font que l'écart type peut être élevé. 
Tout d'abord il peut indiquer une grande dispersion des données autour de la valeur centrale. Plus les valeurs sont largement distribuées, plus l'écart type est élevé. 

Ensuite, en raison de ses liens étroits avec la moyenne, il peut être important parce que la moyenne est importante.
Mais, il n'est pas toujours facile d'évaluer la valeur de l'écart type à partir de laquelle la dispersion doit être considérée comme forte. Pour cette raison, il est quelques fois utile de travailler avec le [[coefficient de variation]] égal à l'écart type divisé par la [[moyenne]] et qui exprime l'écart type en [[pourcentage]] de la moyenne<ref name=Saporta121 group="b"/>.

Enfin, autre raison pour laquelle l'écart type peut être élevé est la présence de valeurs aberrantes dans l'échantillon. Une façon de savoir si la valeur x est une valeur aberrante est de calculer la valeur absolue de <math>\scriptstyle \frac{x - \bar x}{\sigma}</math>, qui, si elle est supérieure ou égale à 3, désigne x comme une valeur potentiellement aberrante<ref name=DAnderson group="b">{{article|langue=en|prénom1=David R.|nom1=Anderson|prénom2=Dennis J.|nom2=Sweeney|prénom3=Thomas A.|nom3=Williams| titre=statistics| périodique=Encyclopaedia Britannica Ultimate Reference Suite| volume=| numéro=| pages=statistics| mois= |année=2010 |url= | consulté le=26 février 2012}}</ref>..

=== Sondages d'opinion ===
Dans les [[Sondage d'opinion|sondages d'opinion]], l'écart type <math>\scriptstyle \sigma_{\bar x}</math> évalue l'incertitude des variations accidentelles de <math>\scriptstyle \bar x</math> inhérentes au sondage, ce qu'on appelle la marge d'erreur due aux variations accidentelles<ref name=WEDeming group="i">{{pdf}}{{article|prénom1=W.E.|nom1=Deming|titre=Quelques méthodes de sondage.|périodique=Revue de statistique appliquée |volume=12 |numéro=4 |mois=| année=1964| url=http://archive.numdam.org/ARCHIVE/RSA/RSA_1964__12_4/RSA_1964__12_4_11_0/RSA_1964__12_4_11_0.pdf|consulté le=9 avril 2012}}</ref>.


De plus, avec la méthode d'échantillonnage représentatif, lorsque les différentes strates ont des écarts types très différents, l'écart type est utilisé pour calculer la répartition optimale de [[Jerzy Neyman|Neyman]] qui permet d'évaluer la population dans les différentes strates en fonction de leur l'écart-type ; en d'autres termes <math>\scriptstyle n_i=n\frac{N_i\sigma_i}{\sum N_j\sigma_j}</math> est la taille de l'échantillon dans la strate i,  où n est la taille totale de l'échantillon, <math>\scriptstyle N_i</math> est la taille de la strate i, <math>\scriptstyle \sigma_i</math> l'écart-type de la strate i<ref name=WEDeming group="i"/>.

== En algorithmique ==

Le calcul de l'écart type par un programme d'ordinateur peut aboutir à des résultats incohérents si on n'utilise pas un algorithme adapté aux données, comme lorsqu'on utilise celui qui exploite directement la formule <math>\scriptstyle \sqrt{ \frac{1}{n}\left( \sum_{i=1}^n {x_i}^2 \right) - \left( {\frac{1}{n}\sum_{i=1}^n x_i} \right)^2 }</math> sur des grands échantillons de valeurs comprises entre 0 et 1<ref group="i"> {{en}} {{Lien web| auteur= John D. Cook| url= http://www.johndcook.com/blog/2008/09/28/theoretical-explanation-for-numerical-results/| titre= Theoretical explanation for numerical results| consulté le = 20 avril 2012}}</ref>{{,}}<ref group="i"> {{en}}
{{Lien web| auteur                 = John D. Cook| url                    = http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation/| titre                  = Comparing three methods of computing standard deviation| consulté le            = 20 avril 2012}}</ref>.

Un des meilleurs algorithmes est celui de B.P. Welford qui est décrit par [[Donald Knuth]] dans son livre {{Lang|en|The Art of Computer Programming, Vol 2}}<ref name=Welfod group="i">{{pdf}} {{article|langue=en|prénom1=B.P.|nom1=Welford| titre=Note on a Method for Calculating Corrected Sums of Squares and Products | périodique=Technometrics| volume=4| numéro=3| pages=419-420| mois=août |année=1962 |url=http://ebookfreetoday.com/view-pdf.php?bt=NOTE-XXX:-WELFORD%E2%80%99S-ALGORITHM-FOR-MEAN-AND-VARIANCE&lj=http://daheiser.info/excel/notes/NOTE%20P.pdf | consulté le=4 avril 2012}}</ref>{{,}}<ref group="i"> {{en}}
{{Lien web | auteur                 = John D. Cook | url                    = http://www.johndcook.com/standard_deviation.html | titre                  = Accurately computing running variance | consulté le            = 20 avril 2012 }}</ref>.

Une approximation de l'écart type de la direction du vent est donnée par l'[[Méthode de Yamartino|algorithme de Yamartino]] dont on se sert dans les [[anémomètre]]s modernes<ref name=RJYamartino group="i"> {{en}} {{pdf}} {{article|langue=en|prénom1=R.J.|nom1=Yamartino| titre=A comparison of several "single-pass" estimators of the standard deviation of wind direction | périodique=Journal of climate and applied meteorology| volume=23| pages=1362-1366| mois=janvier |année=1984 |url=http://journals.ametsoc.org/doi/pdf/10.1175/1520-0450%281984%29023%3C1362%3AACOSPE%3E2.0.CO%3B2 | consulté le=27 avril 2012}}</ref>{{,}}<ref group="i"> {{pdf}} {{en}} {{Lien web| auteur= Mike Bagot| url= http://www.ata.org.au/wp-content/projects/ata_wind_resource_assessment_report.pdf| titre=Victorian Urban Wind Resource Assessment| consulté le = 24 avril 2012}}</ref>.

== Notes et références ==

=== Notes ===

<references group="Note"/>

=== Références ===

==== Ouvrages spécialisés ====

{{Références|colonnes = 3|group="b"}}

==== Articles de revue ====

{{Références|colonnes = 2|group="i"}}

== Voir aussi ==

=== Bibliographie ===

* {{ouvrage|prénom1=Gilbert|nom1=Saporta|titre=Probabilités, Analyse des données et Statistiques|éditeur=éditions Technip|lieu=Paris|année=2006|pages totales=622|isbn=978-2-7108-0814-5}} {{plume}}
* {{ouvrage|prénom1=Alain|nom1=Monfort|titre=Cours de Statistique Mathématique|éditeur=éditions Economica|lieu=Paris|année=1997|pages totales=333|isbn=2-7178-3217-2}} {{plume}}
* {{ouvrage|langue=en|prénom1=|nom1=|titre=Encyclopaedia Britannica Ultimate Reference Suite|éditeur= Encyclopædia Britannica|lieu=Chicago|année=2010|pages totales=|isbn=}} {{plume}}
* {{ouvrage|prénom1=Olivier|nom1=Rioul|titre=Théorie des probabilités|éditeur=éditions Hermes sciences|lieu=Paris|année=2008|pages totales=364|isbn=978-2-7462-1720-1}} {{plume}}
* {{ouvrage|langue=en|prénom1=Yadolah|nom1=Dodge|titre=The Concise Encyclopaedia of Statistics|éditeur= Springer|lieu=New York|année=2010|pages totales=622|isbn=978-0-387-31742-7}} {{plume}}
* {{ouvrage|prénom1=Stéphane|nom1=Tufféry|titre=Data Mining et statistique décisionnelle|éditeur=éditions Technip|lieu=Paris|année=2010| pages totales=705|isbn = 978-2-7108-0946-3}} {{plume}}
* {{ouvrage|langue=en|prénom1=Peter L.|nom1=Bernstein|lien auteur1=|titre=Against the Gods|sous-titre=The Remarkable Story of Risk|lien titre=|numéro d'édition=|éditeur=John Wiley & sons, inc|lien éditeur=John Wiley & Sons|lieu=New York|année=1996|pages totales=383|isbn=978-0-471-12104-6}} {{plume}}
* {{ouvrage|prénom1=Albert|nom1=Jacquard|titre=Les Probabilités|numéro d'édition=|éditeur=Presses Universitaires de France|lien éditeur=Presses universitaires de France|lieu=Paris|année=1976|collection=Que sais-je|numéro dans collection =1571|pages totales=125|isbn=2-13-036532-9}} {{plume}}
* {{ouvrage|prénom1=C.|nom1=Gautier| prénom2=G.|nom2=Girard| prénom3=D.|nom3=Gerll| prénom4=C.|nom4=Thiercé|prénom5=A.|nom5=Warusfel| titre=Aleph1 Analyse|éditeur=éditions Hachette|lieu=Paris|année=1975| pages totales=465|isbn = 2-01-001370-0}}{{plume}}
* {{ouvrage|prénom1=André|nom1=Vessereau|titre=La Statistique|numéro d'édition=|éditeur=Presses Universitaires de France|lien éditeur=Presses universitaires de France|lieu=Paris|année=1976|collection=Que sais-je|numéro dans collection=281|pages totales=128|isbn=2-13-052942-9}} {{plume}}

=== Articles connexes ===
* [[Calcul d'erreur]]
* [[Critères de dispersion]]
* [[Erreur type]]
* [[Écart type géométrique]]

=== Liens externes ===

* {{en}} [[:en:Algorithms_for_calculating_variance|Algorithms for calculating variance]] 

{{Palette|Probabilités et statistiques}}
{{Portail|probabilités}}

{{Article potentiellement bon|oldid=78023523|date=25 avril 2012}}

{{DEFAULTSORT:Ecart type}}
[[Catégorie:Statistique descriptive]]
[[Catégorie:Probabilités]]

{{Lien BA|pl}}

[[ar:انحراف معياري]]
[[be-x-old:Стандартнае адхіленьне]]
[[bg:Стандартно отклонение]]
[[bs:Standardna devijacija]]
[[ca:Desviació tipus]]
[[cs:Směrodatná odchylka]]
[[da:Standardafvigelse]]
[[de:Standardabweichung]]
[[en:Standard deviation]]
[[eo:Norma diferenco]]
[[es:Desviación estándar]]
[[et:Standardhälve]]
[[fa:انحراف معیار]]
[[gl:Desvío estándar]]
[[he:סטיית תקן]]
[[hi:मानक विचलन]]
[[hr:Standardna devijacija]]
[[hu:Szórás (valószínűség-számítás)]]
[[id:Simpangan baku]]
[[is:Staðalfrávik]]
[[it:Deviazione standard]]
[[ja:標準偏差]]
[[kk:Квадраттық ауытқу]]
[[ko:표준편차]]
[[la:Deviatio canonica]]
[[lt:Standartinis nuokrypis]]
[[lv:Standartnovirze]]
[[mk:Стандардно отстапување]]
[[nl:Standaardafwijking]]
[[nn:Standardavvik]]
[[no:Standardavvik]]
[[oc:Desviacion tipica]]
[[pl:Odchylenie standardowe]]
[[pt:Desvio padrão]]
[[ru:Среднеквадратическое отклонение]]
[[scn:Diviazzioni standard]]
[[si:සම්මත අපගමනය]]
[[simple:Standard deviation]]
[[sk:Smerodajná odchýlka]]
[[sl:Standardni odklon]]
[[sq:Devijimi standard]]
[[sr:Стандардна девијација]]
[[su:Simpangan baku]]
[[sv:Standardavvikelse]]
[[ta:சராசரி அகற்சி]]
[[th:ค่าเบี่ยงเบนมาตรฐาน]]
[[tr:Standart sapma]]
[[uk:Стандартне відхилення]]
[[ur:معیاری انحراف]]
[[vi:Độ lệch chuẩn]]
[[war:Standard deviation]]
[[zh:標準差]]