[[Fichier:Pipeline sync-async.svg|thumb|upright=1.2|Principe du pipeline synchrone, en haut, où les données avancent au rythme de l'horloge, et du pipeline asynchrone, en bas, où les étages communiquent localement.]]

Un '''circuit asynchrone''' est un [[circuit électronique]] [[électronique numérique|numérique]] qui n'utilise pas de [[signal d'horloge]] global pour synchroniser ses différents éléments. À la place, ces derniers communiquent souvent localement en indiquant l'envoi et la réception de données. On parle parfois de « circuit '''auto-séquencé''' ».

Ils sont envisagés comme une alternative possible aux [[circuit synchrone|circuits synchrones]], plus répandus, particulièrement pour diminuer la consommation d'énergie, puisqu'une horloge reste active en permanence. L'absence d'[[signal d'horloge|horloge]] peut apporter d'autres avantages, comme une vitesse accrue, une fabrication facilitée et une plus grande fiabilité.

En 2012, malgré ces nombreux atouts potentiels et bien qu'ils aient vu le jour presque en même temps que les circuits synchrones, de tels circuits restent minoritaires. Par exemple, la plupart des processeurs fabriqués sont synchronisés par une horloge, tant sur le marché grand public que sur les serveurs, bien que l'on commence à voir apparaître des [[microcontrôleur]]s asynchrones.

== Histoire ==
L'histoire des circuits asynchrones est jalonnée de nombreuses réalisations majeures, la création d'ordinateurs ou de [[processeur]]s complets constituant une étape marquante. Elles ne doivent pas faire oublier néanmoins les avancées théoriques et, récemment, le développement d'outils de synthèse et de vérification qui les ont permises.

Tout comme dans les autres branches de l'électronique numérique, la recherche continue encore bien que le concept ait été exprimé dès les années 1950. Certains chercheurs présagent l'utilisation généralisée de techniques asynchrones<ref>Par exemple, Alain Martin écrit en 2007 : {{Citation étrangère|langue=en|An asynchronous approach offers many advantages and is unavoidable in the long run}} dans {{ppt}} [http://www.async.caltech.edu/general07.ppt Asynchronous Logic : Results and Prospects]</ref>, mais de telles prédictions<ref name="eHist"/> n'ont pas été réalisées par le passé.

=== Les débuts : circuits indépendants de la vitesse et automates finis ===
La théorie des circuits asynchrones débute avec les travaux de [[David A. Huffman]] sur l'implémentation d'[[Automate fini|automates finis]] en 1953<ref name="eHist">{{PDF}} {{ouvrage|titre=The Early History of Asynchronous Circuits and Systems|prénom1=Charles L.|nom1=Seitz|année=2009|mois=Mai|jour=20|lire en ligne=http://asyncsymposium.org/async2009/slides/seitz-async2009.pdf|langue=en}}</ref>. Le concept de « circuit asynchrone » tel qu'il est aujourd'hui, construit de manière modulaire en utilisant un protocole de communication entre les éléments, remonte à la fin des années 1950, quand les circuits « indépendants de la vitesse » reposant sur un protocole double-rail trois états sont introduits par David Muller<ref>{{harvsp|Sparsø, Furber|2001|p=23|réf=asyncdesign}}</ref> : son équipe construit l'[[ILLIAC]] I en 1952 et l'[[ILLIAC]] II en 1962<ref name="listeordis">{{PDF}} {{ouvrage|titre=Asynchronous Circuit Design|sous-titre=Lecture 1: Introduction, Preface and Chapter 1|langue=en|prénom1=Chris J.|nom1=Myers|lire en ligne=http://www.async.ece.utah.edu/book/lectures/lec1.pdf}}</ref>. Cependant, l'utilisation de ce type de circuits est alors jugée complexe : il n'y a pas de méthode simple permettant de réaliser de tels circuits, alors que les automates finis fournissent un modèle adapté<ref name="eHist"/>.

Durant les années 1960 et 1970, quelques circuits asynchrones voient le jour, dont les « macromodules », composants asynchrones qui sont assemblés pour créer des systèmes complexes<ref>{{PDF}} {{ouvrage|titre=Asynchronous Circuit Design|sous-titre=Lecture 9: Applications|langue=en|prénom1=Chris J.|nom1=Myers|lire en ligne=http://www.async.ece.utah.edu/~myers/nobackup/ece6750_08/lectures/lec9-2x3.pdf}}</ref> : l'interface asynchrone permet de ne se soucier que de problèmes logiques.

Ce n'est qu'à la fin des années 1980, voire à la fin des années 1990<ref>{{harvsp|Sparsø, Furber|2001|p=4|réf=asyncdesign}}</ref>, que les circuits asynchrones reviennent sur le devant de la scène, tant pour la recherche théorique et les publications que pour les réalisations pratiques.

=== L'essor des micropipelines et les premiers microprocesseurs ===
La fin des années 1980 et le début des années 1990 connaissent un regain d'intérêt pour les technologies asynchrones, attribuable sans doute aux travaux d'[[Ivan Sutherland]] et d'Alain Martin<ref>{{ps}} {{ouvrage|titre=Algorithmes de division pour les circuits asynchrones|prénom1=Nicolas|nom1=Boullis|année=2001|lire en ligne=ftp://ftp.ens-lyon.fr/pub/LIP/Rapports/DEA/DEA2001/DEA2001-04.ps.Z}}</ref>. 

En 1989, Ivan Sutherland introduit le concept de ''{{langue|en|micropipeline}}''<ref name="micropipelinesutherland">{{pdf}} Dans l'article {{article |titre=Micropipelines |prénom1=Ivan E. |nom1=Sutherland |périodique=Communications of the ACM |mois=juin |année=1989 |volume=32 |numéro=6 |pages=19 |langue=en |lire en ligne=http://f-cpu.seul.org/new/micropipelines.pdf }}</ref>, implémentation élégante<ref>[http://ieeexplore.ieee.org/Xplore/login.jsp?url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel2%2F436%2F5869%2F00224446.pdf%3Farnumber%3D224446&authDecision=-203 Présentation d'un article sur le site de l'IEEE]</ref> du protocole deux-phases à données groupées. Celui-ci est ensuite sans cesse amélioré, grâce à d'autres implémentations ou à l'utilisation de protocoles à quatre phases.

Cette période voit également l'apparition du premier microprocesseur asynchrone<ref>{{harvsp|Vivet|2001|p=32|réf=Aspro}}</ref>, le ''{{lang|en|Caltech Asynchronous Microprocessor}}'' conçu à [[Caltech]] par l'équipe d'Alain Martin en 1989. Il utilise des [[#Circuits quasi-insensibles aux délais|circuits quasi-insensibles aux délais]] ({{abréviation|QDI|Quasi-delay-insensitive|en}}) générés à partir de code CHP et possède une architecture [[Reduced instruction set computer|RISC]]<ref name="CAP">{{PDF}} {{ouvrage|titre=The Design of an Asynchronous Microprocessor|prénom1=Alain J.|nom1=Martin|prénom2=Steven M.|nom2=Burns|prénom3=T. K.|nom3=Lee|prénom4=Drazen|nom4=Borkovic|prénom5=Pieter J.|nom5=Hazewindus|année=1989|langue=en|lire en ligne=http://authors.library.caltech.edu/26709/1/postscript.pdf}}</ref>.

Il est suivi par les différentes versions de l'[[AMULET (processeur)|AMULET]], basées sur les ''{{langue|en|micropipelines}}'', qui reprennent l'[[architecture ARM]]. La deuxième version introduit les ''{{langue|en|micropipelines}}'' à quatre phases<ref name="fourphase"/>. Un autre microprocesseur QDI de [[Caltech]], le MiniMIPS, considéré comme particulièrement performant<ref>{{harvsp|Vivet|2001|p=34|réf=Aspro}}</ref>{{,}}<ref name="MiniMIPSperf"/>, est conçu et testé à la fin des années 1990. D'autres sont conçus dans des universités de par le monde comme le TITAC 1 en 1994, le TITAC 2 en 1997 (architecture [[RISC]], modèle proche du modèle QDI)<ref>{{harvsp|Vivet|2001|p=33|réf=Aspro}}</ref> ou l'Aspro (RISC et QDI), en 2001 à [[Institut National Polytechnique de Grenoble|Grenoble]]<ref>Sujet de la thèse : {{harvsp|Vivet|2001|réf=Aspro}}</ref>.

En parallèle, de nouvelles méthodes apparaissent : le codage quatre-états, plus efficace que les autres codages double-rail deux-phases, est inventé indépendamment par deux équipes en 1991 et 1992<ref name="DRDP"/> ; les différents pipelines QDI apparaissent et sont utilisés dans les microprocesseurs ; des pipelines potentiellement plus efficaces, faisant des [[#Hypothèses de conception|hypothèses temporelles]], sont proposés<ref name="PS0"/>.

=== Les premières commercialisations ===
À la fin des années 1990, des applications commerciales recommencent à voir le jour : de nombreuses entreprises commencent à utiliser des circuits basés sur une approche asynchrone ou à faire des recherches en ce sens. Parmi elles, [[Philips]] fait figure de précurseur en mettant en œuvre des outils de synthèse de circuits asynchrones utilisant le langage Tangram dès 1995, et ce grâce aux outils de l'entreprise {{lang|en|Handshake Solutions}}<ref name="entreprises"/> ; la société dispose de [[microcontrôleur|microcontrôleurs]] [[Intel 8051|80C51]] asynchrones dès 1998<ref name="80C51">{{PDF}} {{ouvrage|titre=An Asynchronous Low-Power 80C51 Microcontroller|année=1998|prénom1=Hans|nom1=van Gageldonk|langue=en|lire en ligne=http://alexandria.tue.nl/extra3/proefschrift/boeken/9802299.pdf}}</ref>. Des chercheurs de [[Sun Microsystems]], [[Intel]] et [[IBM]] prennent aussi part à cet engouement<ref name="entreprises">{{PDF}} {{ouvrage|titre=The Status of Asynchronous Design in Industry|langue=en|mois=juin|année=2004|numéro d'édition=3|prénom1=D. A.|nom1=Edwards|prénom2=W. B.|nom2=Tom|lire en ligne=http://www.bcim.lsbu.ac.uk/ccsv/ACiD-WG/AsyncIndustryStatus.pdf}}</ref>. De nombreuses [[start-up]]s exploitent les avantages de ces circuits, comme {{lang|en|Theseus Logic}} dès 1996<ref group=note>Fondée en 1990 selon la [http://www.theseusresearch.com/company.htm page web de l'entreprise]</ref>, Fulcrum en 2000<ref name="entreprises"/>, ou plus tard Tiempo<ref>[http://www.tiempo-ic.com/company/technology.html Page de présentation de l'entreprise]</ref>, ou encore {{lang|en|GreenArrays}}.

=== Des avancées récentes ===
La recherche autour des circuits asynchrones se poursuit durant les années 2000 : de nouvelles implémentations de pipelines sont proposées tant multi-rails, comme LP2/1 en 2000<ref name="LP21">{{pdf}} {{ouvrage|titre=High-Throughput Asynchronous Pipelines for Fine-Grain Dynamic Datapaths|prénom1=Montek|nom1=Singh|prénom2=Steven M.|nom2=Nowick|langue=en|année=2000|mois=avril|lire en ligne=http://www.cs.unc.edu/~montek/pubs/singh-nowick-async2000.pdf}}</ref>, qu'à données groupées, comme {{abréviation|IPCMOS|Interlocked Pipeline CMOS|en}} en 2000<ref name="pipelinescomp"/><!--Pas l'article d'origine, mais celui-ci semble ne pas être en accès libre--> ou MOUSETRAP en 2007<ref name="mousetrap">{{pdf}} {{ouvrage|titre=MOUSETRAP: High-Speed Transition-Signaling Asynchronous Pipelines|prénom1=Montek|nom1=Singh|prénom2=Steven M.|nom2=Nowick|langue=en|lire en ligne=http://www.cs.unc.edu/~montek/pubs/mousetrap-tvlsi-jun-2007.pdf|mois=Juin|année=2007|commentaire=Publié dans ''IEEE Transactions on Very Large Scale Integration Systems'', volume 15}}</ref> ; les protocoles dits « ''{{langue|en|single-track}}'' », utilisant un même fil pour la requête et l'acquittement, apparaissent<ref>Apparemment présenté pour la première fois (en interprétant {{harvsp|Dinh-Duc|2003|p=64|réf=synQDI}}) dans {{ouvrage|titre=Single-Track Handshake Signaling with Application to Micropipelines and Handshake Circuits|année=1996|commentaire=Non trouvé en accès libre|prénom1=Kees|nom1=van Berkel|prénom2=Arjan|nom2=Bink|langue=en}}<!-- I hate IEEE --></ref>, avec des implémentations comme {{abréviation|STFB|Single-Track Full-Buffer|en}}<ref name="STFB">{{pdf}} {{ouvrage|prénom1=Marcos |nom1=Ferretti |prénom2=Peter A. |nom2=Beerel |titre=Single-Track Asynchronous Pipeline Templates Using 1-of-N Encoding |langue=en |lire en ligne=http://www.date-conference.com/proceedings/PAPERS/2002/DATE02/PDFFILES/10B_2.PDF|éditeur=University of Southern California|année=2002}}</ref> et GasP<ref name="GasP">{{pdf}} {{ouvrage|titre=GasP: A Minimal FIFO Control|prénom1=Ivan E.|nom1=Sutherland|prénom2=Scott|nom2=Fairbanks|langue=en|année=2001|lire en ligne=http://www.cs.unc.edu/~montek/teaching/spring-04/sutherland-gasp-async2001.pdf|éditeur=Sun Microsystems Laboratories}}</ref>.

En 2004, [[Seiko Epson|Epson]] crée un microcontrôleur [[Circuit imprimé flexible|flexible]], qui est le premier du genre à être asynchrone<ref>[http://fr.ubergizmo.com/2005/02/epson-deacuteveloppe-le-premier-processeur-flexible/ Article en français et annonce de presse d'origine]</ref>.

==Caractéristiques==
Les circuits asynchrones possèdent plusieurs propriétés potentiellement intéressantes<ref>Ces avantages sont décrits dans de nombreux articles relatifs à l'électronique asynchrone, comme {{harvsp|Vivet|2001|p=16-18|réf=Aspro}},{{harvsp|Sparsø, Furber|2001|p=3-4|réf=asyncdesign}} et {{harvsp|Hauck|1995|p=1-2|réf=overview}}</ref> : ils sont particulièrement prometteurs dans des domaines où la fiabilité est requise et pour des circuits à basse consommation, voire pour leur vitesse, bien qu'aucune méthode de conception ne possède tous ces avantages simultanément : il existe de très nombreuses façons de les concevoir, chacune avec ses avantages et ses problèmes.

===Consommation d'énergie===
Un [[signal d'horloge]] change d'état en permanence<ref group=note>Une optimisation pour les circuits synchrones, appelée « {{lang|en|[[clock gating]]}} », consiste à l'arrêter ou à le ralentir lorsque le circuit est inactif pour une « longue période ».</ref> : il peut représenter une part importante de la consommation d'un circuit<ref name="consoW"/>. À contrario, les circuits asynchrones ne sont actifs que lorsque des données sont disponibles. Dans le cas contraire, aucun [[transistor]] ne commute et la seule puissance consommée est due à leurs courants de fuite<ref group=note>Il s'agit de la distinction entre consommation dite « dynamique » et « statique » : la première, consommation d'un circuit qui change d'état, est due au chargement des fils, aux fuites et aux courts-circuits lors des commutations ; la seconde, consommation d'un circuit stable, n'est due qu'au fait que les transistors ne sont pas parfaits et présentent des fuites, et est généralement bien plus faible</ref>, ce qui en fait des candidats crédibles pour des circuits à basse consommation, ou lorsque la charge de travail évolue rapidement<ref name="consoW">{{harvsp|Vivet|2001|p=16-18|réf=Aspro}}</ref>. Cependant, pour assurer la communication entre les éléments, ils utilisent aussi plus de transistors et plus de fils pour une même quantité de données que leurs équivalents synchrones, ce qui peut compenser les gains énergétiques et augmente la taille des puces.

Certains circuits ont néanmoins montré des gains substantiels, avec une efficacité énergétique quatre fois supérieure à un équivalent synchrone pour une implémentation de microcontrôleur 80C51<ref name="80C51"/>, ou proche des autres [[Architecture ARM|processeurs ARM]] du moment pour les processeurs [[AMULET (processeur)|AMULET]].

===Fiabilité===
Selon la méthodologie de conception employée, il est possible de créer des circuits utilisant peu d'hypothèses temporelles (insensibles ou quasi-insensibles aux délais par exemple) : ces circuits ont un comportement correct même si les propriétés physiques du circuit évoluent (à cause de la température<ref>La vitesse des circuits asynchrones [http://www.async.caltech.edu/filter.html varie avec la température] sans nécessiter de réglage, puisque les délais des portes, et non la fréquence d'une horloge, déterminent leur vitesse ; des circuits insensibles ou quasi-insensibles aux délais toléreront mieux ces variations.</ref>, de la tension d'alimentation<ref>Comme démontré [http://www.async.caltech.edu/cam.html avec humour] sur le premier microprocesseur asynchrone</ref> ou d'un changement de technologie de fabrication)<ref>{{harvsp|Vivet|2001|p=19-20|réf=Aspro}} et {{harvsp|Hauck|1995|p=2|réf=overview}}</ref>.

Les circuits asynchrones sont sensibles à tous les changements d'états des fils, et non à des signaux stabilisés échantillonnés lors de commutations de l'horloge<ref>{{harvsp|Davis et Nowick|1997|p=13-17|réf=intro}}</ref> : on parle d'« [[aléa]]s » pour désigner les variations indésirables des signaux<ref group=note>On classe souvent les aléas selon ce qui les provoque : ils peuvent être combinatoires ou séquentiels, logiques ou dynamiques</ref>{{,}}<ref>{{harvsp|Dinh-Duc|2003|p=15-19|réf=synQDI}}</ref>. De tels problèmes sont à prendre en compte à la conception<ref name="synaléas>{{harvsp|Dinh-Duc|2003|p=9|réf=synQDI}}</ref>. Cela leur donne aussi des comportements différents des circuits synchrones en cas d'erreurs ou de [[Parasite (électricité)|parasites]]. De plus, certains protocoles asynchrones (focalisés sur la performance plutôt que la fiabilité) font de nombreuses suppositions sur le comportement des circuits (principalement sur les délais) qui ne sont pas faciles à réaliser en pratique ou peuvent être infirmées par des variations de taille et de performance des transistors.

La conception d'un circuit synchrone demande quant à elle une connaissance précise des délais des portes, mais un délai trop élevé peut être compensé par une diminution de la fréquence d'horloge<ref>{{harvsp|Vivet|2001|p=28|réf=Aspro}}</ref>, au prix d'une baisse de la vitesse globale du circuit toutefois. Un autre problème récurrent est le phénomène de [[Signal_d'horloge#Gigue_d.27horloge|gigue d'horloge]], et en général le problème de la distribution du signal d'horloge dans les circuits, qui compliquent la montée en fréquence et dont la correction nécessite l'utilisation de techniques complexes, parfois énergivores<ref>{{harvsp|Davis et Nowick|1997|p=3-5|réf=intro}}</ref>.

===Vitesse===
En [[électronique numérique]], la vitesse peut être caractérisée de deux manières différentes<ref>{{harvsp|Vivet|2001|p=13|réf=Aspro}} et {{harvsp|Sparsø, Furber|2001|p=47|réf=asyncdesign}}, le temps de cycle étant lié au débit (parfois appelé bande passante par traduction de l'anglais « {{langue|en|bandwidth}} »)</ref>{{,}}<ref group=note>Alors que le temps de cycle est uniforme dans les circuits synchrones, ce n'est pas le cas des circuits asynchrones, et d'autres outils sont nécessaires pour quantifier les performances, par exemple dans le cadre de l'optimisation. C'est le cas pour des circuits formant des boucles, des fourches, ou ayant des délais variables ; dans de tels circuits, le débit et la latence globaux ne sont pas simples à déduire à partir des propriétés locales. On pourra lire les références et le paragraphe de l'article consacré à l'optimisation</ref>{{,}}<ref name="analysis-opt">{{PDF}} {{ouvrage|titre=Analysis and Optimization for Pipelined Asynchronous Systems|prénom1=Genette D.|nom1=Gill|année=2010|lire en ligne=http://www.cs.unc.edu/cms/publications/dissertations/gill.pdf|commentaire=Thèse présentée en 2010 en vue de l'obtention d'un doctorat de l'[[université de Caroline du Nord]]|langue=en}}</ref> : la [[lag (informatique)|latence]], qui correspond au temps que met une donnée pour être traitée par le circuit, et le [[Débit_binaire|débit]], qui est le nombre de données traitées par unité de temps.

Les circuits permettant de passer des données d'un élément à l'autre peuvent introduire une latence supplémentaire pour traiter les signaux de requête et d'acquittement, diminuant le débit et augmentant la latence.

Par contre, la [[lag (informatique)|latence]] comme le [[Débit_binaire|débit]] ne sont pas limités par un [[signal d'horloge]] global, et ne sont pas forcément constants selon les parties du circuit ou les données en entrée. Par exemple, pour des circuits logiques tels que les [[Additionneur|additionneurs]]<ref>Des additionneurs et des multiplieurs utilisant ce principe sont décrits dans {{harvsp|Vivet|2001|p=159-171|réf=Aspro}} ; c'est une technique largement utilisée pour la propagation des retenues</ref>{{,}}<ref name="adders">{{pdf}} {{ouvrage |titre=Minimal Energy Asynchronous Dynamic Adders |prénom1=Ilya |nom1=Obridko |prénom2=Ran |nom2=Ginosar |langue=en |année=2006 |lire en ligne=http://webee.technion.ac.il/~ran/papers/Low%20Energy%20Adders%20Obridko%20Ginosar%20TR.pdf |éditeur=Israel Institute of Technology |pages=16 |commentaire=Une version raccourcie de cet article a été acceptée pour publication par la revue ''IEEE Trans. On VLSI'' en 2006}}</ref>, certaines implémentations asynchrones peuvent renvoyer le résultat dès qu'il est calculé (ici, la retenue), alors qu'un circuit synchrone doit toujours attendre jusqu'au signal d'horloge suivant (qui doit être suffisamment tard pour que les calculs soient terminés même dans le pire des cas). Ils permettent donc de bonnes implémentations en temps moyen de calcul, bien que le pire des cas puisse être bien plus long<ref>{{harvsp|Vivet|2001|p=14-15|réf=Aspro}}</ref>.

Certains microprocesseurs asynchrones fabriqués exhibent ainsi de très bonnes performances, la vitesse du MiniMIPS étant environ égale à quatre fois celle de processeurs équivalents pour une efficacité énergétique proche<ref name="MiniMIPSperf">{{ps}} {{ouvrage|titre=Speed and Energy Performance of an Asynchronous MIPS R3000 Microprocessor|langue=en|prénom1=Alain J.|nom1=Martin|prénom2=Mika|nom2=Nyström|prénom3=Paul|nom3=Penzes|prénom4=Catherine|nom4=Wong|année=2001|jour=22|mois=juin|lire en ligne=http://authors.library.caltech.edu/27033/0/2001_mipsresults.ps}}</ref>.

===Simplicité de conception===
Il est facile d'utiliser ensemble des circuits asynchrones différents (même utilisant des protocoles différents, en ajoutant un élément traduisant de l'un à l'autre), car aucun élément n'a besoin d'être commun, au contraire de l'horloge des circuits synchrones. Il est même possible de les interfacer avec des circuits synchrones, et d'interfacer des circuits synchrones entre eux par des liaisons asynchrones<ref>{{pdf}} {{ouvrage|titre=Practical Design of Globally-Asynchronous Locally-Synchronous Systems|prénom1=Jens|nom1=Muttersbach|prénom2=Thomas|nom2=Villiger|prénom3=Wolfgang|nom3=Fichtner|langue=en|éditeur=Swiss Federal Institute of Technology|lire en ligne=http://eecourses.technion.ac.il/048878/048878papers/Muttersbach-GALS-Async00.pdf}}</ref>{{,}}<ref group=note>On parle de circuits globalement asynchrones mais localement synchrones</ref>. Cette modularité est un avantage pour l'interfaçage avec d'autres éléments<ref>{{harvsp|Davis et Nowick|1997|p=5|réf=intro}}</ref>{{,}}<ref name="modular">{{harvsp|Vivet|2001|p=20-21|réf=Aspro}}</ref>, mais aussi pour la conception en général, qui s'en trouve facilitée<ref name="modular" /> et peut être faite avec des [[Langage de haut niveau|langages de haut niveau]].

L'absence d'une horloge unique et donc d'une base de temps globale simplifie grandement le processus, en évitant de devoir prendre en compte les délais et la distribution du signal d'horloge à toutes les échelles lors de la phase de conception<ref>{{harvsp|Vivet|2001|p=16|réf=Aspro}}</ref>, mais elle introduit d'autres problèmes qui doivent être pris en compte par les outils ou par le concepteur.

===Émissions électromagnétiques===
Du fait que les différents éléments ne sont pas synchronisés, les émissions électromagnétiques sont réduites tandis que la consommation électrique est lissée<ref name="conso">{{harvsp|Vivet|2001|p=18-19|réf=Aspro}}</ref> : cela peut être exploité pour se prémunir de [[Analyse de consommation (cryptographie)|certaines attaques exploitant celles-ci]]<ref>{{pdf}} {{Article |titre=Delay Insensitive Encoding and Power Analysis: A Balancing Act|prénom1=Konrad J. |nom1=Kulikowski |prénom2=Ming |nom2=Su |prénom3=Alexander |nom3=Smirnov |prénom4=Alexander |nom4=Taubin |prénom5=Mark G. |nom5=Karpovsky |prénom6=Daniel |nom6=MacDonald |langue=en |périodique= ASYNC |année=2005 |passage=116-125 |éditeur=IEEE |lire en ligne=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.64.920&rep=rep1&type=pdf}}</ref>, car il est alors plus difficile de déduire l'activité du circuit à partir de ses émissions ou de sa consommation. Cette propriété peut être exploitée dans tout environnement où les émissions électromagnétiques doivent être minimisées<ref name="conso"/>, par exemple au voisinage d'une [[Antenne radioélectrique|antenne]].

Un circuit synchrone, a contrario, a tendance à émettre des ondes harmoniques de sa fréquence d'horloge et à présenter des pics de puissance consommée à chaque front d'horloge<ref name="conso"/>.

===Pipelines élastiques===
Comme les synchronisations entre éléments se font au niveau local, les [[pipeline (électronique)|pipelines]] asynchrones sont naturellement élastiques<ref>{{harvsp|Vivet|2001|p=15|réf=Aspro}}</ref>, c'est-à-dire qu'ils peuvent contenir un nombre variable de données. C'est une conséquence du fait que celles-ci se propagent indépendamment de celles qui les suivent.

===Traitements non-déterministes===
L'absence de base de temps globale permet de traiter facilement des événements extérieurs<ref name="arb">{{harvsp|Vivet|2001|p=15-16|réf=Aspro}}</ref>. Un tel événement peut survenir au plus mauvais moment, par exemple en même temps qu'un signal d'horloge ou que l'arrivée de données, ce qui peut entraîner l'apparition d'un [[Bascule_(circuit_logique)#M.C3.A9tastabilit.C3.A9|état métastable]] dont on s'accommode difficilement en électronique synchrone<ref name="arb"/>. Pour des circuits asynchrones, en revanche, il existe des portes spécialisées permettant de réaliser un « arbitrage »<ref group=note>Le but est d'imposer un ordre entre deux événements potentiellement simultanés, ce qui est impossible en un temps borné ; par exemple, un arbitrage devra choisir quelle entrée sera relayée en premier à une sortie commune</ref>{{,}}<ref>{{harvsp|Sparsø, Furber|2001|p=77-80|réf=asyncdesign}}</ref> : il est possible d'attendre jusqu'à ce que l'état métastable soit résolu<ref>{{harvsp|Vivet|2001|p=15-16|réf=Aspro}}</ref>.

==Protocoles de communication==
[[Fichier:Bundled data protocol.svg|thumb|Communication asynchrone utilisant un protocole à données groupées : n fils pour n bits, un fil de requête et un fil d'acquittement]]
[[Fichier:Dual-rail protocol.svg|thumb|Avec un protocole double rail, où la requête est incluse dans les fils de données. On pourrait coder des nombres dans une autre base que le binaire en utilisant k*n fils.]]
[[Fichier:Single track protocols.svg|thumb|Des protocoles ''{{lang|en|single-track}}'' : façon GasP en haut, STFB en bas]]

Il existe de nombreux moyens de réaliser une interface asynchrone entre deux éléments<ref>{{harvsp|Dinh-Duc|p=13|réf=synQDI}}</ref>. Dans une communication à sens unique impliquant un émetteur et un destinataire, l'envoi des données est signalé par une requête, notée « Req », et leur réception par un acquittement, noté « Ack » : on parle d'une « poignée de main » pour désigner cet échange. Un envoi se déroule comme suit : lorsque ses données sont prêtes, l'émetteur les copie en sortie et en informe le destinataire via une requête ;  quand celui-ci les a transmises, copiées ou utilisées, il renvoie un acquittement pour signifier qu'il n'en a plus besoin. L'émetteur peut alors envoyer de nouvelles données<ref name="typesprotocoles"/>.

La requête et l'acquittement ne sont pas forcément des signaux à part ; ils peuvent être codés chacun sur un fil, mais ce n'est pas une nécessité : différents codages existent.

===Protocoles===
On distingue deux familles de [[Protocole de communication|protocoles]]<ref name="typesprotocoles">{{harvsp|Vivet|2001|p=8-10|réf=Aspro}}</ref> selon la façon dont les événement, tels que les requêtes et les acquittements, sont codés :
* soit ils correspondent à une transition quelconque, c'est-à-dire un changement de valeur sur un fil, de 1 à 0 ou de 0 à 1. On parle de protocole à deux phases, de « ''{{lang|en|half-handshake}}'' », de « ''{{langue|en|transition signalling}}'' » ou de codage NRZ (''{{lang|en|Non-return-to-zero}}'') : l'émetteur envoie les données et émet une requête, que le destinataire traite avant d'envoyer l'acquittement.
* soit l’émetteur et le destinataire remettent les signaux de requête et d'acquittement à leur état initial après avoir signalé la transmission et la réception des données par des transitions. On parle de protocole à quatre phases, de « ''{{lang|en|full-handshake}}'' » ou de codage RZ (''{{lang|en|Return-to-zero}}'') : après l'envoi des données et de la requête, puis leur réception et l'acquittement, l'émetteur ramène le signal de requête à son état initial, puis le destinataire fait de même avec le signal d'acquittement. Malgré une apparente complexité, ce protocole permet des implémentations souvent plus simples et rapides que le protocole à deux phases<ref name="fourphase">{{pdf}} {{ouvrage |titre=Four-Phase Micropipeline Latch Control Circuit |prénom1=Stephen B. |nom1=Furber |prénom2=Paul |nom2=Day |langue=en |lire en ligne=http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A82C3B3430D2507A018CBAF817EC29E1?doi=10.1.1.41.8360&rep=rep1&type=pdf}}</ref>.

Des protocoles plus complexes sont aussi utilisés. Très fréquemment, la requête est codée sur les mêmes fils que les données, ce qui donne les codages sur plusieurs fils présentés dans la section suivante. Certains protocoles, moins répandus, n'ont pas de fil séparé pour l'acquittement et utilisent un ou des fils sur lesquels l'émetteur comme le destinataire peuvent agir<ref group=note>De tels protocole ont été proposés pour différents codages</ref>{{,}}<ref name="STFB"/>{{,}}<ref name="GasP"/> : de cette manière, il n'y a que deux phases (envoi, puis remise à zéro après réception), et moins de fils de communication. D'autres protocoles ont été proposés : basé sur des impulsions, c'est-à-dire une remise à zéro précoce des fils<ref>{{PDF}} {{ouvrage|titre=Asynchronous Pulse Logic|prénom1=Mika|nom1=Nyström|jour=14|mois=Mai|année=2001|langue=en|éditeur=California Institute of Technology|commentaire=Thèse présentée en 2001 en vue de l'obtention d'un doctorat de [[université de Californie|Berkeley]]|lire en ligne=http://authors.library.caltech.edu/26921/0/tr_main.ps}}</ref>, ou supprimant les [[Bascule (circuit logique)|bascules]]<ref group=note>On en trouve différents exemples dans la littérature, bien qu'ils ne possèdent pas toutes les propriétés des autres circuits asynchrones et que de tels ''{{lang|en|wave pipelines}}'' soient aussi utilisés en synchrone</ref>{{,}}<ref>{{pdf}} Par exemple {{ouvrage|titre=Fault Tolerant Clockless Wave Pipeline Design|langue=en|prénom1=T.|nom1=Feng|prénom2=B.|nom2=Jin|prénom3=J.|nom3=Wang|prénom4=N.|nom4=Park|prénom5=Y.B.|nom5=Kim|prénom6=F.|nom6=Lombardi|lire en ligne=http://www.ece.neu.edu/groups/hpvlsi/publication/FAULT_CKLESS_ACMCF.pdf}} et {{ouvrage|titre=Asynchronous Wave Pipelines for High Throughput Dynamic Datapaths|langue=en|prénom1=O.|nom1=Hauck|prénom2=S. A.|nom2=Huss|éditeur=Darmstadt University of Technology|lire en ligne=http://www.iss.tu-darmstadt.de/publications/downloads/hauck98b.pdf}}</ref>, voire utilisant des signaux non binaires, avec plus de deux tensions signifiantes<ref name="DRDP"/>.

===Codage des données===
Dans les circuits asynchrones, il existe de nombreux moyens de coder les données. Le codage le plus évident, similaire à celui des circuits synchrones, utilise un fil pour un [[bit]]<ref group=note>Parfois, pour des raisons de performances ou de fiabilité, on code néanmoins sur deux fils, codant le bit et son complémentaire</ref> ; en asynchrone, on l'appelle « codage à données groupées »<ref name="bundled data">{{harvsp|Vivet|2001|p=11|réf=Aspro}}</ref>. Cependant, comme toute transmission de données asynchrone s'accompagne d'une requête permettant de la signaler, une autre technique est de coder ensemble les données et la requête, comme par exemple dans les codages « double-rail », sur deux fils, qui sont très utilisés.

====Codage à données groupées====
Dans ce protocole, un ou plusieurs fils transportent les données, avec un fil par [[bit]]. Un fil (Req) est destiné à la requête de l'émetteur indiquant que les données sont prêtes, et un autre (Ack) à la réponse du destinataire. De tels circuits sont conçus en utilisant un modèle où certains délais sont considérés comme bornés, car sinon les données pourraient ne pas être valides alors même que la requête serait arrivée<ref name="bundled data"/>.

On parle souvent de ''{{langue|en|micropipelines}}'' pour désigner les circuits utilisant ce codage, avec soit deux soit quatre<ref name="fourphase"/> phases. À l'origine, ce terme fait référence à un pipeline asynchrone utilisant un protocole deux-phases données-groupées<ref name="micropipelinesutherland"/>.

====Codage sur plusieurs fils====
[[Fichier:Protocole 3 états.svg|thumb|Les états et les transitions pour le codage trois états|upright=0.7]]
[[Fichier:Protocole 4 états.svg|thumb|Les états et les transitions pour le codage quatre états|alt=4 états, 00, 01, 10 et 11, deux codant le 0 et deux le 1, de telle manière qu'il soit toujours possible d'atteindre l'un des deux en ne changeant que l'un des deux bits.|upright=0.7]]

Ici, il n'y a pas de fil séparé destiné à l'envoi de la requête, qui est implicite<ref>Le fil d'acquittement est présent, sauf protocole particulier</ref>{{,}}<ref>{{harvsp|Vivet|2001|p=10-11|réf=Aspro}}</ref>. De manière générale, il existe des codages complexes dits « m parmi n », dont on n'utilise surtout qu'un cas particulier dit « double rail », avec deux fils. Dans ces codages, une donnée est représentée par m transitions parmi n fils, et le destinataire peut considérer que les données sont valides dès lors que m transitions ont eu lieu, ce qui rend le codage lui-même insensible aux délais<ref name="DIcodesDinh">{{harvsp|Dinh-Duc|p=11-12|réf=synQDI}}</ref>{{,}}<ref name="DIcodes">{{ps}} {{ouvrage|titre=Delay-Insensitive Codes—An Overview|prénom1=Tom|nom1=Verhoeff|année=1987|mois=janvier|lire en ligne=http://alexandria.tue.nl/extra1/wskrap/publichtml/8837761.pdf|langue=en}}</ref>{{,}}<ref group=note>Ce n'est pas le cas des implémentations ; celles-ci sont souvent quasi-insensibles aux délais, en utilisant une fourche isochrone sur le signal d'acquittement, mais l'interface entre deux éléments sera insensible aux délais</ref>. Un tel codage est approprié tant pour un protocole à quatre phases, où on remet tous les fils à leur état initial après chaque échange, que pour un protocole à deux phases.

Le type le plus simple de codage m parmi n est le codage 1 parmi n, dit aussi « {{langue|en|''[[Encodage one-hot|one-hot]]''}} » : dans ce cas, il y a n transitions possible, ce qui permet de représenter un chiffre en [[Base (arithmétique)|base]] n, le cas particulier du double-rail donnant une écriture binaire. Bien sûr, d'autres codages sont possibles<ref name="DIcodes"/>, mais peu usités, comme les autres codages m parmi n.

Les protocoles double-rail sont largement répandus<ref group=note>Ils sont d'ailleurs parfois les seuls cités, comme dans {{harvsp|Vivet|2001|p=10-11|réf=Aspro}}</ref>{{,}}<ref name="DIcodesDinh"/>. Le protocole à quatre phases, aussi dit « codage trois-états », est le plus populaire<ref>{{harvsp|Vivet|2001|p=11|réf=Aspro}}</ref>. Il comporte une valeur invalide (typiquement 00), et deux valeurs signifiantes (01 codant 0 et 10 codant 1 par exemple), l'état 11 étant inutilisé. L'émetteur repasse par l'état invalide à chaque envoi de données. Pour signifier que le destinataire a remarqué le changement de valeur, une réponse est émise à chaque fois, y compris au passage par l'état invalide, ce qui permet au signal d'acquittement de revenir aussi à son état initial. Dans le protocole à deux phases, une seule transition a lieu à chaque envoi, et elle est acquittée par une seule transition également<ref>{{harvsp|Vivet|2001|p=9-10|réf=Aspro}}</ref>.

Enfin, un dernier codage double rail existe, destiné à un protocole à deux phases : tous les états sont signifiants, mais deux codes correspondent à chaque valeur d'un bit, ce qui permet bien de changer d'état à chaque nouvelle donnée ; on parle de ''codage quatre-états''<ref group=note>On appelle aussi ce codage LEDR pour {{Citation étrangère|lang=en|Level Encoded Dual-Rail}}</ref>{{,}}<ref name="DRDP">{{PDF}} {{ouvrage|titre=Time-Multiplexed Dual-Rail Protocol for Low-Power Delay-Insensitive Asynchronous Communication|prénom1=Marco|nom1=Storto|prénom2=Roberto|nom2=Saletti|langue=en|lire en ligne=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.40.6543&rep=rep1&type=pdf}}</ref>.

==Conception de circuits asynchrones==
=== Hypothèses de conception ===
Les circuits asynchrones regroupent en fait plusieurs classes de circuits<ref>{{harvsp|Vivet|2001|p=21-24|réf=Aspro}}, {{harvsp|Hauck|1995|p=3-22|réf=overview}} et {{harvsp|Sparsø, Furber|2001|p=25|réf=asyncdesign}}}</ref> ayant des propriétés différentes, selon les hypothèses faites à la conception. En effet, la communication locale des circuits asynchrones peut permettre de s'affranchir de certaines contraintes de temps, les éléments indiquant eux-mêmes la disponibilité des données. Ces propriétés s'échelonnent de l'insensibilité aux délais (on parle de circuits {{abréviation|DI|Delay-insensitive|en}}), où le circuit est correct quels que soient les délais des portes et des fils, au modèle des délais bornés, où les délais ont une limite connue. Deux classes de circuits intermédiaires sont utilisées : quasi-insensibles aux délais ({{abréviation|QDI|Quasi-delay-insensitive|en}}) et indépendants de la vitesse ({{abréviation|SI|Speed-independent|en}}).

Pour simplifier, ces modèles considèrent les signaux comme booléens<ref name="QDI-Turing"/>, les portes et les fils apportant juste un retard de propagation. En réalité, les délais sont parfois plus complexes que de simples retards<ref group=note>Ces modèles ne prennent en compte que des « délais purs », opposés aux « délais inertiels » qui eux considèrent des variations lissées réalistes : les impulsions trop courtes peuvent alors ne pas être transmises, et les transitions ne sont pas instantanées</ref>, les signaux sont en réalité des tensions et non des valeurs binaires<ref group=note>Avec tous les effets parasites que cela implique : bruit, oscillations, [[Transistor_à_effet_de_champ_à_grille_métal-oxyde#Tension_de_seuil|tensions de seuil]] des transistors, effets [[Capacité_électrique|capacitifs]], [[Résistance_(électricité)#La_propri.C3.A9t.C3.A9_physique|résistifs]], [[Propagation_des_ondes|propagatifs]], etc.</ref>, et les portes elles-mêmes sont souvent implémentées en utilisant un modèle de délais bornés.

==== Circuits insensibles aux délais ====
De tels circuits fonctionnent correctement quels que soient les délais dans les fils et les portes. Ils ne sont cependant pas réalisables avec des portes logiques simples (les seules portes logiques à une sortie utilisables sont les [[Porte C|portes C]] et l'inverseur)<ref name="lims">{{pdf}} {{ouvrage |titre=The Limitations to Delay Insensitivity in Asynchronous Circuits |prénom1=Alain J.|nom1=Martin |pages=20 |année=1990 |mois=janvier |langue=en |lire en ligne=http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA447737&Location=U2&doc=GetTRDoc.pdf}}, cité dans {{harvsp|Sparsø, Furber|2001|p=25|réf=asyncdesign}} et {{harvsp|Hauck|1995|p=13|réf=overview}}</ref>, et ne sont donc pas souvent utilisés en pratique. Ce modèle est cependant utilisable avec des portes plus complexes, elles-mêmes implémentées suivant d'autres hypothèses.

==== Circuits quasi-insensibles aux délais ====
Les circuits quasi-insensibles aux délais ({{abréviation|QDI|Quasi-delay-insensitive|en}}) ajoutent l'hypothèse, pour la conception, qu'il est possible d'obtenir des « fourches isochrones », c'est-à-dire une séparation d'un fil en plusieurs possédant exactement le même délai, supposition qui est généralement considérée comme réalisable en pratique<ref group=note>Du fait que les portes suivant la fourche ont elles-mêmes un délai, ce qui permet de ne le vérifier qu'approximativement. Voir {{harvsp|Vivet|2001|p=23|réf=Aspro}}</ref>, mais présente certains risques, car les délais des fils peuvent être longs en comparaison de ceux des portes avec les technologies [[Complementary metal oxide semi-conductor|CMOS]]<ref group=note>D'autant plus avec les progrès de la miniaturisation</ref>{{,}}<ref name="bewareIF">{{pdf}} {{ouvrage|titre=Beware the isochronic fork|langue=en|prénom1=Kees|nom1=van Berkel|année=1991|mois=Janvier|lire en ligne=http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=380B231B55BB4F45F6E4B72D4D273D44?doi=10.1.1.72.3108&rep=rep1&type=pdf|commentaire=Publié dans la revue ''Integration, the VLSI Journal'', volume 13, en Juin 1992}}</ref>, et car la propagation des signaux est plus complexe que de simples délais sur des signaux binaires. C'est pourtant un modèle de conception largement utilisé, car de tels circuits sont [[Turing-complet|Turing-complets]]<ref name="QDI-Turing">{{PDF}} {{ouvrage|titre=Quasi-delay-insensitive circuits are Turing-complete|langue=en|prénom1=Rajit|nom1=Manohar|prénom2=Alain J.|nom2=Martin|année=1995|mois=novembre|jour=17|lire en ligne=http://vlsi.cornell.edu/~rajit/ps/qdi.pdf}}</ref>.

==== Circuits indépendants de la vitesse ====
On peut aussi considérer les délais dans les fils comme étant négligeables. Cela se vérifie difficilement dans les systèmes actuels<ref name="SI1">{{harvsp|Vivet|2001|p=23-24|réf=Aspro}} et {{harvsp|Hauck|1995|p=21-22|réf=overview}} ; pour une explication plus poussée, voir l'article {{lien web|titre=3D Integration : A Revolution in Design|url=http://realworldtech.com/page.cfm?ArticleID=RWT050207213241&p=2|site=Real World Technologies|langue=en}}</ref>, c'est pourquoi on peut leur préférer des circuits conçus comme quasi-insensibles aux délais. Cependant, les deux hypothèses sont en fait très proches<ref name="SI1" />, le cas indépendant de la vitesse revenant à considérer toutes les fourches comme isochrones. Malgré ce défaut apparent, cette hypothèse de conception est donc très répandue<ref group=note>Elle est utilisée par l'outil « {{lang|en|Petrify}} »</ref>.

==== Circuits à délais bornés ====
Comme l'indique leur nom, les délais dans les portes et les fils sont supposés connus ou bornés par une durée connue. On les appelle parfois circuits de [[David Albert Huffman|Huffman]]. C'est le principe de conception en électronique synchrone, où les impulsions d'horloge se font à intervalle suffisamment long pour que les signaux se soient propagés et stabilisés dans tout le circuit. En électronique asynchrone, ce modèle est aussi utilisé, tout d'abord pour concevoir la structure interne de nombreuses portes complexes vues comme des [[Automate fini|automates finis]]<ref>{{harvsp|Hauck|1995|p=3-10|réf=overview}}</ref>, mais également dans les {{langue|en|micropipelines}}<ref group=note>Le circuit opérant la poignée de main est souvent insensible au délai, alors que les bascules sont construites sur un modèle de délais bornés</ref>{{,}}<ref>{{harvsp|Hauck|1995|p=10-11|réf=overview}}</ref>, ou pour obtenir des circuits particulièrement performants<ref group=note>C'est le cas de presque tous les circuits simulés dans {{harvsp|Shojaee, Gholipour, Nourani|2006|réf=pipelinecomp}}</ref>.

On distingue parfois les micropipelines des autres circuits à délais bornés, en les définissant comme des circuits possédant une partie de contrôle insensible aux délais envoyant les signaux d'acquittement et de requêtes, et des bascules de mémorisation construites selon un modèle de délais bornés<ref>{{harvsp|Vivet|2001|p=25-26|réf=Aspro}}</ref>, ce qui en fait une méthode de conception à part entière. Selon cette définition, les micropipelines ne regroupent donc pas tous les circuits utilisant un protocole à données groupées, car les structures de contrôle peuvent être aussi à délais bornés.

==== Contraintes sur l'environnement ====
La conception peut être simplifiée en imposant certaines caractéristiques à l'environnement extérieur du circuit, soit en lui imposant des contraintes temporelles, soit en restreignant le nombre d'entrées qui sont autorisées à changer simultanément. Le circuit fonctionne en {{citation|mode fondamental}} lorsque les entrées ne changent que lorsque le circuit a atteint un état stable<ref>{{PDF}} {{ouvrage|titre=Digital Principles and Design|sous-titre=Chapter 18, Lesson 1: Fundamental Mode Sequential Circuits|lire en ligne=http://www.dauniv.ac.in/downloads/Digitalsystems_PPTs/DigDesignCh18L01.pdf|année=2006|prénom1=Raj|nom1=Kamal|langue=en}}</ref>{{,}}<ref name="modes">{{harvsp|Kishinevsky, Lavagno, Vanbekbergen|1995|p=29|réf=sysdesign}}</ref>, et en {{citation|mode entrée-sortie}} si elles peuvent changer dès que le circuit a répondu par un changement d'état<ref name="modes"/>.

===Automatisation de la conception et langages===
Comme les seules choses qui importent dans un circuit asynchrone sont les interactions entre les éléments, on peut concevoir les circuits par [[Compilateur|compilation]] de langages de haut niveau<ref name="langages">{{harvsp|Sparsø, Furber|2001|p=123|réf=asyncdesign}}, {{harvsp|Vivet|2001|p=43-53|réf=Aspro}}</ref> ou par une description de son comportement, par exemple à partir de [[Réseau de Petri|réseaux de Petri]]<ref>{{harvsp|Dinh-Duc|2003|p=35-37|réf=synQDI}}</ref>{{,}}<ref>{{harvsp|Sparsø, Furber|2001|p=86-114|réf=asyncdesign}}, {{harvsp|Vivet|2001|p=27-28|réf=Aspro}} et {{harvsp|Hauck|1995|p=22-28|réf=overview}}</ref>.

Différentes méthodologies et outils de synthèse ont été développés<ref>{{harvsp|Dinh-Duc|p=25-39|réf=synQDI}}</ref>, tant dans le milieu académique (Balsa<ref>{{harvsp|Dinh-Duc|p=30-31|réf=synQDI}}</ref>, {{citation|Caltech}}<ref>{{harvsp|Dinh-Duc|p=31-33|réf=synQDI}}</ref>, Minimalist<ref>{{harvsp|Dinh-Duc|p=37-39|réf=synQDI}}</ref>{{,}}<ref>{{PDF}} {{ouvrage|titre=MINIMALIST: An Environment for the Synthesis, Verification and Testability of Burst-Mode Asynchronous Machines|lire en ligne=http://www1.cs.columbia.edu/async/publications/minimalist-tech-report.pdf|prénom1=Robert M.|nom1=Fuhrer|prénom2=Steven M.|nom2=Nowick|prénom3=Michael|nom3=Theobald|prénom4=Niraj K.|nom4=Jha|prénom5=Bill|nom5=Lin|prénom6=Luis|nom6=Plana|année=1999|mois=juin|jour=26|langue=en}}</ref>, Petrify<ref>{{harvsp|Dinh-Duc|p=35-37|réf=synQDI}}</ref>) que dans l'industrie (Tangram<ref>{{harvsp|Dinh-Duc|p=29-30|réf=synQDI}}</ref>, Null Convention Logic<ref>{{harvsp|Dinh-Duc|p=33-35|réf=synQDI}}</ref>{{,}}<ref>{{PDF}} {{ouvrage|titre=NULL Convention Logic{{TM}}|éditeur=Theseus Logic Inc.|année=1997|prénom1=Karl M.|nom1=Fant|prénom2=Scott A.|nom2=Brandt|langue=en|lire en ligne=http://users.soe.ucsc.edu/~sbrandt/papers/NCL2.pdf}}</ref>).

==== Synthèse par traduction ou compilation ====
De nombreuses méthodologies de conception partent d'un langage de haut niveau qui est soit compilé, soit directement traduit en un circuit. De cette manière, la conception est relativement intuitive, et il est possible de construire un système complexe par assemblage de briques de base. Parmi ces langages, beaucoup sont dérivés de [[Communicating_sequential_processes|CSP]], qui reflète bien la [[Parallélisme (informatique)|concurrence]] : Tangram a été développé dans ce but chez [[Philips]], Balsa à l'[[université de Manchester]], CHP à [[Caltech]]<ref>{{harvsp|Dinh-Duc|2003|p=26|réf=synQDI}}</ref>. On peut aussi utiliser des langages de description matérielle plus classiques, comme [[Verilog]] ou [[VHDL]], qui n'y sont pas spécifiquement destinés<ref>{{harvsp|Vivet|2001|réf=Aspro}} présente une méthode de conception utilisant une traduction de [[CHP]] vers [[VHDL]]</ref>.

==== Synthèse à partir d'une spécification ====
D'autres méthodes de synthèse existent, basées elles sur une description du comportement du circuit. Elles permettent d'obtenir des circuits efficaces<ref name="STG-ASM">{{harvsp|Dinh-Duc|2003|p=26-27|réf=synQDI}}</ref>, mais au prix d'une conception plus complexe, tant du point de vue du programmeur que de celui de la complexité algorithmique<ref name="specsyn">{{harvsp|Rezzag|2004|p=30|réf=synMP}}</ref>. La spécification se fait sous forme de graphe : soit un [[réseau de Petri]] ou un réseau de Petri simplifié appelé graphe de transitions de signaux ({{abréviation|STG|Signal Transition Graph|en}}), soit un [[automate fini]] ({{abréviation|ASM|Asynchronous State Machine|en}})<ref name="STG-ASM"/>. Selon les outils, le circuit généré sera indépendant de la vitesse ou à délais bornés<ref name="specsyn"/>.

==== Vérification, analyse des performances et optimisation ====
Il est nécessaire d'être capable de vérifier que le circuit se comporte comme prévu : cela passe par une série de vérifications avant et après la production : il s'agit d'éviter d'une part les problèmes dus à des erreurs lors de la phase de conception, ce qui peut être fait par des preuves formelles et des simulations, et d'autre part ceux dus à des défauts lors de la fabrication qui peuvent rendre certains circuits fabriqués incorrects<ref name="analysis-opt"/>. Pour ces derniers, il est nécessaire d'élaborer des [[Protocole de test|protocoles de test]] applicables aux circuits produits.

L'optimisation des circuits asynchrones a donné lieu à de nombreuses recherches. Leurs propriétés sont très différentes des circuits synchrones : ils rendent impossibles certaines optimisations classiques, qui peuvent introduire des incertitudes temporelles<ref group=note>On cherchera à éviter qu'elles violent les hypothèses d'insensibilité aux délais, par exemple</ref> et des aléas, mais l'analyse des performances est aussi complètement différente<ref name="analysis-opt"/> du fait de l'absence d'un rythme commun : les éléments ont des délais variables et interagissent de manière complexe. L'optimisation, généralement apportée par l'outil de conception, utilise plusieurs transformations<ref>{{PDF}} {{ouvrage|titre=Performance Analysis and Optimization of Asynchronous Circuits|prénom1=Steven M.|nom1=Burns|lire en ligne=http://authors.library.caltech.edu/26733/1/postscript.pdf|année=1991|langue=en|passage=49-55}}</ref> à différents niveaux : au delà du choix de la technologie, il est possible d'optimiser à un niveau « abstrait » (changement de codage des données, ajout d'étages de pipelines, etc.) aussi bien qu'à un niveau local (remplacement de groupes de portes par des groupes équivalents). Ce processus est automatisé, et peut recourir à des simulations pour évaluer les performances d'un circuit.

===Problèmes liés à la conception===
La conception de circuits asynchrones souffre d'un manque d'outils dédiés<ref>{{lien web|url=http://www.eetimes.com/discussion/cole-bin/4024444/Will-Self-timed-Asynchronous-Logic-Rescue-CPU-Design-|titre=Will Self-timed Asynchronous Logic Rescue CPU Design?|auteur=Bernard Cole|année=2002|jour=24|mois=août}}</ref>, les principaux [[Langage de description de matériel|langages de description de matériels]] ciblant la conception de circuits synchrones, bien qu'il soit possible de les utiliser en électronique asynchrone.

Une autre limitation tient à la formation, qui est généralement focalisée sur l'électronique synchrone, l'électronique asynchrone étant moins répandue<ref>{{harvsp|Davis et Nowick|1997|p=2|réf=intro}}</ref> et souvent vue comme moins efficace ou plus complexe<ref name="CAP"/>.

Enfin, les problèmes de fiabilités dus entre autres à des contraintes temporelles sont aussi présents dans les circuits asynchrones<ref name="bewareIF"/>, et peuvent être particulièrement graves puisque les temps de propagation ne sont pas réglables comme avec une horloge<ref>{{harvsp|Hauck|1995|p=2|réf=overview}}</ref>. Ils sont surtout présents dans des circuits faisant beaucoup d'hypothèses temporelles (comme le modèle de délais bornés), mais existent aussi avec des modèles introduisant une certaine insensibilité aux délais<ref name="bewareIF"/> ; du fait des imperfections des techniques de fabrication, ils ne peuvent être évités que par des [[Simulateur logique|simulations]] en premier lieu, puis des tests après production<ref name="analysis-opt"/>.

=== Choix techniques et leurs conséquences ===
De nombreux critères entrent en ligne de compte pour évaluer les différents types de circuits asynchrones : leur vitesse en termes de latence et de débit, leur taille et leur consommation, mais aussi leur fiabilité. Ces caractéristiques varient beaucoup selon le protocole utilisé et l'implémentation choisie, mais aussi les technologies de fabrication et les optimisations appliquées au circuit. Cette section donne donc seulement un aperçu de l'impact de certains choix sur le circuit.

==== Deux ou quatre phases ? ====
Les protocoles à deux et à quatre phases ont chacun leurs avantages. À priori, les protocoles à deux phases impliquent moins de transitions, et devraient donc être plus rapides et plus économes. En pratique, cependant, un passage par un protocole à quatre phases est réalisé à un moment ou à un autre : c'est entre autres le cas de l'implémentation d'origine des micropipelines. De plus, il est plus simple d'implémenter les protocoles à quatre phases et leur nombre restreint d'états possibles<ref name="comp2-4">{{harvsp|Vivet|2001|p=9-10|réf=Aspro}}</ref>, particulièrement lorsqu'il faut effectuer des calculs en multi-rail.

De fait, les deux protocoles ont été utilisés pour le codage données-groupées. Par contre, les protocoles sur plusieurs rails utilisés sont plutôt de type retour à zéro<ref name="comp2-4"/>, sauf lorsque la communication se fait sur de longs fils, donc avec des délais et une consommation plus importants, et qu'il devient intéressant de restreindre le nombre de transitions<ref name="comp2-4"/>, comme dans le cas de [[Communication série|liens série]]<ref>{{harvsp|Vivet|2001|p=104-106|réf=Aspro}}</ref>.

==== Données groupées et multi-rail ====
La première différence tient au nombre de fils utilisés : les protocoles à données groupées n'utilisent qu'un fil par bit, plus la requête et l'acquittement, alors que les protocoles multi-rails en utilisent plusieurs. Cela rend ces derniers moins adaptés à l'envoi de nombreux bits ; par contre, ils permettent de créer des circuits QDI, plus robustes ; ils ont aussi l'avantage d'inclure la requête dans les données, ce qui permet directement la création de portes à complétion anticipée, renvoyant les résultats sans attendre que toutes les données d'entrée soient disponibles<ref name="earlyoutput">{{pdf}} {{ouvrage |prénom1=Charlie |nom1=Brej |titre=Asynchronous Early Output and Early Acknowledge Dual-Rail Protocols |année=2002 |mois=octobre |langue=en |lire en ligne=http://brej.org/papers/mphil.pdf |id=Brej2002 |commentaire=Thèse présentée en 2002 en vue de l'obtention d'un doctorat de l'[[université de Manchester]] }} et {{pdf}} {{ouvrage |prénom1=Charlie |nom1=Brej |titre=Early Output Logic and Anti-Tokens |année=2005 |mois=septembre |langue=en |lire en ligne=http://brej.org/papers/thesis.pdf |id=Brej2005 |commentaire=Thèse présentée en 2005 en vue de l'obtention d'un doctorat de l'[[université de Manchester]]}}</ref>{{,}}<ref name="adders"/>{{,}}<ref group=note>On utilise un tel protocole pour la propagation de retenues dans les additionneurs, même si le reste de l'environnement est à données groupées</ref> (par exemple une porte ou si une entrée vaut 1) ; de tels pipelines « à grains fins », avec une requête par [[bit]], autorisent l'envoi séparé de chaque partie du résultat.

Les circuits à données groupées sont censés consommer moins et occuper moins de place<ref>{{harvsp|Dinh-Duc|2003|p=64|réf=synQDI}}</ref> ; on peut y réutiliser<ref group=note>Comme par exemple dans les {{langue|en|micropipelines}}</ref> des portes issues de la [[Fonction_logique|logique combinatoire]], comme pour les circuits synchrones ; il est alors nécessaire d'adapter le délai imposé au signal de requête aux délais des portes, bien qu'il soit possible là aussi de créer des portes à délai variable<ref group=note>On trouve de nombreux exemples de telles portes dans la littérature, y compris en électronique synchrone</ref>, soit par un choix de délai à partir des opérandes, soit par une détection de fin de calcul à partir de la consommation du circuit. En termes de fréquence pure, ils se comportent en général légèrement mieux<ref name="pipelinescomp">{{harvsp|Shojaee, Gholipour, Nourani|2006|réf=pipelinecomp}}</ref>, mais cela n'est pas forcément représentatif de la réalité, où des calculs complexes sont réalisés avec des pipelines non linéaires. On constate ainsi que les microprocesseurs asynchrones les plus performants, comme le MiniMIPS, ont utilisé des protocoles multi-rails, alors que le [[Intel 8051|80C51]] de Philips<ref name="80C51"/> et les processeurs [[AMULET (processeur)|AMULET]], qui visent une faible consommation, utilisent des protocoles données-groupées.

Parfois, on utilise ensemble les deux types de protocole pour exploiter le mieux possible les avantages de chacun, en choisissant par exemple du double-rail pour les calculs mais du données-groupées pour les interfaces<ref name="adders"/>.

==== Choix d'implémentations ====
===== Données-groupées =====
Il existe de nombreuses implémentations de protocoles données-groupées : en deux-phases, outre l'implémentation d'origine<ref name="micropipelinesutherland"/>, on en trouve basées sur des [[Bascule_(circuit_logique)#Bascule_D|bascules D]] réagissant aux deux fronts d'horloge<ref>{{PDF}} {{Ouvrage|titre=Asynchronous Micropipeline Using an Effective Double Edge Trigerred D Flipflop|prénom1=S.|nom1=Kaja Mohideen|prénom2=J.|nom2=Rajapaul Perinbam|mois=octobre|année=2005|langue=en|lire en ligne=http://www2.cambr.uidaho.edu/symposiums/12TH_NASA_VLSI_Proceedings/03%20-%20Asynchronous%20Systems/3.2%20-%20Mohideen%20-%20Asynchronous%20Micropipeline%20Using%20an%20Effecti.pdf}}</ref>, ou de type MOUSETRAP<ref name="mousetrap"/>.

Pour les protocoles à quatre phases données-groupées, on peut supprimer certaines dépendances entre les signaux de requête et d'acquittement émis par des étages contigus, lui permettant d'en renvoyer certains plus tôt<ref>{{harvsp|Sparsø, Furber|2001|p=120|réf=asyncdesign}}</ref>. On obtient ainsi des structures plus complexes, mais potentiellement plus performantes.

===== Multi-rail =====
De même, en multi-rail quatre-phases, on distingue trois types de circuits courants, tous trois QDI : {{abréviation|WCHB|Weak Condition Half-Buffer|en}}, {{abréviation|PCHB|PreCharge Half-Buffer|en}} et {{abréviation|PCFB|PreCharge Full-Buffer|en}}<ref>{{harvsp|Vivet|2001|p=70|réf=Aspro}} et {{harvsp|Dinh-Duc|2003|p=65-68|réf=synQDI}}</ref>. En WCHB, un étage attend la validité (respectivement la remise à zéro) de toutes ses entrées avant de renvoyer une donnée à sa sortie (respectivement de revenir à l'état intermédiaire). Par conséquent, au plus un étage sur deux contiendra réellement des données, d'où le nom de « {{langue|en|''half-buffer''}} », et surtout la complétion anticipée est impossible<ref name="WCHB">{{harvsp|Dinh-Duc|2003|p=65-66|réf=synQDI}}</ref>. Les protocoles PCHB et PCFB, en vérifiant explicitement la validité des entrées, peuvent renvoyer des données avant l'arrivée de toutes les entrées et permettent une remise à zéro rapide<ref group=note>Cette phase de remise à zéro est appelée « précharge »</ref>, sans attendre l'invalidité des entrées<ref name="PCHB">{{harvsp|Dinh-Duc|2003|p=66-67|réf=synQDI}}</ref>{{,}}<ref name="PCFB">{{harvsp|Dinh-Duc|2003|p=67-68|réf=synQDI}}</ref>. Le protocole PCFB permet en plus à chaque étage de contenir des données en renvoyant un acquittement dès la remise à zéro des étages précédents : on parle de « {{langue|en|''full-buffer''}} »<ref name="PCFB"/>. Cependant, la simplicité des circuits de type WCHB les rend parfois plus rapides que les autres<ref name="WCHB"/>, tandis que les circuits de type PCFB requièrent plus de porte logiques<ref>{{harvsp|Vivet|2001|p=70|réf=Aspro}}</ref> : le choix n'est donc pas évident.

D'autres types de pipelines sont possibles en utilisant un modèle de délais bornés : avec de la logique à précharge<ref group=note>Remise à zéro par un signal dit « de précharge » avant chaque calcul</ref>, comme PS0<ref name="PS0">{{PDF}} {{ouvrage|titre=Self-timed rings and their application to division|langue=en|année=1991|mois=mai|prénom1=Ted Eugene|nom1=Williams|lire en ligne=http://www-vlsi.stanford.edu/papers/Williams_PhDThesis_SelfTimedRings.pdf|commentaire=Thèse présentée en 1991 en vue de l'obtention d'un doctorat de l'[[université Stanford]]}}</ref> ou LP2/1<ref name="LP21"/>, ou avec des protocoles « ''single-track'' »<ref name="STFB"/>. Toutefois, bien que plus rapides, ils sont aussi moins fiables du fait de l'ajout d'hypothèses temporelles.

Une particularité des protocoles multi-rail est la possibilité d'utiliser plus de fils pour coder les données grâce à un codage 1 parmi n plutôt que double-rail, diminuant ainsi le nombre de fils qui commutent, ce qui peut diminuer la consommation électrique<ref name="renaudin">{{PDF}} {{ouvrage|titre=Circuits Asynchrones et Consommation|prénom1=Marc|nom1=Renaudin|jour=20|mois=novembre|année=2003|lire en ligne=http://www.i3s.unice.fr/ECoFaC/PDF/renaudin/Renaudin.pdf}}</ref>.

==== Choix des composants et du processus de fabrication ====
Parmi les propriétés des circuits, beaucoup dépendent des techniques de fabrication, que ce soient le processus de fabrication lui-même ou les bibliothèques de composants utilisées. L'amélioration des possibilités d'[[Intégration à très grande échelle|intégration]] due à l'amélioration des processus de fabrication, qui est quantifiée par la [[loi de Moore]], permet d'obtenir des circuits électroniques plus compacts et performants. L'optimisation des circuits asynchrones peut aussi passer par le choix de composants adaptés, par l'optimisation du {{lang|en|layout}}, c'est-à-dire de l'agencement des composants sur le [[circuit intégré]], voire par le contrôle des tailles des transistors de manière à adapter au mieux chaque porte à son {{langue|en|[[fan-out]]}}<ref>{{PDF}} {{ouvrage|titre=Designing Fast Asynchronous Circuits|prénom1=Ivant E.|nom1=Sutherland|prénom2=Jon K.|nom2=Lexau|langue=en|lire en ligne=https://labs.oracle.com/features/tenyears/volcd/papers/Suthrlnd.pdf}}</ref>.

==== Adaptation dynamique de la tension d'alimentation ====
Les circuits électroniques consomment d'autant plus que leur tension d'alimentation est élevée. Par conséquent, on cherche à la minimiser pour des circuits à basse consommation. Pour réduire celle-ci sans sacrifier les performances, on peut adapter la tension à la charge de travail du circuit ou à la vitesse désirée<ref name="renaudin"/>. Cette optimisation existe aussi pour des circuits synchrones, mais nécessite conjointement une adaptation de la [[fréquence d'horloge]] qui se fait lentement, là où le processus de mise en veille et de sortie de veille d'un circuit asynchrone est extrêmement rapide<ref name="renaudin"/>.

== Notes et références ==
=== Notes ===
{{Références|colonnes=2|groupe=note}}

=== Références ===
{{Références|colonnes=2}}

==Voir aussi==
===Bibliographie===
* {{pdf}} {{ouvrage |titre=Une méthodologie de conception de circuits intégrés quasi-insensibles aux délais |sous-titre=Application à l'étude et à la réalisation d'un processeur RISC 16-bit asynchrone |prénom1=Pascal |nom1=Vivet |jour=21 |mois=juin |année=2001 |lire en ligne=http://hal.inria.fr/docs/00/04/54/03/PDF/tel-00002974.pdf |id=Aspro |commentaire=Thèse présentée en 2001 en vue de l'obtention d'un doctorat de l'[[INPG]]}}
* {{pdf}} {{ouvrage|titre=Synthèse automatique de circuits asynchrones QDI|prénom1=Anh-Vu|nom1=Dinh-Duc|jour=14|mois=mars|année=2003|langue=fr|commentaire=Thèse présentée en 2003 en vue de l'obtention d'un doctorat de l'[[INPG]]|lire en ligne=http://tel.archives-ouvertes.fr/docs/00/04/53/78/PDF/tel-00002937.pdf|id=synQDI}}
* {{pdf}} {{ouvrage|titre=Synthèse Logique de Circuits Asynchrones Micropipeline|prénom1=Amine|nom1=Rezzag|jour=13|mois=décembre|année=2004|langue=fr|commentaire=Thèse présentée en 2004 en vue de l'obtention d'un doctorat de l'[[INPG]]|lire en ligne=http://tima.imag.fr/publications/files/th/2006/lsm_204.pdf|id=synMP}}
* {{pdf}} {{ouvrage |titre=The Systematic Design of Asynchronous Circuits|langue=en|lire en ligne=http://dlc.iust.ac.ir/files1/Docs/iccad-95-tut.ps.pdf|prénom1=Michael|nom1=Kishinevsky|prénom2=Luciano|nom2=Lavagno|prénom3=Peter|nom3=Vanbekbergen|id=sysdesign|année=1995}}
* {{pdf}} {{article |titre=Asynchronous Design Methodologies: An Overview |prénom1=Scott |nom1=Hauck |mois=janvier |année=1995 |langue=en
|périodique=[[Proceedings of the IEEE]] |volume=83 |numéro=1 |passage=69-93
|lire en ligne=http://www.ee.washington.edu/people/faculty/hauck/publications/AsynchArt.pdf |id=overview}}
* {{pdf}} {{ouvrage |titre=An Introduction to Asynchronous Circuit Design |prénom1=Al |nom1=Davis |prénom2=Steven M. |nom2=Nowick |jour=19 |mois=septembre |année=1997 |langue=en |lire en ligne=http://www.cs.utah.edu/~ald/pubs/ald-nowick-tr-intro.pdf |id=intro |commentaire=Dans l'introduction du document, les auteurs affirment que : {{citation|L'intention de cette monographie est de présenter à la fois une introduction au domaine de la conception de circuits numériques asynchrones et un survol de l'état de l'art en 1997.}} }}
* {{pdf}} {{Ouvrage |langue=en | titre  = Principles of asynchronous circuit design - A systems perspective |prénom1= Jens |nom1= Sparsø |directeur1=oui |prénom2= Steve|nom2= Furber |directeur2=oui | éditeur = Kluwer Academic Publishers | année= 2001 |pages=337|lire en ligne=http://owlhouse.csie.nctu.edu.tw/~dannim/AsynCD/principles_of_ASYNC.pdf|commentaire=Les chapitres 1 à 8, qui sont beaucoup cités, sont aussi publiés indépendamment : {{PDF}} {{ouvrage |titre=Asynchronous circuit design|sous-titre=A tutorial |langue=en |prénom1=Jens |nom1=Sparsø |année=2006 |lire en ligne=http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/855/pdf/imm855.pdf}}|id=asyncdesign}}
* {{pdf}} {{ouvrage|titre=Comparative study of asynchronous pipeline design methods|jour=26|mois=janvier|année=2006|langue=en|prénom1=K.|nom1=Shojaee|prénom2=M.|nom2=Gholipour|prénom3=A.|nom3=|prénom4=M.|nom4=Nourani|id=pipelinecomp|lire en ligne=http://www.jstage.jst.go.jp/article/elex/3/8/163/_pdf|commentaire=Simulation des performances de divers pipelines asynchrones, publiée dans ''IEICE Electronics Express'', volume 3, en Avril 2006}}
{{Portail|électronique|informatique}}

[[Catégorie:Électronique numérique]]

[[de:Asynchroner Schaltkreis]]
[[en:Asynchronous circuit]]
[[fi:Asynkroninen piiri]]
[[ko:비동기 회로]]
[[ru:Асинхронная логика]]
[[zh:非同步電路]]

{{Article potentiellement bon|oldid=77467465|date=8 avril 2012}}