{{Langue du titre|en}}
[[Fichier:Matching of two images using the SIFT method.jpg|thumb|right|alt=Exemple de mise en correspondance de deux images par la méthode SIFT : des lignes vertes relient entre-eux les descripteurs communs à un tableau et une photo de ce même tableau, de moindre qualité, ayant subi des transformations. |Exemple de résultat de la comparaison de deux images par la méthode SIFT (''Fantasia ou Jeu de la poudre, devant la porte d’entrée de la ville de Méquinez'', par [[Eugène Delacroix]], 1832'').]]

'''{{lang|en|Scale-invariant feature transform}}''' ('''SIFT'''), que l'on peut traduire par « transformation de caractéristiques visuelles invariante à l'échelle », est un algorithme utilisé dans le domaine de la [[vision par ordinateur]] pour détecter et identifier les éléments similaires entre différentes [[image numérique|images numériques]] (éléments de paysages, objets, personnes, etc.). Il a été développé en 1999 par le chercheur [[David Lowe (chercheur scientifique)|David Lowe]].

L'étape fondamentale de la méthode proposée par Lowe consiste à calculer ce que l'on appelle les « descripteurs SIFT » des images à étudier. Il s'agit d'informations numériques dérivées de l'analyse locale d'une image et qui caractérisent le contenu visuel de cette image de la façon la plus indépendante possible de l'échelle (« zoom » et résolution du capteur), du cadrage, de l'angle d'observation et de l'exposition (luminosité). Ainsi, deux photographies de la tour Eiffel auront toutes les chances d'avoir des descripteurs SIFT similaires, et ceci d'autant plus si les instants de prise de vue et les angles de vue sont proches. D'un autre côté, deux photographies de sujets très différents produiront selon toute vraisemblance des descripteurs SIFT très différents eux aussi (pouvoir discriminant). Cette robustesse, vérifiée dans la pratique, est une exigence fondamentale de la plupart des applications et explique en grande partie la popularité de la méthode SIFT.

Les applications de la méthode sont nombreuses et ne cessent de s'étendre ; elles couvrent au début du {{s-|XXI|e}} des domaines tels que la [[détection d'objet]], la [[cartographie (robotique)|cartographie]] et la navigation, l'[[assemblage de photos]], la [[modélisation 3D]], la [[recherche d'image par le contenu]], le [[tracking|{{lang|en|tracking video}}]] ou le [[match moving|{{lang|en|match moving}}]].

Cet algorithme est protégé aux [[États-Unis]] par un [[brevet]] détenu par l’[[université de la Colombie-Britannique]].

== Origines de la méthode ==

[[Fichier:Example of SIFT features.jpg|thumb|alt=Exemple de caractéristiques SIFT sur une image. Sur cette image, des flèches représentent ces caractéristiques : leurs queues sont positionnées aux centre des points-clés correspondants, leurs normes sont proportionnelles aux facteurs d'échelle des caractéristiques et les directions correspondent aux directions principales associées.|Exemple de caractéristiques SIFT. Chaque point-clé est représenté par une flèche dont la direction correspond à la direction principale associée et la norme au facteur d'échelle (illustration : ''Fantasia, par Eugène Delacroix'').]]
C'est en 1999 que le chercheur [[Canada|canadien]] David G. Lowe, professeur à l’[[université de la Colombie-Britannique]], présente lors de la [[International Conference on Computer Vision|conférence ICCV]]<ref name=lowe99>{{Chapitre|éditeur=|collection=|série=|titre=Object recognition from local scale-invariant features|titre ouvrage=Proceedings of the [[International Conference on Computer Vision]] |auteurs ouvrage=|titre vo=|ref=|volume=2|titre volume=|auteur=David G. Lowe |prénom=|nom=|auteurs=|trad=|langue=en|lieu=|année=1999 |mois=|jour=|publi=|pages=|format=|isbn=|issn=|présentation en ligne=|lire en ligne=http://www.cs.ubc.ca/~lowe/papers/iccv99.pdf |partie=|numéro=|chap=|passage=1150–1157|id=|commentaire=}}.</ref> une nouvelle méthode d'[[Extraction de caractéristique en vision par ordinateur|extraction de caractéristiques]] et de détection d'objets dans des [[Image numérique#Images en teintes (ou niveaux) de gris|images en niveaux de gris]]. Le nom de ''{{lang|en|Scale-invariant feature transform}}'' (SIFT) a été choisi car la méthode transforme les données d'une image en coordonnées invariantes à l'échelle et rapportées à des caractéristiques locales. Des améliorations et une description précise de la méthode sont proposées par Lowe en 2004 dans la revue ''[[International Journal of Computer Vision]]''<ref name=lowe2004>{{article|id=lowe2004|langue=en|prénom1=David G.|nom1=Lowe |lien auteur1=|titre=Distinctive image features from scale-invariant keypoints |périodique=[[International Journal of Computer Vision]] |lien périodique=|volume=60|numéro=2|jour=|mois=|année=2004 |pages=91–110 |issn=|url texte=http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf |consulté le=}}.</ref>. L’Université de la Colombie-Britannique a demandé et obtenu la protection par brevet de cet algorithme aux [[États-Unis]]<ref name="GuptaJarvis">{{Chapitre|éditeur=Springer Verlag |collection=|série=|titre= Using a Virtual World to Design a Simulation Platform for Vision and Robotic Systems |titre ouvrage=Proceedings of the 5th International Symposium on Advances in Visual Computing |auteurs ouvrage=|titre vo=|ref=|volume=|titre volume=|auteur=|prénom=|nom=|auteurs=Om K. Gupta  et Ray A. Jarvis|trad=|langue=en |lien langue=|lieu=|année=2009 |mois=|jour=|publi=|pages=|format=|isbn=978-3-642-10330-8 |issn=|présentation en ligne=|lire en ligne=http://books.google.fr/books?id=2F_nPm9LjJEC&pg=PA241#v=onepage&q&f=false |partie=|numéro=|chap=|passage=241|id=|commentaire=}}.</ref>{{,}}<ref>{{US patent|6,711,293}}, « ''Method and apparatus for identifying scale invariant features in an image and use of same for locating an object in an image'' ».</ref>.

L'idée d'utiliser des points d'intérêt locaux remonte aux travaux de [[Hans Moravec]] en 1981 sur la recherche de correspondance entre images stéréoscopiques<ref name=lowe2004-93>[[#lowe2004|Lowe (2004)]], {{p.|93}}.</ref>{{,}}<ref name="Moravec1981">{{Chapitre|éditeur=|collection=|série=|titre=Rover Visual Obstacle Avoidance|titre ouvrage= Proceedings of the seventh International Joint Conference on Artificial Intelligence |auteurs ouvrage=|titre vo=|ref=|volume=|titre volume=|auteur= |prénom= |nom= |auteurs=Hans P. Moravec |trad=|langue=en|lieu= Vancouver, Colombie Britannique |année=1981 |mois=août |jour= |publi= |pages= |format= |isbn= |issn= |présentation en ligne=|lire en ligne=http://www.frc.ri.cmu.edu/~hpm/project.archive/robot.papers/1981/ijcai81.mss |partie= |numéro= |chap=|passage= 785-790 |id=|commentaire=}}.</ref> et aux améliorations apportées en 1988 par Harris et Stephens<ref name=lowe2004-93>[[#lowe2004|Lowe (2004)]], {{p.|93}}.</ref>{{,}}<ref name="HarrisStephens1988">{{Chapitre|éditeur=|collection=|série=|titre= A combined corner and edge detector|titre ouvrage= Proceedings of the fourth Alvey Vision Conference |auteurs ouvrage=|titre vo=|ref=|volume=|titre volume=|auteur= |prénom= |nom= |auteurs=Chris Harris et Mike Stephens |trad=|langue=en|lieu= Manchester |année=1988 |mois=|jour= |publi= |pages= |format= |isbn= |issn= |présentation en ligne=|lire en ligne=http://www.bmva.org/bmvc/1988/avc-88-023.pdf |partie= |numéro= |chap=|passage= 147–151 |id=|commentaire=}}.</ref>. À la suite de quoi, différents types de caractéristiques sont proposés<ref name=lowe99></ref>, tels que des segments de lignes<ref name="Grimson-Lozano-Pérez">{{article|langue=en |prénom1=W. E. L. |nom1=Grimson |lien auteur1=|prénom2=Thomas |nom2=Lozano-Pérez |titre=Localizing overlapping parts by searching the interpretation tree |périodique=[[IEEE Transactions on Pattern Analysis and Machine Intelligence]] |lien périodique=|volume=PAMI-9 |numéro=4 |jour=|mois=|année=1987 |pages=469–482|issn=|url texte=|consulté le=}}.</ref>, des groupements d'arêtes<ref name=lowe87>{{article|langue=en|prénom1=David G. |nom1=Lowe|lien auteur1=|titre= Three-dimensional object recognition from single two-dimensional images |périodique= Artificial Intelligence |lien périodique=|volume=31 |numéro=3 |jour=|mois=|année=1987 |pages= 355–395 |issn=|url texte= http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.5388&rep=rep1&type=pdf |consulté le=}}.</ref>{{,}}<ref name=Nelson98>{{article|langue=en|prénom1= Randal C. |nom1= Nelson |lien auteur1=|prénom1=Andrea |nom1=Selinger |titre= Large-scale tests of a keyed, appearance-based 3-D object recognition system |périodique= Vision Research |lien périodique=|volume=38|numéro=15|jour=|mois=|année=1998 |pages= 2469–2488 |issn=|url texte=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.1121&rep=rep1&type=pdf |consulté le=}}.</ref> ou de zones<ref>{{article|langue=en|prénom1= Ronen |nom1=Basri |lien auteur1=|prénom2= David. W. |nom2= Jacobs |titre= Recognition using region correspondences |périodique= [[International Journal of Computer Vision]] |lien périodique=|volume=25 |numéro=2 |jour=|mois=|année=1996 |pages=141–162 |issn=|url texte=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.1494&rep=rep1&type=pdf |consulté le=}}.</ref>. En 1992, l'intérêt de ce type de détecteur est confirmé par les travaux de Harris<ref name=BlakeYuille>{{Chapitre|éditeur=MIT Press |collection=|série=|titre=Geometry from visual motion |titre ouvrage=Active Vision |auteurs ouvrage=A. Blake and A. Yuille |titre vo=|ref=|volume=|titre volume=|auteur=|prénom=|nom=|auteurs=C. Harris |trad=|langue=|lien langue=|lieu=|année=1992 |mois= |jour= |publi= |pages=|format=|isbn=|issn=|présentation en ligne=|lire en ligne=|partie=|numéro=|chap=|passage=263-284 |id=|commentaire=}}.</ref> et le descripteur de coins qu'il propose, améliorant la plupart des défauts du détecteur de Moravec, va connaître un important succès. Il faut cependant attendre 1997 et le travail précurseur de Cordelia Schmid et [[Roger Mohr]] pour établir l'importance, dans le domaine de la vision, des caractéristiques locales invariantes appliquées au problème général de détection et de recherche de correspondance<ref name=lowe2004-93>[[#lowe2004|Lowe (2004)]], {{p.|93}}.</ref>{{,}}<ref>{{article |langue=en |prénom1=C. |nom1=Schmid |lien auteur1=|prénom2=R. |nom2=Mohr |titre=Local grayvalue invariants for image retrieval |périodique=[[IEEE Transactions on Pattern Analysis and Machine Intelligence]] |lien périodique=|volume=19 |numéro=5 |jour=|mois=|année=1997 |pages=530-534 |issn=|url texte=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.8184&rep=rep1&type=pdf |consulté le=}}.</ref>. Si le descripteur qu'ils développent (et qui s'appuie sur le détecteur de coins de Harris) apporte l'invariance à la rotation, il reste cependant sensible aux changements d'échelle, d'angle d'observation et d'exposition<ref name=lowe2004-103>[[#lowe2004|Lowe (2004)]], {{p.|103}}.</ref>. Lowe comblera grandement ces défauts avec son descripteur SIFT<ref name=lowe2004-91>[[#lowe2004|Lowe (2004)]], {{p.|91}}.</ref>.

== Généralités ==

La méthode proposée par Lowe comprend deux parties<ref name=lowe2004-91>[[#lowe2004|Lowe (2004)]], {{p.|91}}.</ref> (chacune ayant fait l'objet de recherches plus ou moins indépendantes par la suite) :
*un algorithme de [[Détection de zones d'intérêt|détection de caractéristiques]] et de [[Extraction de caractéristique (vision par ordinateur)|calcul de descripteurs]] ;
*un algorithme de mise en correspondance proprement dit.
De ces deux aspects, le premier est sans doute celui qui a le plus assuré la popularité de la méthode<ref name=MarcoTreiber>{{ouvrage|langue=en|prénom1=Marco |nom1=Treiber |lien auteur1=|titre=An introduction to object recognition: selected algorithms for a wide variety of application |sous-titre=|numéro d'édition=|éditeur= Springer |lien éditeur=|lieu=|jour=|mois=|année=2010 |volume=|tome=|pages totales=|passage=147|isbn=9781849962346 |lire en ligne=http://books.google.fr/books?id=ovQAibXV4ZoC&pg=PA147#v=onepage&q&f=false |consulté le=30 décembre 2010}}.</ref>, à tel point que le sigle SIFT fait plus souvent référence aux « descripteurs SIFT » qu'à la méthodologie globale. Il s'agit tout d'abord de détecter sur l'image des zones circulaires « intéressantes », centrées autour d'un ''point-clé'' et de rayon déterminé appelé ''facteur d'échelle''. Celles-ci sont caractérisées par leur unité visuelle et correspondent en général à des éléments distincts sur l'image. Sur chacune d'elles, on détermine une orientation intrinsèque qui sert de base à la construction d'un histogramme des orientations locales des contours, habilement pondéré, seuillé et normalisé pour plus de stabilité. C'est cet histogramme qui sous la forme d'un vecteur à 128 dimensions (ou valeurs) constitue le descripteur SIFT du point-clé, et l'ensemble des descripteurs d'une image établissent ainsi une véritable signature numérique du contenu de celle-ci.

Ces descripteurs présentent l'avantage d'être invariants à l'orientation et à la résolution de l'image, et peu sensibles à son exposition, à sa netteté ainsi qu'au point de vue 3D. Ils possèdent ainsi des propriétés similaires à celles des neurones du [[lobe temporal| gyrus temporal inférieur]] utilisées en détection d'objet en vision primaire<ref>{{article|langue=en |prénom1=T.|nom1=Serre|prénom2=M.|nom2= Kouh|prénom3= C.|nom3= Cadieu |prénom4= U.|nom4= Knoblich|prénom5= G.|nom5= Kreiman|prénom6= T.|nom6= Poggio|lien auteur1=|titre=A Theory of Object Recognition: Computations and Circuits in the Feedforward Path of the Ventral Stream in Primate Visual Cortex |périodique=Computer Science and Artificial Intelligence Laboratory Technical Report |lien périodique=|volume=|numéro=MIT-CSAIL-TR-2005-082 |jour=19|mois=décembre |année=2005 |pages=|issn=|url texte=http://dspace.mit.edu/bitstream/handle/1721.1/36407/MIT-CSAIL-TR-2005-082.pdf?sequence=1,%20http://www.google.com/search?client=safari&rls=en&q=%22A+Theory+of+Object+Recognition:+Computations+and+Circuits+in+the+Feedforward+Path+of+the+Ventral+Stream+in+Primate+Visual+Cortex%22&ie=UTF-8&oe=UTF-8 |consulté le=31 décembre }}.</ref>. Par exemple, ils décriront de façon très semblable les détails d'une image originale et ceux d'une image retouchée par l'application d'une rotation, d'un recadrage ou d'un lissage, par une correction de l'exposition, par occultation partielle ou encore par l'insertion dans une image plus grande.

Une fois le calcul des descripteurs effectué, l'algorithme de mise en correspondance intervient dans le but de rechercher les zones d'une image dite suspecte qui contiennent des éléments visuellement similaires à ceux d'une bibliothèque d'images de référence, c'est-à-dire des descripteurs numériquement proches. Ceci ne peut fonctionner que parce que les descripteurs SIFT sont à la fois robustes (aux principales transformations affines et aux changements d'exposition ou de perspective) et discriminants (deux objets différents ont une grande probabilité d'avoir des descripteurs différents).

== Détection des points-clés et calcul du descripteur SIFT ==

La première étape de l'algorithme est la [[détection_de_zones_d'intérêt#Points_d'intérêt|détection des points d'intérêt]], dits ''points-clés''. Un point-clé <math>(x,y,\sigma)</math> est défini d'une part par ses coordonnées sur l'image (<math>x</math> et <math>y</math>) et d'autre part par son facteur d'échelle caractéristique (<math>\sigma</math>). En toute rigueur, il s'agit d'une [[Détection de zones d'intérêt|zone d'intérêt]] circulaire, le rayon de la zone étant proportionnel au facteur d'échelle. Il s'ensuit une étape de reconvergence et de filtrage qui permet d'améliorer la précision sur la localisation des points-clés et d'en éliminer un certain nombre jugés non pertinents. Chaque point-clé restant est ensuite associé à une orientation intrinsèque, c'est-à-dire ne dépendant que du contenu local de l'image autour du point clé, au facteur d'échelle considéré. Elle permet d'assurer l'invariance de la méthode à la rotation et est utilisée comme référence dans le calcul du descripteur, qui constitue la dernière étape de ce processus<ref name=lowe2004-92>[[#lowe2004|Lowe (2004)]], {{p.|92}}.</ref>.

=== Détection d’extrema dans l’espace des échelles === 

La détection s'effectue dans un [[structure discrète|espace discret]] que l'on appelle ''espace des échelles'' (''{{lang|en|scale space}}'') qui comporte trois dimensions : les coordonnées cartésiennes <math>x</math> et <math>y</math> et le facteur d'échelle <math>\sigma</math>. On appelle ''gradient'' de facteur d'échelle <math>\sigma</math> (noté <math>L</math>) le résultat de la [[produit de convolution|convolution]] d'une image <math>I</math> par un [[Lissage d'images#Filtre gaussien|filtre gaussien]] <math>G</math> de paramètre <math>\sigma</math>, soit<ref name=lowe2004-95>[[#lowe2004|Lowe (2004)]], {{p.|95}}.</ref> :
:<math>L \left( x, y, \sigma \right) = G \left( x, y, \sigma \right) * I \left( x, y \right)</math>
[[Fichier:Pyramid of gradients.png|thumb|center|upright=2|alt=Illustration représentant la construction d'une pyramide de gradients|Pyramide de gradients : 3 octaves de 5 gradients.]]
Cette convolution a pour effet de lisser l'image originale <math>I</math> de telle sorte que les détails trop petits, c'est-à-dire de rayon inférieur à <math>\sigma</math><ref group="note">Ici comme dans la littérature scientifique en général, le facteur d'échelle – paramètre du filtre gaussien <math>\sigma</math> – est assimilé à une distance en pixels sur l'image, que l'on pourrait appeler ''rayon associé'' <math>r</math>. En fait, ils sont proportionnels (<math>r = \alpha \sigma</math>), avec un facteur <math>\alpha</math> qui varie généralement entre 3 et 4 selon les auteurs. Il est tout simplement lié au nombre de coefficients au-delà duquel les valeurs de la gaussienne deviennent négligeables.</ref>, sont estompés. Par conséquent, la détection des objets de dimension approximativement égale à <math>\sigma</math> se fait en étudiant l'image appelée ''[[Différence de gaussiennes|différences de gaussiennes]]'' (en anglais ''{{lang|en|difference of gaussians}}'', DoG) définie comme suit :
:<math>D \left( x, y, \sigma \right) = L \left( x, y, k\sigma \right) - L \left( x, y, \sigma \right)</math>, 
où <math>k</math> est un paramètre fixe de l'algorithme qui dépend de la finesse de la discrétisation de l'espace des échelles voulue<ref name=lowe2004-95>[[#lowe2004|Lowe (2004)]], {{p.|95}}.</ref>.

Dans cette image ne persistent plus que les objets observables dans des facteurs d'échelle qui varient entre <math>\sigma</math> et <math>k \sigma</math>. De ce fait, un point-clé candidat <math>(x,y,\sigma)</math> est défini comme un point où un extremum du DoG est atteint par rapport à ses voisins immédiats, c'est-à-dire sur l'ensemble contenant 26 autres points défini par :

:<math>\{ D \left( x + \delta_x, y + \delta_y, s \sigma \right), \delta_x \in \{-1, 0, 1\}, \delta_y \in \{-1,0, 1\}, s \in \{k^{-1}, 1, k\} \}</math>

L'utilisation d'une [[Pyramide (traitement d'image)|pyramide]] est préconisée pour optimiser le temps de calcul des images floutées à un grand nombre d'échelles différentes. La base de la pyramide est en général l'image originale et un niveau donné – on parle d'''octave'' par analogie avec la [[Octave (musique)|musique]] – est obtenu à partir du précédent en divisant la résolution de l'image par 2, ce qui revient à doubler le facteur d'échelle. Au sein d'une même octave, le nombre de convolées à calculer est constant. Le facteur fixe <math>k</math> dans les formules ci-dessus est calculé pour qu'au final, l'espace discrétisé des facteurs d'échelles considérés corresponde à une progression géométrique <math>\{ \sigma_0, k \sigma_0, k^2 \sigma_0, ... \}</math>, avec à chaque changement d'octave une valeur <math>k^p \sigma_0</math> qui devient égale à une quantité de la forme <math>2^t \sigma_0</math>. Ce détail – la progression géométrique des facteurs d'échelle – est important pour que les valeurs des DoG à différentes échelles soient comparables entre elles et il évite, observe Lowe, d'avoir à utiliser un facteur de normalisation dans leur calcul<ref name=lowe2004-97>[[#lowe2004|Lowe (2004)]], {{p.|97}}.</ref>.

[[Fichier:Differences-of-Grandients.png|thumb|center|upright=2|alt=Schéma illustrant la construction d'une pyramide de différences de gaussiens (DoG) à partir de la pyramide de gradients. 3 octaves de 5 gradients sont représentées, et sous chaque octave, sont représntées les différences de gaussiennes issues de chaque paire d'images successives.|Construction de la pyramide de différences de gaussiens (DoG) à partir de la pyramide de gradients.]]

L'étape de détection des points-clés candidats décrite ci-dessus est une variante de l'une des méthodes de ''{{lang|en|blob detection}}'' (détection de zones) développée par Lindeberg, qui utilise le laplacien normalisé par le facteur d'échelle<ref name=Lindeberg1998>{{article|langue=en|prénom1=Tony|nom1=Lindeberg|lien auteur1=|titre=Feature detection with automatic scale selection |périodique=International Journal of Computer Vision |lien périodique=International Journal of Computer Vision |volume=30|numéro=2|jour=|mois=|année= 1998 |pages=79–116|issn=|url texte=http://www.nada.kth.se/cvap/abstracts/cvap198.html|consulté le=25 octobre 2010}}.</ref> au lieu des DoG. Ces derniers peuvent être considérés comme une approximation des laplaciens et présentent l'avantage d'autoriser l'utilisation d'une technique pyramidale<ref name=lowe2004-96>[[#lowe2004|Lowe (2004)]], {{p.|96}}.</ref>.

[[Fichier:Détection-d'extrémums-dans-l'espace-des-échelles.png|thumb|center|upright=2|alt=Exemple de détection d'extremums dans l'espace des échelles. Ici, sont représentés trois portions de DoGs successifs, ainsi que la position d'un extrémum dans l'espace des échelles.|Exemple de détection d'extremums dans l'espace des échelles.]]

=== Localisation précise de points clés ===

[[Fichier:Sift keypoints filtering.jpg|thumb|alt=Cette illustration représente trois fois la même image à des étapes différentes de la détection des extremums. Sur la première image sont représentés tous les points bruts tels que déterminés par l'algorithme. Sur la deuxième image, les point de faible contraste ont été éliminés. Sur la dernière image, les points situés sur les arêtes ont été éliminés.|Après la détection des extremums dans l'espace des échelles (leurs positions sont indiquées sur l'image du haut), l'algorithme élimine les points de faible contraste (les points restants apparaissent sur l'image du milieu), puis les points situés sur les arêtes. Les points restants sont indiqués sur l'image du bas.]]
L'étape de détection d’extremums<ref name=lowe2004-97>[[#lowe2004|Lowe (2004)]], {{p.|97}}.</ref> produit en général un grand nombre de points-clés candidats, dont certains sont instables ; de plus, leur localisation, en particulier aux échelles les plus grandes (autrement dit dans les octaves supérieures de la pyramide où la résolution est plus faible) reste approximative. De ce fait, des traitements supplémentaires sont appliqués, pour un objectif double : d'une part, reconverger la position des points pour améliorer la précision sur <math>x</math>, <math>y</math> et <math>\sigma</math> ; d'autre part, éliminer les points de faible contraste ou situés sur des arêtes de contour à faible courbure et donc susceptibles de « glisser » facilement.

==== Amélioration de la précision par interpolation des coordonnées ====

Visant à augmenter de façon significative la stabilité et la qualité de la mise en correspondance ultérieure<ref name=lowe2004-100>[[#lowe2004|Lowe (2004)]], {{p.|100}}.</ref>, cette étape, qui est une amélioration de l'algorithme original, s'effectue dans l'espace des échelles à trois dimensions, où <math>D(x, y, \sigma)</math>, qui n'est connu que pour des valeurs discrètes de <math>x</math>, <math>y</math> et <math>\sigma</math>, doit être interpolé.

Cette interpolation s'obtient par un [[développement de Taylor]] à l'ordre 2 de la fonction [[différence de gaussiennes]] <math>D ( x, y, \sigma )</math>, en prenant comme origine les coordonnées du point-clé candidat<ref name=lowe2004-100>[[#lowe2004|Lowe (2004)]], {{p.|100}}.</ref>. Ce développement s'écrit comme suit :

:<math>D(\textbf{x}) = D + \frac{\partial D^{\rm T}}{\partial \textbf{x}}\textbf{x} + \frac{1}{2}\textbf{x}^{\!\rm T}\,\frac{\partial^2 D}{\partial \textbf{x}^2} \textbf{x}</math>

où <math>D</math> et ses dérivées sont évaluées au point-clé candidat et où <math>\textbf{x} = (x, y, \sigma)^{\!\rm T}</math> est un delta par rapport à ce point. Les dérivées sont estimées par [[différences finies]] à partir des points voisins connus de façon exacte. La position précise de l'extremum <math>\hat{\textbf{x}}</math> est déterminée en résolvant l'équation annulant la dérivée de cette fonction par rapport à <math>\textbf{x}</math> ; on trouve ainsi<ref name=lowe2004-100>[[#lowe2004|Lowe (2004)]], {{p.|100}}.</ref> :

:<math>\hat{\textbf{x}} = - \frac{\partial^2 D}{\partial \textbf{x}^2} ^ {-1} \frac{\partial D}{\partial \textbf{x}}</math>

Un delta <math>\hat{\textbf{x}}</math> supérieur à <math>0,5</math> dans l'une des trois dimensions signifie que le point considéré est plus proche d'un des voisins dans l'espace des échelles discret. Dans ce cas, le point-clé candidat est mis à jour et l'interpolation est réalisée à partir des nouvelles coordonnées. Sinon, le delta est ajouté au point candidat initial qui gagne ainsi en précision<ref name=lowe2004-101>[[#lowe2004|Lowe (2004)]], {{p.|101}}.</ref>.

Un algorithme de reconvergence similaire a été proposé dans l'implémentation temps-réel basée sur les pyramides hybrides de Lindeberg et Bretzner<ref>{{Chapitre|éditeur= Springer-Verlag |collection=|série=|titre=Real-time scale selection in hybrid multi-scale representations |titre ouvrage= Proceedings of the 4th international conference on Scale space methods in computer vision |auteurs ouvrage=|titre vo=|ref=|volume=2695|titre volume=|auteur=|prénom=|nom=|auteurs=Tony Lindeberg et Lars Bretzner |trad=|langue=en|lien langue=|lieu=Berlin|année=2003 |mois=|jour=|publi=|passage=148–163|format=|isbn= 3-540-40368-X |issn=|présentation en ligne=|lire en ligne=http://www.nada.kth.se/cvap/abstracts/cvap279.html|partie=|numéro=|chap=|passage=148–163|id=|commentaire=}}.</ref>.

==== Élimination des points-clés de faible contraste ====

La valeur de <math>D(\hat{\textbf{x}})</math> aux coordonnées précises du point-clé peut être calculée à partir du développement de Taylor de cette fonction, et constitue donc un extremum local. Un seuillage absolu sur cette valeur permet d'éliminer les points instables, à faible contraste<ref name=lowe2004-100>[[#lowe2004|Lowe (2004)]], {{p.|100}}.</ref>{{,}}<ref group="note">Il est à noter que dans la méthodologie originale de Lowe, le point-clé candidat est éliminé si la valeur de <math>D(\hat{\textbf{x}})</math> est inférieure à 0,03 (en valeur absolue).</ref>.

==== Élimination des points situés sur les arêtes ====

Les points situés sur les arêtes (ou contours) doivent être éliminés car la fonction DoG y prend des valeurs élevées, ce qui peut donner naissance à des extremums locaux instables, très sensibles au bruit : si l'image devait subir un changement numérique même imperceptible, de tels points-clés peuvent se retrouver déplacés ailleurs sur la même arête, ou même simplement disparaître<ref name=lowe2004-102>[[#lowe2004|Lowe (2004)]], {{p.|102}}.</ref>.

Un point candidat à éliminer, si l'on considère les deux directions principales à sa position, est caractérisé par le fait que sa [[courbure principale]] le long du contour sur lequel il est positionné est très élevée par rapport à sa courbure dans la direction orthogonale<ref name=lowe2004-102>[[#lowe2004|Lowe (2004)]], {{p.|102}}.</ref>. La courbure principale est représentée par les [[valeurs propres]] de la [[matrice hessienne]] <math>\textbf{H}</math> :
:<math> \textbf{H} =  \begin{bmatrix}
  D_{xx} & D_{xy} \\
  D_{xy} & D_{yy}
\end{bmatrix}</math>
Les dérivées doivent être évaluées aux coordonnées du point d'intérêt <math>( x, y, \sigma)</math> dans l'espace des échelles. Les valeurs propres de <math>\textbf{H}</math> sont proportionnelles aux courbures principales de <math>D</math>, dont seul le rapport <math>r</math> est intéressant. La trace de <math>\textbf{H}</math> représente la somme de ces valeurs, le déterminant son produit. Par conséquent, en adoptant un seuil <math>r_{\text{th}}</math> sur le ratio des courbures (<math>r_{\text{th}} = 10</math> dans la méthode originale de Lowe<ref name=lowe2004/>), un point-clé candidat va être retenu, selon le critère adopté par Lowe, si<ref name=lowe2004-102>[[#lowe2004|Lowe (2004)]], {{p.|102}}.</ref> :
:<math>R = \frac{\operatorname{tr} (\textbf{H})^2}{\operatorname{det} ( \textbf{H})} = \frac{( r+1 )^2}{r} < \frac{( r_{\text{th}} + 1 )^2}{r_{\text{th}}}</math>
La vérification de ce critère est rapide, ne nécessitant qu'une dizaine d'opérations flottantes seulement. Lorsque ce critère n'est pas vérifié, le point est considéré comme localisé le long d'une arête et il est par conséquent rejeté.

Cette étape est inspirée de la technique de [[détection de points d'intérêt]] par l'opérateur de Harris ; pour le seuillage, une matrice hessienne est utilisée au lieu de la matrice des moments d'ordre 2 ([[tenseur]])<ref name=lowe2004-102>[[#lowe2004|Lowe (2004)]], {{p.|102}}.</ref>.

=== Assignation d’orientation ===

L'étape d'assignation d’orientation consiste à attribuer à chaque point-clé une ou plusieurs orientations déterminées localement sur l'image à partir de la direction des gradients dans un voisinage autour du point. Dans la mesure où les descripteurs sont calculés relativement à ces orientations, cette étape est essentielle pour garantir l'invariance de ceux-ci à la rotation : les mêmes descripteurs doivent pouvoir être obtenus à partir d'une même image, quelle qu'en soit l'orientation<ref name=lowe2004-103>[[#lowe2004|Lowe (2004)]], {{p.|103}}.</ref>.

Pour un point-clé donné <math>( x_0, y_0, \sigma_0 )</math>, le calcul s'effectue sur <math>L ( x, y, \sigma_0 )</math>, à savoir le gradient de la pyramide dont le paramètre est le plus proche du facteur d'échelle du point. De cette façon, le calcul est également invariant à l'échelle. À chaque position dans un voisinage du point-clé, on estime le gradient par différences finies symétriques, puis son amplitude (c.-à-d. sa [[Norme (mathématiques)|norme]]) <math>m ( x, y )</math>, et son orientation <math>\theta ( x, y )</math><ref name=lowe2004-103>[[#lowe2004|Lowe (2004)]], {{p.|103}}.</ref> :

:<math>m \left( x, y \right) = \sqrt{\bigl( L \left( x+1, y \right) - L \left( x-1, y \right) \bigr)^2 + \bigl( L \left( x, y+1 \right) - L \left( x, y-1 \right) \bigr)^2}</math>

:<math>\theta \left( x, y \right) = \tan^{-1}\left(\frac{L \left( x, y+1 \right) - L \left( x, y-1 \right)}{L \left( x+1, y \right) - L \left( x-1, y \right)} \right)</math>

::<math>\forall\left(x, y\right) \hbox{ dans un voisinage de } \left(x_0, y_0\right)</math>.

[[Fichier:SIFT histogram construction.svg|thumb|right|upright=2|alt=Illustration de la construction de l'histogramme des orientations à partir de la direction des gradients dans un voisinage autour du point.|Construction de l'histogramme des orientations.]]
Un [[histogramme]] des orientations sur le voisinage est réalisé avec 36 intervalles, couvrant chacun 10 degrés d'angle. L'histogramme est doublement pondéré : d'une part, par une fenêtre circulaire gaussienne de paramètre égal à 1,5 fois le facteur d'échelle du point-clé <math>\sigma_0</math> ; d'autre part, par l'amplitude de chaque point. Les pics dans cet histogramme correspondent aux orientations dominantes. Toutes les orientations dominantes permettant d'atteindre au moins 80 % de la valeur maximale sont prises en considération, ce qui provoque si nécessaire la création de points-clés supplémentaires ne différant que par leur orientation principale<ref name=lowe2004-103>[[#lowe2004|Lowe (2004)]], {{p.|103}}.</ref>.

À l'issue de cette étape, un point-clé est donc défini par quatre paramètres <math>( x, y, \sigma, \theta )</math>. Il est à noter qu'il est parfaitement possible qu'il y ait sur une même image plusieurs points-clés qui ne différent que par un seul de ces quatre paramètres (le facteur d'échelle ou l'orientation, par exemple).

=== Descripteur de point-clé ===

Une fois les points-clés, associés à des facteurs d'échelles et à des orientations, détectés et leur invariance aux changements d'échelles et aux rotations assurée, arrive l'étape de calcul des vecteurs descripteurs, traduisant numériquement chacun de ces points-clés. À cette occasion, des traitements supplémentaires vont permettre d'assurer un surcroît de pouvoir discriminant en rendant les descripteurs invariants à d'autres transformations telles que la luminosité, le changement de point de vue 3D, etc. Cette étape est réalisée sur l'image lissée avec le paramètre de facteur d'échelle le plus proche de celui du point-clé considéré<ref name=lowe2004-104>[[#lowe2004|Lowe (2004)]], {{p.|104}}.</ref>.

[[Fichier:SIFT gradient magnitude and orientation computation.svg|thumb|right|upright=1.5|alt=Illustration de la construction d'un descripteur SIFT à partir d'une région de {{nobr|16 × 16}} pixels.|Construction d'un descripteur SIFT.]]
Autour de ce point, on commence par modifier le système de coordonnées local pour garantir l'invariance à la rotation, en utilisant une rotation d'angle égal à l'orientation du point-clé, mais de sens opposé. On considère ensuite, toujours autour du point-clé, une région de {{nobr|16 × 16}} pixels, subdivisée en {{nobr|4 × 4}} zones de {{nobr|4 × 4}} pixels chacune. Sur chaque zone est calculé un histogramme des orientations comportant 8 intervalles. En chaque point de la zone, l'orientation et la magnitude du gradient sont calculés comme précédemment. L'orientation détermine l'intervalle à incrémenter dans l'histogramme, ce qui se fait avec une double pondération – par l'amplitude et par une fenêtre gaussienne centrée sur le point clé, de paramètre égal à 1,5 fois le facteur d'échelle du point-clé<ref name=lowe2004-105>[[#lowe2004|Lowe (2004)]], {{p.|105}}.</ref>.

Ensuite, les 16 histogrammes à 8 intervalles chacun sont concaténés et normalisés. Dans le but de diminuer la sensibilité du descripteur aux changements de luminosité, les valeurs supérieures à 0,2 sont remplacées par 0,2 et l'histogramme est de nouveau normalisé, pour finalement fournir le descripteur SIFT du point-clé, de dimension 128<ref name=lowe2004-106>[[#lowe2004|Lowe (2004)]], {{p.|106}}.</ref>.

Cette dimension peut paraître bien élevée, mais la plupart des descripteurs de dimension inférieure proposés dans la littérature présentent de moins bonnes performances dans les tâches de mise en correspondance<ref name=lowe2004-106>[[#lowe2004|Lowe (2004)]], {{p.|106}}.</ref> pour un gain en coût de calculs bien modéré, en particulier quand la technique ''{{lang|en|Best-Bin-First}}'' (BBF) est utilisée pour trouver le plus proche voisin. Par ailleurs, des descripteurs de plus grande dimension permettraient probablement d'améliorer les résultats, mais les gains escomptés seraient dans les faits assez limités, alors qu'à l'inverse augmenterait sensiblement le risque de sensibilité à la distorsion ou à l'occlusion. Il a également été démontré que la précision de recherche de correspondance de points dépasse 50 % dans les cas de changement de point de vue supérieur à 50 degrés, ce qui permet d'affirmer que les descripteurs SIFT sont invariants aux [[transformation affine|transformations affine]]s modérées. Le pouvoir discriminant des descripteurs SIFT a pour sa part été évalué sur différentes tailles de bases de données de points-clés ; il en ressort que la précision de mise en correspondance est très marginalement impactée par l'augmentation de la taille de la base de données, ce qui constitue une bonne confirmation du pouvoir discriminant des descripteurs SIFT<ref name=lowe2004-107>[[#lowe2004|Lowe (2004)]], {{p.|107}}.</ref>.

== Utilisation pour la recherche d'objets dans des images ==

La problématique de base pour laquelle la méthode SIFT a été conçue est la suivante : peut-on trouver dans une image donnée (dite ''image question'' ou ''image suspecte''), des objets déjà présents dans une collection d'images de référence pré-établie ?

Dans la méthode originale de David Lowe<ref name="lowe99" />{{,}}<ref name="lowe2004" />, les points-clés et les descripteurs SIFT sont tout d'abord extraits des images de référence et stockés dans une sorte de base de données. Un objet est identifié dans l'image question en effectuant une comparaison de ses descripteurs à ceux des images de référence disponibles en base de données, fondée simplement sur la [[distance euclidienne]]. Parmi toutes les correspondances ainsi établies, des sous-ensembles (''clusters'') sont identifiés, au sein desquels la mise en correspondance est cohérente du point de vue des positions des points, des facteurs d'échelle et des orientations. Les clusters contenant au moins trois correspondances ponctuelles sont conservés. Dans chacun d'eux, on modélise la transformation permettant de passer de l'image question à l'image de référence, et on élimine les correspondances aberrantes par simple vérification de ce modèle. Enfin, Lowe applique un modèle probabiliste pour confirmer que la détection d'une correspondance d'objets entre l'image question et l'une des images de référence n'est pas due au hasard, basé sur l'idée que si de nombreux points n'ont pas pu être mis en correspondance c'est que l'on a peut-être affaire à un [[faux positif]]<ref name=lowe2004-109>[[#lowe2004|Lowe (2004)]], {{p.|109}}.</ref>.

À chacune de ces étapes, Lowe<ref name="lowe2004" /> propose une approche efficace présentée succinctement dans le tableau ci-dessous et dont les principes sont détaillés dans les paragraphes suivants.

{| class="wikitable"
|+ Résumé des techniques utilisées dans les différentes étapes de SIFT
|-
! scope=col | Étape
! scope=col | Techniques utilisées
! scope=col | Avantages
|-
| ''Extraction des points-clés'' (des images de référence et de l'image question)
| Pyramide de gradients, différence de gaussiens, assignation d’orientation
| Précision, stabilité, invariance aux modifications d’échelle et à la rotation
|-
| ''Calcul des descripteurs'' (des images de référence et de l'image question)
| Échantillonnage et lissage des plans locaux d’orientation de l’image
| Stabilité relative aux transformations affines et à la luminosité
|-
| ''Indexation'' des descripteurs des images de référence
| Arbre kd
| Efficacité
|-
| ''Recherche des correspondances'' avec les descripteurs de l'image question
| Plus proche voisin approximatif (''{{lang|en|Best Bin First}}'')
| Rapidité
|-
| ''Identification de clusters''
| Transformée de Hough et table de hachage
| Modèle de transformation fiable
|-
| ''Vérification du modèle''
| Moindres carrés linéaires
| Élimination des fausses correspondances
|-
| ''Validation de l'hypothèse''
| Inférence bayésienne
| Fiabilité
|}

=== Indexation des descripteurs et recherche de correspondances ===

L’indexation est l’opération de stockage des descripteurs SIFT des images de référence, d'une manière qui facilite l’identification des descripteurs correspondants de l'image question. Lowe utilise un [[arbre kd]] pour indexer les descripteurs puis une méthode de recherche dans cet arbre modifiée par rapport à l'approche classique, appelée ''{{lang|en|Best bin first}}''<ref name=Beis1997>{{Chapitre|éditeur=|collection=|série=|titre=Shape indexing using approximate nearest-neighbour search in high-dimensional spaces|titre ouvrage=Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition|auteurs ouvrage=|titre vo=|ref=|volume=|titre volume=|auteur=|prénom=|nom=|auteurs=Jeffrey S. Beis et David G. Lowe|trad=|langue=en|lien langue=|lieu= San Juan, Porto-Rico |année=1997|mois=|jour=|publi=|pages=|format=|isbn=|issn=|présentation en ligne=|lire en ligne=http://www.cs.ubc.ca/~lowe/papers/cvpr97.pdf|partie=|numéro=|chap=|passage= 1000 - 1006 |id=|commentaire=}}.</ref>. Cette dernière est capable de trouver les [[Recherche des plus proches voisins|plus proches voisins]] d'un descripteur question avec une bonne probabilité de façon très économe en temps de calcul. Cet algorithme se fonde sur deux astuces. Tout d'abord, le nombre de boîtes (feuilles de l'arbre kd) à explorer pour trouver le plus proche voisin d'un descripteur question donné est limité à une valeur maximale fixée, par exemple à 200. Ensuite, les nœuds de l'arbre kd sont explorés dans l'ordre de leur distance au descripteur question, grâce à l'utilisation d'une [[file de priorité]] basée sur un [[Tas (informatique)|tas binaire]]. Par distance d'un nœud à un descripteur, on entend distance euclidienne de la boîte englobante des feuilles sous-jacentes à ce descripteur. Dans la plupart des cas, ce procédé fournit la meilleure correspondance, et, dans les cas restants, un descripteur très proche de celle-ci<ref name=lowe2004-110>[[#lowe2004|Lowe (2004)]], {{p.|110}}.</ref>.

De façon à déconsidérer les descripteurs faiblement discriminants (parce qu'ils sont trop souvent présents dans la base de référence, comme par exemple ceux qui sont extraits de motifs d'arrière-plan souvent répétés dans les images), Lowe recherche à la fois le plus proche voisin et le second plus proche voisin de chaque descripteur question. Lorsque le rapport des distances est supérieur à 0,8, la correspondance est éliminée, car considérée comme ambiguës (« bruit »). Par ce critère, Lowe parvient à éliminer 90 % des fausses correspondances en perdant moins de 5 % de correspondances correctes<ref name=lowe2004-111>[[#lowe2004|Lowe (2004)]], {{p.|111}}.</ref>.

=== Identification de clusters par la transformée de Hough ===

On appelle ''pose'' d'un objet le point de vue sous lequel il est photographié, c'est-à-dire sa position dans l'espace par rapport à un repère absolu. En général, la base des images de référence contient différentes poses d'objets identiques. Chaque correspondance individuelle entre points-clés obtenue à l'étape précédente constitue une hypothèse quant à la pose de l'objet sur l'image question par rapport à l'image de référence concernée. Il s'agit donc maintenant de grouper les hypothèses cohérentes entre elles de façon à mettre en correspondance les objets des images et non plus seulement des points isolés. La [[transformée de Hough]] est utilisée pour cela. Elle identifie des groupes de correspondances, que l'on appelle ''clusters'', par un système de vote<ref name=lowe2004-111>[[#lowe2004|Lowe (2004)]], {{p.|111}}.</ref>. 

Alors qu'un objet rigide possède en général six degrés de liberté dans l'espace (trois rotations, trois translations), une correspondance de points-clés entre deux images ne permet de relier que quatre paramètres (deux paramètres de position, l'angle et l'échelle) et constitue donc une approximation parmi toutes les poses possibles. Le sous-espace des poses ainsi paramétré est segmenté en boîtes de façon assez grossière : Lowe utilise des intervalles de 30 degrés pour l’orientation, un facteur de 2 pour l’échelle et 0,25 fois la dimension maximale de l’image de référence concernée (compte tenu du rapport d'échelles) pour la position<ref name="lowe2004" />. Chaque correspondance vote pour la boîte associée. Les correspondances concernant les facteurs d'échelles les plus élevés, considérées comme plus pertinentes, se voient attribuer un poids double de celles concernant les facteurs les plus bas. En outre, pour éviter les effets de bord lors de l’assignation des boîtes, chaque correspondance vote pour les 2 plus proches intervalles dans chaque dimension, c'est-à-dire qu'elle vote 16 fois<ref name=lowe2004-111>[[#lowe2004|Lowe (2004)]], {{p.|111}}.</ref>.

Lors de l'implémentation, Lowe recommande l'utilisation d'une [[table de hachage]] pour éviter de représenter toutes les boîtes possibles en mémoire dont seulement un petit nombre risque d'être non vide<ref name=lowe2004-111>[[#lowe2004|Lowe (2004)]], {{p.|111}}.</ref>.

Lorsque plusieurs correspondances votent pour une même boîte, c'est-à-dire pour la même pose d’un objet, la probabilité que la correspondance soit correcte est largement supérieure à ce que pourrait donner une correspondance isolée. Ainsi, les boîtes contenant au moins trois entrées sont considérées comme des clusters fiables, et retenus pour la suite de l'analyse<ref name=lowe2004-111>[[#lowe2004|Lowe (2004)]], {{p.|111}}.</ref>.

=== Vérification du modèle par la méthode des moindres carrés ===

Les clusters obtenus sont soumis à une procédure de vérification par l’application de la [[méthode des moindres carrés]] aux paramètres de la [[transformation affine]] reliant le modèle (image de référence) à l’image question<ref name=lowe2004-112>[[#lowe2004|Lowe (2004)]], {{p.|112}}.</ref>. La transformation affine d’un point modèle <math>[x y]^{\rm T}</math> en un point image <math>[u v]^{\rm T}</math> peut s’écrire ainsi :

:<math>
\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} m_1 & m_2 \\ m_3 & m_4 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} + \begin{bmatrix} t_x \\ t_y \end{bmatrix}
</math>,

où <math>[t_x t_y]^{\rm T}</math> est la translation du modèle, et où les paramètres <math>m_1</math>, <math>m_2</math>, <math>m_3</math> et <math>m_4</math> modélisent la transformation vectorielle (qui peut être une rotation à l'origine, une similitude, une distorsion, etc.) Pour obtenir les paramètres de la transformation, l’équation ci-dessus est réécrite de manière à regrouper les inconnues dans un vecteur-colonne :

:<math>
\begin{bmatrix} x & y & 0 & 0 & 1 & 0 \\ 0 & 0 & x & y & 0 & 1 \\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \end{bmatrix} \begin{bmatrix}m_1 \\ m_2 \\ m_3 \\ m_4 \\ t_x \\ t_y \end{bmatrix} = \begin{bmatrix} u \\ v \\ \vdots \end{bmatrix}
</math>

Cette équation représente une seule correspondance. Pour obtenir les autres correspondances, il suffit de rajouter pour chacune d’elles deux lignes à la matrice et deux coefficients au second membre. Trois correspondances au minimum sont requises pour espérer une solution. Le système linéaire complet s’écrit alors :

:<math>A\mathbf{x} \approx \mathbf{b}</math>

où <math>A</math> est une matrice <math>m \times n</math> connue (vérifiant généralement <math> m > n</math>), <math>\mathbf{x}</math> un vecteur inconnu de paramètres de dimension <math>n</math> et enfin <math>b</math> un vecteur connu de mesures de dimension <math>m</math>.

Ce système n'a en général pas de solution exacte puisqu'il est sur-dimensionné ; mais ce qui nous intéresse est qu'il est possible d'en trouver une [[pseudo-solution]] <math>\hat{\mathbf{x}}</math> au sens des moindres carrés, c'est-à-dire un vecteur qui minimise l'erreur <math>\|b-A\mathbf{x}\| _2</math>, et qui est aussi solution de l’« équation normale » :
:<math> A^{\rm T} \! A \hat{\mathbf{x}} = A^{\rm T} \mathbf{b}</math>

La solution du système d’équations linéaires, qui s'exprime grâce à la matrice <math>(A^TA)^{-1}A^T</math>, dite [[Pseudo-inverse#Pseudo-inverse de Moore-Penrose|pseudo-inverse de Moore-Penrose]] de <math>A</math>, est donnée par :

:<math> \hat{\mathbf{x}} = (A^{\rm T}\!A)^{-1} A^{\rm T} \mathbf{b}</math> 

Elle minimise la somme des carrés des distances entre les positions des points-clés calculées à partir de la transformée des positions du modèle, et les positions correspondantes sur l'image cible<ref name=lowe2004-112>[[#lowe2004|Lowe (2004)]], {{p.|112}}.</ref>.

Les [[donnée aberrante|données aberrantes]] peuvent ensuite être écartées en vérifiant la concordance de chaque correspondance avec la solution des moindres carrés. Sont éliminées les correspondances qui se situent au-delà de la moitié de l’intervalle d’erreur utilisé pour la discrétisation de l'espace des poses dans la transformée de Hough. Une nouvelle solution par la méthode des moindres carrés est calculée à partir des points restants, et le processus est itéré plusieurs fois. Il est également possible grâce à la transformation estimée par la méthode des moindres carrés de récupérer des correspondances qui avaient été manquées précédemment. À la fin du processus, s’il reste moins de 3 points après suppression des points aberrants, alors la correspondance est dite infructueuse et rejetée<ref name=lowe2004-113>[[#lowe2004|Lowe (2004)]], {{p.|113}}.</ref>.

=== Validation finale par inférence bayésienne ===

La décision finale d’admission ou de rejet d’une mise en correspondance d'objets (hypothèse) se fait sur la base d’un modèle probabiliste<ref>D.G. Lowe, « ''Local feature view clustering for 3D object recognition'' », IEEE [[Conference on Computer Vision and Pattern Recognition]], Kauai, Hawaii, 2001, pp. 682-688.</ref>. Cette méthode détermine en premier lieu le nombre attendu de fausses correspondances, connaissant la taille estimée du modèle, le nombre de points-clés dans la région et la précision de la correspondance. Une analyse par [[inférence bayésienne]] donne ensuite la probabilité que l’objet trouvé sur l'image de référence soit effectivement présent dans l'image question à partir du nombre de vraies correspondances trouvées. Une hypothèse est acceptée si cette probabilité est supérieure à 0,98. À l'exception d'images présentant de grands écarts de luminosité ou des objets ayant subi des transformations sévères ou non rigides, la détection d’objet au moyen de la méthode SIFT parvient ainsi à d'excellents résultats<ref name=lowe2004-113>[[#lowe2004|Lowe (2004)]], {{p.|113}}.</ref>.

== Comparaison des descripteurs SIFT aux autres descripteurs locaux ==

Plusieurs études d’évaluation des performances de différents descripteurs locaux (notamment le descripteur SIFT) ont été menées, dont les principales conclusions sont<ref name=Mikolajczyk2005>{{article|langue=en|prénom1=K.|nom1=Mikolajczyk|prénom2=C.|nom2=Schmid|lien auteur1=|titre=A performance evaluation of local descriptors |périodique=[[IEEE Transactions on Pattern Analysis and Machine Intelligence]] |lien périodique=|volume=27 |numéro=10 |jour=|mois=|année=2005|pages=1615–1630 |issn=|url texte=http://research.microsoft.com/users/manik/projects/trade-off/papers/MikolajczykPAMI05.pdf|consulté le=30 septembre 2010}}</ref> :
* les caractéristiques SIFT et apparentées (en particulier GLOH) donnent les meilleurs taux de [[précision et rappel|rappel]] pour des transformations affines de moins de 50 degrés ; au-delà de cette limite, les résultats deviennent incertains ;
* le caractère discriminant des descripteurs est mesuré en sommant leurs valeurs propres, obtenues par analyse en composantes principales appliquée à ces descripteurs normalisés par leur variance ; cela traduit dans les faits la quantité de variance « capturée » par les différents descripteurs et, partant, leur pouvoir discriminant ; les caractéristiques PCA-SIFT, GLOH et SIFT donnent les meilleurs résultats ;
* les descripteurs à base de SIFT dépassent en performance les autres types de descripteurs locaux en présence de scènes texturées ou, dans une moindre mesure, de scènes structurées ;
* les descripteurs à base de SIFT dépassent en performance les autres descripteurs dans le cas d'images texturées ou structurées ayant subi un changement d'échelle d'un ratio compris entre 2 à 2,5 ou une rotation comprise entre 30 et 45 degrés ;
* les performances de tous les descripteurs, et notamment ceux basé sur l'analyse des contours, tel que [[shape context|{{lang|en|shape context}}]], se dégradent en présence d'images ayant subi un floutage substantiel (du fait de l'estompage des contours) ; en tout état de cause, les descripteurs GLOH, PCA-SIFT et SIFT sont ceux qui présentent Les meilleures performances de tous les descripteurs étudiés. Il en est de même dans les situations de modifications d'illumination.

Il ressort de ces études que les descripteurs à base de SIFT, du fait de leur plus grande robustesse et de leur meilleur pouvoir discriminant, sont particulièrement adaptés à la recherche de correspondances.

Des descripteurs plus récents, tel que SURF, n'ont cependant pas été pris en compte dans ces études, quoiqu'une étude ultérieure ait montré que ce dernier avait des performances similaires à celles de SIFT tout en étant sensiblement plus rapide<ref>{{Chapitre|éditeur=|collection=|série=|titre=Comparing several implementations of two recently published feature detectors|titre ouvrage=Proceedings of the International Conference on Intelligent and Autonomous Systems|auteurs ouvrage=|titre vo=|ref=|volume=|titre volume=|auteur=|prénom=|nom=|auteurs=Johannes Bauer, Niko Sünderhauf et Peter Protzel|trad=|langue=en|lien langue=|lieu=Toulouse|année=2007|mois=|jour=|publi=|pages=|format=|isbn=|issn=|présentation en ligne=|lire en ligne=http://www.tu-chemnitz.de/etit/proaut/rsrc/iav07-surf.pdf|partie=|numéro=|chap=|passage=3|id=|commentaire=}}.</ref>.

Une nouvelle variante du descripteur SIFT, basée sur un [[histogramme]] irrégulier, a récemment été proposée<ref name=irrgrid>{{Chapitre|éditeur=Springer|collection=|série=|titre=Scale invariant feature transform with irregular orientation histogram binning|titre ouvrage=Proceedings of the International Conference on Image Analysis and Recognition|auteurs ouvrage=|titre vo=|ref=|volume=|titre volume=|auteur=|prénom=|nom=|auteurs=Yan Cui, Nils Hasler, Thorsten Thormählen et Hans-Peter Seidel|trad=|langue=en|lien langue=|lieu=Halifax (Canada)|année=2009|mois=|jour=|publi=|pages=|format=|isbn=|issn=|présentation en ligne=|lire en ligne=http://www.mpi-inf.mpg.de/~hasler/download/CuiHasThoSei09igSIFT.pdf|partie=|numéro=|chap=|passage=|id=|commentaire=}}.</ref>, apportant une nette amélioration des performances. Au lieu d'utiliser des grilles de boîtes d'histogrammes de {{nobr|4 × 4}}, toutes les boîtes sont ramenées au centre de la caractéristique, d'où une amélioration significative de la robustesse aux changements d'échelle.

Enfin, comparée au descripteur SIFT standard, la technique SIFT-Rank s'est révélée avoir de meilleures performances<ref name=ToewsCVPR:10>{{Chapitre|éditeur=|collection=|série=|titre=SIFT-Rank: ordinal descriptors for invariant feature correspondence|titre ouvrage=IEEE International Conference on Computer Vision and Pattern Recognition|auteurs ouvrage=|titre vo=|ref=|volume=|titre volume=|auteur=|prénom=|nom=|auteurs=Matthew Toews et William M. Wells III|trad=|langue=en|lien langue=|lieu=|année=2009|mois=|jour=|publi=|pages=|format=|isbn=|issn=|présentation en ligne=|lire en ligne=http://www.cim.mcgill.ca/~mtoews/papers/cvpr09-matt.final.pdf|partie=|numéro=|chap=|passage=172–177|id=|commentaire=}}.</ref>. Un descripteur SIFT-Rank est généré à partir d'un descripteur SIFT standard, en fixant chaque boîte d'histogramme à son rang dans un tableau de boîtes. La [[distance euclidienne]] entre les descripteurs SIFT-Rank est invariante aux modifications monotones arbitraires dans les valeurs des boîtes d'histogramme, et elle est liée à la [[corrélation de Spearman]]<ref name=ToewsCVPR:10 />.

== Applications des descripteurs SIFT ==

Outre la reconnaissance d'objet, les avantageuses propriétés des descripteurs SIFT (caractère discriminant, invariance à la translation, à la rotation et au changement d'échelle et robustesse aux transformations affines en général (distorsions), aux changements de points de vue 3D ainsi qu'aux changements de luminosité) en font un excellent choix pour d'autres applications dont quelques-unes sont énumérées ici.

=== Localisation de robot en environnement inconnu ===

Le but de cette application est de fournir une solution robuste et fiable au problème de localisation de robots en environnement inconnu<ref name=Se2001>{{Chapitre|éditeur=|collection=|série=|titre=Vision-based mobile robot localization and mapping using scale-invariant features|titre ouvrage=Proceedings of the IEEE International Conference on Robotics and Automation|auteurs ouvrage=|titre vo=|ref=|volume=2|titre volume=|auteur=|prénom=|nom=|auteurs=Stephen Se, David Lowe et Jim Little|trad=|langue=en|lien langue=|lieu=|année=2001|mois=|jour=|publi=|pages=|format=|isbn=|issn=|présentation en ligne=|lire en ligne=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.2166&rep=rep1&type=pdf|partie=|numéro=|chap=|passage=2051|id=|commentaire=}}.</ref>. Un système de [[stéréoscopie]] trinoculaire est utilisé pour estimer les positions 3D des points-clés déterminés. Un point-clé n'est pris en compte que s'il apparaît simultanément sur les trois images du système trinoculaire, et à condition que les disparités entre ces images ne soient pas handicapantes, afin de générer le moins possible de données aberrantes<ref name=Se2001-2053>{{Chapitre|éditeur=|collection=|série=|titre=Vision-based mobile robot localization and mapping using scale-invariant features|titre ouvrage=Proceedings of the IEEE International Conference on Robotics and Automation|auteurs ouvrage=|titre vo=|ref=|volume=2|titre volume=|auteur=|prénom=|nom=|auteurs=Stephen Se, David Lowe et Jim Little|trad=|langue=en|lien langue=|lieu=|année=2001|mois=|jour=|publi=|pages=|format=|isbn=|issn=|présentation en ligne=|lire en ligne=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.2166&rep=rep1&type=pdf|partie=|numéro=|chap=|passage=2053|id=|commentaire=}}.</ref>. En mouvement, le robot se localise en comparant les caractéristiques vues avec celles de la carte 3D qu'il a en mémoire ; il enrichit au fur et à mesure cette carte de nouvelles caractéristiques et met à contribution un [[filtre de Kalman]] pour mettre à jour leurs positions tridimensionnelles<ref name=Se2001-2053/>.

=== L’assemblage de panoramas ===

[[Fichier:Rochester_NY.jpg|thumb|right|upright=2|alt=Exemple de détection de zones de recouvrement pour l'assemblage d'un panorama : une série de six images sont assemblées en panorama, une ligne rouge délimitant les zones de recouvrement.|Exemple de détection de zones de recouvrement pour l'assemblage d'un panorama.]]
La mise en correspondance de points d'intérêt par la méthode SIFT peut être utilisée comme première étape d'un processus d'[[assemblage de photos]] entièrement automatisé. Cette application consiste à construire une image panoramique unique à partir de prises de vues successives d'un même sujet (souvent un paysage) décalées les unes des autres par un déplacement dans l'espace. La technique proposée par Brown et Lowe en 2003<ref name=Brown2003>{{Chapitre|éditeur=|collection=|série=|titre=Recognising panoramas|titre ouvrage=Proceedings of the ninth IEEE International Conference on Computer Vision|auteurs ouvrage=|titre vo=|ref=|volume=2|titre volume=|auteur=|prénom=|nom=|auteurs=M. Brown et David G. Lowe|trad=|langue=en|lien langue=|lieu=|année=2003|mois=|jour=|publi=|pages=|format=|isbn=|issn=|présentation en ligne=|lire en ligne=http://graphics.cs.cmu.edu/courses/15-463/2005_fall/www/Papers/BrownLowe.pdf|partie=|numéro=|chap=|passage=1218–1225|id=|commentaire=}}.</ref> a l'avantage de fonctionner sur une collection d'images, sans qu'aucune connaissance a priori ne soit nécessaire sur l'endroit précis de la prise de vue de chacune de ces images, ni même leur ordre ; elle peut aussi exclure de l'assemblage des images qui n'auraient rien à voir avec un panorama donné et même reconstituer en même temps plusieurs panoramas indépendants.

L'idée de l'application est la suivante<ref name=Brown2003/>. Les descripteurs SIFT sont extraits de chacune des images à étudier et mis en correspondance les uns avec les autres (recherche des <math>k</math> plus proches voisins comme dans la méthode SIFT générale). On construit ensuite un graphe dont les sommets sont les différentes images et l'on ajoute des arêtes entre les images qui ont le plus de points-clés en correspondance. Cela signifie qu'elles ont potentiellement des éléments du décor en commun. Pour en être sûr, on essaie de déterminer la transformation géométrique permettant de passer de l'une à l'autre, ou [[Fonction homographique|homographie]], en utilisant un [[RANSAC]] entre les points. En cas de succès, la paire d'images est soumise à une vérification supplémentaire basée sur un modèle probabiliste et destinée à s'assurer que les correspondances ne sont pas dues au hasard. Seules sont conservées dans le graphe les arêtes qui passent le test. Les composantes connexes correspondent alors à autant de panoramas indépendants, qui sont assemblés séparément. Pour cela, une technique dite d'« ajustement de faisceaux » (''{{lang|en|bundle adjustment}}'') est utilisée pour déterminer les paramètres objectifs de la caméra lors de la prise de vue de chaque image, par rapport au panorama global. Une fois que cet ajustement est fait, il est théoriquement possible d'obtenir la valeur de chacun des pixels du panorama global à partir des images de départ. Pour la construction finale, on applique une pondération et un lissage liés à la nécessité d'harmoniser l'exposition des images de départ, de telle sorte que les grandes étendues uniformes (cieux, etc.) ne présentent pas trop de variations dans leur luminosité tout en s'assurant que les détails ne seront pas perdus. Cette technique sophistiquée s'appelle assemblage multi-bande (''{{lang|en|multi band blending}}'').

La technique a donné naissance au logiciel [[Autostitch]]<ref>{{article
| langue          = en
| prénom1         = Matthew
| nom1            = Brown
| prénom2         = David
| nom2            = Lowe
| titre           = Automatic Panoramic Image Stitching using Invariant Features
| périodique      = {{Lang|en|[[International Journal of Computer Vision]]}}
| volume          = 74
| numéro          = 1
| année           = 2007
| pages           = 59-73
| url texte       = http://cvlab.epfl.ch/~brown/papers/ijcv2007.pdf
}}.</ref>.

=== Modélisation, reconnaissance et suivi d'objets 3D ===

Cette problématique consiste à déterminer la position exacte d'une caméra à partir des images qu'elle acquiert, simplement en analysant l'orientation et la position d'un objet précis apparaissant dans son champ de vision. Les applications en sont diverses ; la plus populaire étant sans doute la [[réalité augmentée]], qui consiste à intégrer de nouveaux objets au sein d'une scène filmée, par exemple une bouteille sur une table dès que ladite table apparaît dans le champ de la caméra, et cela si possible en temps réel.

Une façon de traiter cette application a été proposée par Gordon et Lowe en 2006<ref>{{Chapitre|éditeur=Springer-Verlag |collection=|série=|titre=What and where: 3D object recognition with accurate pose |titre ouvrage=Toward Category-Level Object Recognition |auteurs ouvrage=|titre vo=|ref=|volume=|titre volume=|auteur=|prénom=|nom=|auteurs=Iryna Gordon et David G. Lowe |trad=|langue=en |lien langue=|lieu=|année=2006 |mois=|jour=|publi=|pages=|format=|isbn=|issn=|présentation en ligne=|lire en ligne=http://www.cs.ubc.ca/labs/lci/papers/docs2006/lowe_gordon.pdf |partie=|numéro=|chap=|passage=67-82 |id=|commentaire=}}.</ref>. Elle comprend une phase d'apprentissage qui permet d'établir une modélisation 3D de l'objet témoin à partir de l'analyse de différentes prises de vues 2D de celui-ci. Les descripteurs SIFT sont utilisés pour mettre en relation les points-clés entre différentes images et construire un modèle 3D de l'objet (mise en correspondance dite « 2D-to-3D »). Une fois cette phase effectuée, on extrait de chaque image filmée les descripteurs SIFT et on les met en correspondance avec ceux des images de référence, de manière à obtenir les correspondances « 2D-to-3D » de l'image filmée. La position de la caméra est alors déterminée par une technique de RANSAC améliorée (Levenberg-Marquardt) qui minimise l'erreur de projection.

=== Descripteurs SIFT 3D pour la reconnaissance de mouvements humains ===

Des extensions au descripteur SIFT aux données spatio-temporelles de dimension 2+1, dans le contexte de reconnaissance de mouvements humains dans des séquences vidéo, ont été proposées dans la littérature scientifique<ref name=LapLin04>{{Chapitre|éditeur=Springer|collection=|série=|titre=Local descriptors for spatio-temporal recognition |titre ouvrage=ECCV'04 Workshop on Spatial Coherence for Visual Motion Analysis |auteurs ouvrage=|titre vo=|ref=|volume=3667 |titre volume=|auteur=|prénom=|nom=|auteurs=Ivan Laptev et Tony Lindeberg |trad=|langue=en |lien langue=|lieu=|année=2004|mois=|jour=|publi=|pages=|format=|isbn=|issn=|présentation en ligne=|lire en ligne=ftp://ftp.nada.kth.se/CVAP/reports/LapLin04-SCVMA.pdf |partie=|numéro=|chap=|passage=91–103|id=|commentaire=}}.</ref>{{,}}<ref name=Lap>{{article|langue=en |prénom1=Ivan |nom1=Laptev |lien auteur1=|prénom2=Barbara |nom2=Caputo |prénom3=Christian |nom3=Schuldt |prénom4=Tony |nom4=Lindeberg |titre=Local velocity-adapted motion events for spatio-temporal recognition |périodique=Computer Vision and Image Understanding |lien périodique=|volume=108 |numéro=|jour=|mois=|année=2007 |pages=207–229 |issn=|url texte=http://www.csc.kth.se/cvap/abstracts/LapCapSchLin07-CVIU.html |consulté le=}}.</ref>{{,}}<ref name=Scovanner2007>{{Chapitre|éditeur=|collection=|série=|titre=A 3-dimensional SIFT descriptor and its application to action recognition|titre ouvrage=|auteurs ouvrage=Proceedings of the 15th International Conference on Multimedia|titre vo=|ref=|volume=|titre volume=|auteur=|prénom=|nom=|auteurs=Paul Scovanner, Saad Ali et Mubarak Shah |trad=|langue=en |lien langue=|lieu=|année=2007 |mois=|jour=|publi=|pages=|format=|isbn= 978-1-59593-702-5 |issn=|présentation en ligne=|lire en ligne=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.75.2677&rep=rep1&type=pdf |partie=|numéro=|chap=|passage=357–360 |id=|commentaire=}}.</ref>. Le calcul d'histogrammes dépendants de positions locales est étendu, dans l'algorithme SIFT, de 2 à 3 dimensions pour adapter les caractéristiques SIFT au domaine spatio-temporel. Pour réaliser une reconnaissance de mouvements humains dans une séquence vidéo, un échantillonnage de vidéos d'apprentissage est effectué soit à des points d'intérêt spatio-temporel, soit à des positions, instants et échelles aléatoirement choisis. Les régions spatio-temporelles autour de ces points d'intérêt sont décrites au moyen du descripteur SIFT 3D. Ces descripteurs sont enfin rangés en clusters formant un [[sac de mots|modèle de sac de mots]] spatio-temporel. Les descripteurs SIFT 3D extraits des vidéos de test sont enfin comparés à ces « mots ».

Les auteurs confirment l'obtention de meilleurs résultats grâce à cette approche à base de descripteur 3D que par les autres approches utilisant de simples descripteurs SIFT 2D ou les magnitudes de gradients<ref>{{Chapitre|éditeur=|collection=|série=|titre=Unsupervised learning of human action categories using spatial-temporal words|titre ouvrage=Proceedings of the British Machine Vision Conference |auteurs ouvrage=|titre vo=|ref=|volume=|titre volume=|auteur=|prénom=|nom=|auteurs=Juan Carlos Niebles, Hongcheng Wang et Li Fei-Fei1 |trad=|langue=en |lien langue=|lieu=Edinburgh |année=2006|mois=|jour=|publi=|pages=|format=|isbn=|issn=|présentation en ligne=|lire en ligne=http://vision.stanford.edu/documents/NieblesHWangFei-Fei_BMVC2006.pdf|partie=|numéro=|chap=|passage=|id=|commentaire=}}.</ref>.

=== Analyse du cerveau humain au moyen d’images RMN 3D ===

La [[morphométrie]] basée sur des descripteurs d'image (FBM, ''{{lang|en|feature based morphometry}}''), est une technique d'analyse et de classification d'images 3D du cerveau humain obtenus par [[Imagerie par résonance magnétique|résonance magnétique]]<ref name=ToewsNI:10>{{article|langue=en |prénom1=Matthew |nom1=Toews |lien auteur1=|prénom2=William M. |nom2=Wells III |prénom3=D. Louis |nom3=Collins |prénom4=Tal |nom4=Arbel |titre=Feature-based morphometry: discovering group-related anatomical patterns |périodique=NeuroImage |lien périodique=NeuroImage|volume=49|numéro=3 |jour=|mois=|année=2010 |pages=2318-2327|issn=|url texte=http://www.cim.mcgill.ca/~mtoews/papers/matt_neuroimage09.pdf |consulté le=}}.</ref>. Grâce à la technique SIFT, l'image est vue comme un assemblage géométrique de structures indépendantes (zones d'intérêt). En utilisant un modèle probabiliste assez fin et en se basant sur l'observation de différentes classes de sujets dûment étiquetés, par exemple des sujets sains d'un côté et des sujets atteints de la [[maladie d'Alzheimer]] de l'autre, la technique FBM parvient à regrouper les structures similaires d'une image à l'autre et à les classifier selon l’étiquetage. Un système ainsi entraîné devient capable d'étiqueter automatiquement une image étrangère : sur un ensemble de test constitué d'environ 200 images IRM du cerveau humain, la détection des cas sains et des cas atteints de la maladie d'Alzheimer est correcte dans 80 % des cas, précisent les auteurs.

== Variantes et extensions ==

; RIFT
: RIFT<ref>{{Chapitre|éditeur=|collection=|série=|titre=Semi-local affine parts for object recognition|titre ouvrage= Proceedings of the British Machine Vision Conference |auteurs ouvrage=|titre vo=|ref=|volume=|titre volume=|auteur=|prénom=|nom=|auteurs= Svetlana Lazebnik, Cordelia Schmid et Jean Ponce |trad=|langue=en|lien langue=|lieu=|année=2004|mois=septembre|jour=|publi=|pages=|format=|isbn=|issn=|présentation en ligne=|lire en ligne=http://www-cvr.ai.uiuc.edu/ponce_grp/publication/paper/bmvc04.pdf|partie=|numéro=|chap=|passage=959-968|id=|commentaire=}}.</ref> est une variante de SIFT adaptée aux images texturées sur lesquelles la notion d'orientation principale n'a pas vraiment de sens, tout en étant invariante aux rotations. Le descripteur est construit en utilisant des patches circulaires normalisés divisés en anneaux également circulaires et d’égale largeur ; un histogramme d’orientation de gradient est calculé à partir de chacun de ces anneaux. Pour garantir l’invariance à la rotation, l'orientation est mesurée pour chaque point du centre vers l’extérieur.

; G-RIF
: G-RIF<ref>{{Chapitre|éditeur=|collection=|série=|titre=Object recognition using a generalized robust invariant feature and Gestalt’s law of proximity and similarity|titre ouvrage= Conference on Computer Vision and Pattern Recognition Workshop |auteurs ouvrage=|titre vo=|ref=|volume=|titre volume=|auteur=|prénom=|nom=|auteurs= Sungho Kim, Kuk-Jin Yoon et In So Kweon |trad=|langue=en|lien langue=|lieu= New York |année=2006|mois=|jour=|publi=|pages=|format=|isbn=0-7695-2646-2|issn=|présentation en ligne=|lire en ligne=http://rcv.kaist.ac.kr/v2/bbs/board.php?bo_table=rs_publications&wr_id=152|partie=|numéro=|chap=|passage=726-741|id=|commentaire=}}.</ref> (ou ''{{lang|en|Generalized robust invariant feature}}'') est un descripteur de contexte regroupant les orientations des contours, les densités des contours et les informations de teintes dans une forme unifiée, combinant information perceptive et codage spatial. Une nouvelle méthode de classification est utilisée, basée sur un modèle probabiliste utilisant les votes des descripteurs locaux dans un voisinage.

; SURF
: Présenté comme une solution de hautes performances capable de d’approcher et même de dépasser les schémas précédents du point de vue répétitivité, distinctivité et robustesse, [[SURF]]<ref>{{Chapitre|éditeur=|collection=|série=|titre=SURF: Speeded Up Robust Features|titre ouvrage= Proceedings of the European Conference on Computer Vision |auteurs ouvrage=|titre vo=|ref=|volume=|titre volume=|auteur=|prénom=|nom=|auteurs=Herbert Bay, Tinne Tuytelaars et Luc Van Gool|trad=|langue=en|lien langue=|lieu=|année=|mois=mai|jour=2006|publi=|pages=|format=|isbn=|issn=|présentation en ligne=|lire en ligne=http://www.vision.ee.ethz.ch/~surf/eccv06.pdf|partie=|numéro=|chap=|passage= 404-417|id=|commentaire=}}.</ref> (ou ''[[Speeded Up Robust Features|{{lang|en|Speeded Up Robust Features}}]]'') est un détecteur de points d’intérêts invariant aux changements d’échelle et aux rotations. Afin de réduire les temps de traitement, il fait usage d'[[image intégrale|images intégrales]] pour le calcul de la [[produit de convolution|convolution]], et reprend les points forts des meilleurs détecteurs et descripteurs l’ayant précédé, en utilisant des mesures à base de [[matrice hessienne|matrices hessiennes]] rapides pour le détecteur et un descripteur basé sur les distributions. Il décrit une distribution de réponses d’[[ondelette de Haar|ondelettes de Haar]] dans le voisinage du point d’intérêt. Seulement 64 dimensions sont utilisées, ce qui permet de réduire le temps de calcul des caractéristiques et de recherche des correspondances. L’étape d’indexation est basée sur le signe du [[Laplacien]], d’où l'accélération de la recherche de correspondances et l’amélioration de la robustesse du descripteur.

; PCA-SIFT et GLOH
: PCA-SIFT<ref name=pca-sift>{{Chapitre|éditeur=|collection=|série=|titre=PCA-SIFT: A More Distinctive Representation for Local Image Descriptors|titre ouvrage=Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition|auteurs ouvrage=|titre vo=|ref=|volume=2|titre volume=|auteur=|prénom=|nom=|auteurs=Yan Ke et R. Sukthankar |trad=|langue=en|lien langue=|lieu=|année=2004|mois=|jour=|publi=|pages=|format=|isbn=|issn=|présentation en ligne=|lire en ligne=http://www.cs.cmu.edu/~rahuls/pub/cvpr2004-keypoint-rahuls.pdf|partie=|numéro=|chap=|passage=506-513|id=|commentaire=}}.</ref> et GLOH<ref>{{article|langue=en|prénom1=Krystian|nom1=Mikolajczyk|lien auteur1=|prénom2=Cordelia|nom2=Schmid|titre=A performance evaluation of local descriptors |périodique=IEEE Transactions on Pattern Analysis and Machine Intelligence |lien périodique=|volume=27|numéro=10|jour=|mois=octobre|année=2005|pages=1615-1630|issn=|url texte=http://lear.inrialpes.fr/pubs/2005/MS05/mikolajczyk_pami05.pdf|consulté le=|langue=en}}.</ref> sont des variantes de SIFT. Le descripteur PCA-SIFT est un vecteur de gradients d’image dans les directions <math>x</math> et <math>y</math> calculé à l’intérieur de la région de support. La région du gradient est échantillonnée en {{nobr|39 × 39}} positions, générant un vecteur de dimension 3042. Cette dimension est réduite à 20 par la méthode d’[[analyse en composantes principales]]<ref>{{Chapitre|éditeur=|collection=|série=|titre=PCA-SIFT: A More Distinctive Representation for Local Image Descriptors|titre ouvrage=Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition|auteurs ouvrage=|titre vo=|ref=|volume=2|titre volume=|auteur=|prénom=|nom=|auteurs=Yan Ke et R. Sukthankar |trad=|langue=en|lien langue=|lieu=|année=2004|mois=|jour=|publi=|pages=|format=|isbn=|issn=|présentation en ligne=|lire en ligne=http://www.cs.cmu.edu/~rahuls/pub/cvpr2004-keypoint-rahuls.pdf|partie=|numéro=|chap=|passage=508|id=|commentaire=}}.</ref>. GLOH (pour ''{{lang|en|Gradient location-orientation histogram}}'') est une extension du descripteur SIFT dont la robustesse et le caractère distinctif ont été améliorés. Comparé à SIFT, le descripteur GLOH est calculé à partir d’une grille de positions log-polaires avec trois boîtes en direction radiale (de rayons 6, 11 et 15) et huit en direction angulaire, soit 17 boîtes de position. La boîte centrale n’est pas divisée dans les directions angulaires. Les orientations des gradients sont quantifiées en 16 boîtes, d’où 272 histogrammes de boîtes. Ici aussi la taille du descripteur est réduite au moyen de l’analyse en composantes principales. La [[matrice de variance-covariance]] de l’[[Analyse en composantes principales|ACP]] est estimée sur les patches d’image collectés de différentes images. Les 128 plus importants vecteurs propres sont utilisés pour la description. 

; Autres méthodes
: Une équipe formée autour de Wagner a développé deux algorithmes de détection d’objet spécialement pensés pour tenir compte des limitations techniques des [[Photophone (téléphone-appareil-photo)|photophones]]<ref name=Wagner>{{Chapitre|éditeur=|collection=|série=|titre=Pose tracking from natural features on mobile phones |titre ouvrage=7th IEEE/ACM International Symposium on Mixed and Augmented Reality |auteurs ouvrage=|titre vo=|ref=|volume=|titre volume=|auteur=|prénom=|nom=|auteurs=Daniel Wagner, Gerhard Reitmayr, Alessandro Mulloni, Tom Drummond et Dieter Schmalstieg|trad=|langue=en|lien langue=|lieu=|année=2008|mois=septembre|jour=|publi=|pages=|format=|isbn=|issn=|présentation en ligne=|lire en ligne=http://mi.eng.cam.ac.uk/~gr281/docs/WagnerIsmar08NFT.pdf |partie=|numéro=|chap=|passage=125-134|id=Wagner|commentaire=}}.</ref>, mettant en œuvre le [[détection_de_zones_d'intérêt#Points_d'intérêt|détecteur de coins]] [[Features from Accelerated Segment Test|FAST]] pour la détection de caractéristiques. Les deux algorithmes distinguent deux phases : une phase de préparation hors-ligne, au cours de laquelle les caractéristiques sont calculées pour différents niveaux d’échelles, et une phase de traitement en ligne, au cours de laquelle seules les caractéristiques du niveau d’échelle fixé de la photographie issue du [[Photophone (téléphone-appareil-photo)|photophone]] sont calculées. De plus, des caractéristiques sont obtenues à partir d’une [[imagette]] d’une taille fixe de {{nobr|15 × 15}} pixels, formant un descripteur SIFT réduit à 36 dimensions<ref>[[#Wagner|Wagner ''et al''.]] (2008), {{p.|128}}.</ref>. Cette approche a été ensuite améliorée par l’intégration d’un [[arbre de vocabulaire évolutif]] dans le pipeline de détection<ref>{{Chapitre|éditeur=|collection=|série=|titre=What is That? Object Recognition from Natural Features on a Mobile Phone|titre ouvrage=Proceedings of the Workshop on Mobile Interaction with the Real World|auteurs ouvrage=|titre vo=|ref=|volume=|titre volume=|auteur=|prénom=|nom=|auteurs=Niels Henze, Torben Schinke et Susanne Boll|trad=|langue=en|lien langue=|lieu=|année=2009|mois=|jour=|publi=|pages=|format=|isbn=|issn=|présentation en ligne=|lire en ligne=http://mirw09.offis.de/paper/What%20is%20That%20-%20Object%20Recognition%20from%20Natural%20Features%20on%20a%20Mobile%20Phone.pdf|partie=|numéro=|chap=|passage=|id=|commentaire=}}.</ref>. Il s’ensuit une meilleure reconnaissance des objets sur les photophones. Le principal inconvénient de cette approche est son important besoin de [[mémoire vive]]<ref>[[#Wagner|Wagner ''et al''.]] (2008), {{p.|133-134}}.</ref>.

== Notes et références ==
=== Notes ===

{{Références|groupe=note}}

=== Références ===

{{Traduction/Référence|en|Scale-invariant feature transform|379046521}}

{{Références|colonnes=2}}

== Annexes ==
=== Articles connexes ===
*[[Autostitch]]
*[[Détection de zones d'intérêt]]
*[[Speeded Up Robust Features]] (SURF)

=== Bibliographie ===

==== Publications ====

*{{article|langue=en|prénom1= YuanBin |nom1=Wang |prénom2= Zhang |nom2=Bin |prénom3= Yu |nom3=Ge |titre= The Invariant Relations of 3D to 2D Projection of Point Sets |périodique=Journal of Pattern Recognition Research |lien périodique=|volume=3|numéro=1|jour=14|mois=juin|année=2008|pages=14-23|issn=|url texte=http://www.jprr.org/index.php/jprr/article/viewFile/26/13|consulté le=27 septembre 2010}}
* {{article|langue=en|prénom1=David G.|nom1=Lowe|lien auteur1=|titre=Distinctive Image Features from Scale-Invariant Keypoints|périodique=[[International Journal of Computer Vision]]|lien périodique=|volume=60|numéro=2|jour=|mois=|année=2004|pages=91-110|issn=|url texte=http://www.cs.jhu.edu/~misha/ReadingSeminar/Papers/Lowe04.pdf|consulté le=27 septembre 2010}}
* {{article|langue=en|prénom1=Krystian|nom1=Mikolajczyk|prénom2=Cordelia|nom2=Schmid|lien auteur1=|titre=A performance evaluation of local descriptors|périodique=[[IEEE Transactions on Pattern Analysis and Machine Intelligence]]|lien périodique=|volume=27|numéro=10|jour=|mois=|année=2005|pages=1615-1630|issn=|url texte=http://lear.inrialpes.fr/pubs/2005/MS05/mikolajczyk_pami05.pdf|consulté le=27 septembre 2010}}
* {{article|langue=en|prénom1=Svetlana |nom1=Lazebnik |prénom2=Cordelia |nom2=Schmid |prénom3=Jean |nom3=Ponce |titre=Semi-Local Affine Parts for Object Recognition |périodique= Proceedings of the British Machine Vision Conference |lien périodique=|volume=2|numéro=|jour=|mois=septembre |année=2004 |pages= 959-968 |issn=|url texte=http://www-cvr.ai.uiuc.edu/ponce_grp/publication/paper/bmvc04.pdf|consulté le=27 septembre 2010}}

==== Implémentations de SIFT ====
* {{en}} {{Lien web | url = http://blogs.oregonstate.edu/hess/code/sift/ | titre = SIFT Library | auteur = Rob Hess | année = 2010 | éditeur = | consulté le = 27 septembre 2010}}.
* {{en}} {{Lien web | url = http://www.cs.ubc.ca/~lowe/keypoints/ | titre = Demo Software: SIFT Keypoint Detector | auteur = David G. Lowe | année = 2005 | éditeur = | consulté le = 27 septembre 2010}}.
* {{en}} {{Lien web | url = http://www.cs.cmu.edu/~yke/pcasift/ | titre = PCA-SIFT: A More Distinctive Representation for Local Image Descriptors | auteur = Yan Ke et Rahul Sukthankar | année = 2004 | éditeur = | consulté le = 27 septembre 2010}}.
* {{en}} {{Lien web | url =http://www.ipol.im/pub/algo/my_affine_sift/ | titre = ASIFT: A New Framework for Fully Affine Invariant Comparison | auteur = Jean-Michel Morel et Guoshen Yu | année = 2009 | éditeur = | isbn=2105-1232| consulté le = 27 septembre 2010}}.
* {{en}} {{Lien web | url =http://www.vlfeat.org/ | titre = VLFeat.org | auteur = Andrea Vedaldi et Brian Fulkerson | année = 2005 | éditeur = | consulté le = 27 septembre 2010}}.
* {{en}} {{Lien web | url =http://www.cs.cityu.edu.hk/~wzhao2/lip-vireo.htm | titre = Lip-vireo | auteur = Wan-Lei Zhao | année = 2010 | éditeur = | consulté le = 27 septembre 2010}}.
* Depuis sa version 2.2 (décembre 2010), [[OpenCV]] intègre une implémentation de SIFT.

{{Portail|Imagerie numérique}}
{{Article de qualité|oldid=61383818|date=26 janvier 2011}}

[[Catégorie:Vision artificielle]]

[[ar:تحويل صفة صورة غير مرتبط بمقياس]]
[[ca:Scale-invariant feature transfrom]]
[[de:Scale-invariant feature transform]]
[[en:Scale-invariant feature transform]]
[[fa:تبدیل ویژگی مقیاس‌نابسته]]
[[ht:SIFT]]
[[it:Scale-invariant feature transform]]
[[nl:Scale-invariant feature transform]]
[[pl:Skaloniezmiennicze przekształcenie cech]]
[[sv:SIFT]]
[[zh:尺度不變特徵轉換]]