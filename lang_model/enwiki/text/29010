{{Unreferenced|date=November 2007}}
In [[mathematics]], a '''probability density function (pdf)''' is a function that represents a [[probability distribution]] in terms of [[integral]]s.

Formally, a probability distribution has density ''f'', if ''f'' is a non-negative [[Lebesgue integration|Lebesgue-integrable]] function <math>\mathbb{R}\to\mathbb{R}</math> such that the probability of the interval [''a'', ''b''] is given by

:<math>\boldsymbol{Fx}=\int_a^b f(x)\,dx</math>

for any two numbers ''a'' and ''b''. This implies that the total integral of ''f'' must be 1. Conversely, any non-negative Lebesgue-integrable function with total integral 1 is the probability density of a suitably defined{{clarifyme}} probability distribution.

Intuitively, if a probability distribution has density ''f''(''x''), then the infinitesimal [[interval (mathematics)|interval]] [''x'', ''x'' + d''x''] has probability ''f''(''x'')&nbsp;d''x''.

Informally, a probability density function can be seen as a "smoothed out" version of a [[histogram]]: if one empirically samples enough values of a [[continuous random variable]], producing a histogram depicting relative frequencies of output ranges, then this histogram will resemble the random variable's probability density, assuming that the output ranges are sufficiently narrow.

==Simplified explanation==
A '''probability density function''' is any function ''f''(''x'') that describes the probability density in terms of the input variable ''x'' in a manner described below. 
* ''f''(''x'') is greater than or equal to zero for all values of ''x'' 
* The total area under the graph is 1:
::<math> \int_{-\infty}^\infty \,f(x)\,dx = 1. </math> 

The actual probability can then be calculated by taking the integral of the function ''f''(''x'') by the integration interval of the input variable ''x''. 

For example: the probability of the variable ''X'' being within the interval [4.3,7.8] would be

:<math>\Pr(4.3 \leq X \leq 7.8) = \int_{4.3}^{7.8} f(x)\,dx.</math>

==Further details==
For example, the [[Uniform distribution (continuous)|continuous uniform distribution]] on the interval [0,1] has probability density ''f''(''x'') = 1 for 0 <U+2264> ''x'' <U+2264> 1 and ''f''(''x'') = 0 elsewhere. The standard [[normal distribution]] has probability density

:<math>f(x)={e^{-{x^2/2}}\over \sqrt{2\pi}}</math>

If a random variable ''X'' is given and its distribution admits a probability density function ''f''(''x''), then the [[expected value]] of ''X'' (if it exists) can be calculated as

:<math>\operatorname{E}(X)=\int_{-\infty}^\infty x\,f(x)\,dx</math>

Not every probability distribution has a density function: the distributions of [[discrete random variable]]s do not; nor does the [[Cantor distribution]], even though it has no discrete component, i.e., does not assign positive probability to any individual point.

A distribution has a density function if and only if its [[cumulative distribution function]] ''F''(''x'') is [[absolute continuity|absolutely continuous]]. In this case: ''F'' is [[almost everywhere]] [[derivative|differentiable]], and its derivative can be used as probability density:

:<math>\frac{d}{dx}F(x) = f(x)</math>

If a probability distribution admits a density, then the probability of every one-point set {''a''} is zero. 

It is a common mistake to think of ''f''(''x'') as the probability of {''x''}, but this is incorrect; in fact, ''f''(''x'') will often be bigger than 1 - consider a random variable that is [[uniform distribution|uniformly distributed]] between 0 and <U+00BD>.  Loosely, one may think of ''f''(''x'')&nbsp;''dx'' as the probability that a random variable whose probability density function is ''f'' , is in the interval from ''x'' to ''x''&nbsp;+&nbsp;''dx'', where ''dx'' is an infinitely small increment.

Two probability densities ''f'' and ''g'' represent the same [[probability distribution]] precisely if they differ only on a set of [[Lebesgue measure|Lebesgue]] [[measure zero]].

In the field of [[statistical physics]], a non-formal reformulation of the relation above between the derivative of the [[cumulative distribution function]] and the probability density function is generally used as the definition of the probability density function. This alternate definition is the following:

If ''dt'' is an infinitely small number, the probability that <math>X</math> is included within the interval (''t'',&nbsp;''t''&nbsp;+&nbsp;''dt'') is equal to <math>f(t)\,dt</math>, or:

:<math>\Pr(t<X<t+dt) = f(t)\,dt~</math>

== Link between discrete and continuous distributions ==

The definition of a probability density function at the start of this page makes it possible to describe the variable associated with a continuous distribution using a set of binary discrete variables associated with the intervals [''a'';&nbsp;''b''] (for example, a variable being worth 1 if ''X'' is in [''a'';&nbsp;''b''], and 0 if not).

It is also possible to represent certain discrete random variables using a density of probability, via the [[Dirac delta function]]. For example, let us consider a binary discrete [[random variable]] taking &minus;1 or 1 for values, with probability <U+00BD> each.

The density of probability associated with this variable is: 

:<math>f(t) = \frac{1}{2}(\delta(t+1)+\delta(t-1)).</math>

More generally, if a discrete variable can take 'n' different values among real numbers, then the associated probability density function is: 

:<math>f(t) = \sum_{i=1}^nP_i\, \delta(t-x_i),</math>

where <math>x_1, \ldots, x_n</math> are the discrete values accessible to the variable and <math>P_1, \ldots, P_n</math> are the probabilities associated with these values.

This expression allows for determining statistical characteristics of such a discrete variable (such as its [[mean]], its [[variance]] and its [[kurtosis]]), starting from the formulas given for a continuous distribution. 

In [[physics]], this description is also useful in order to characterize mathematically the initial configuration of a [[Brownian movement]].

== Probability function associated to multiple variables ==<!-- This section is linked from [[Sufficiency (statistics)]] -->

For continuous [[random variable]]s <math>X_1,\ldots,X_n</math>, it is also possible to define a probability density function associated to the set as a whole, often called '''joint probability density function'''. This density function is defined as a function of the ''n'' variables, such that, for any domain ''D'' in the ''n''-dimensional space of the values of the variables <math>X_1,\ldots,X_n</math>, the probability that a realisation of the set variables falls inside the domain ''D'' is

:<math>\Pr \left( X_1,\ldots,X_N \isin D \right) 
 = \int_D f_{X_1,\dots,X_n}(x_1,\ldots,x_N)\,dx_1 \cdots dx_N.</math>

For ''i''=1, 2, <U+2026>,''n'', let <math>f_{X_i}(x_i)</math> be the probability density function associated to variable <math>X_i</math> alone. This probability density can be deduced from the probability densities associated of the random variables <math>X_1,\ldots,X_n</math> by integrating on all values of the ''n''&nbsp;&minus;&nbsp;1 other variables:

:<math>f_{X_i}(x_i) = \int f(x_1,\ldots,x_n)\, dx_1 \cdots dx_{i-1}\,dx_{i+1}\cdots dx_n</math>

=== Independence ===

Continuous random variables <math>X_1,\ldots,X_n</math> are all [[statistical independence|independent]] from each other if and only if

:<math>f_{X_1,\dots,X_n}(x_1,\ldots,x_N) = f_{X_1}(x_1)\cdots f_{X_n}(x_n).</math>

=== Corollary ===

If the joint probability density function of a vector of ''n'' random variables can be factored into a product of ''n'' functions of one variable

:<math>f_{X_1,\dots,X_n}(x_1,\ldots,x_n) = f_1(x_1)\cdots f_n(x_n),</math>

then the ''n'' variables in the set are all [[statistical independence|independent]] from each other, and the marginal probability density function of each of them is given by

:<math>f_{X_i}(x_i) = \frac{f_i(x_i)}{\int f_i(x)\,dx}.</math>

=== Example === 

This elementary example illustrates the above definition of multidimensional probability density functions in the simple case of a function of a set of two variables. Let us call <math>\vec R</math> a 2-dimensional random vector of coordinates <math>(X,Y)</math>: the probability to obtain <math>\vec R</math> in the quarter plane of positive ''x'' and ''y'' is

:<math>\Pr \left( X > 0, Y > 0 \right)
 = \int_0^\infty \int_0^\infty f_{X,Y}(x,y)\,dx\,dy.</math>

==Sums of independent random variables==

The probability density function of the sum of two [[statistical independence|independent]] random variables ''U'' and ''V'', each of which has a probability density function is the [[convolution]] of their separate density functions:

:<math> f_{U+V}(x) = \int_{-\infty}^\infty f_U(y) f_V(x - y)\,dy. </math>

== Dependent variables ==

If the probability density function of an independent random variable ''x'' is given as ''f''(''x''), it is possible (but often not necessary; see below) to calculate the probability density function of some variable ''y'' which depends on ''x''. This is also called a "change of variable" and is in practice used to generate a random variable of arbitrary shape "f" using a known (for instance uniform) random number generator. If the dependence is ''y'' = ''g''(''x'') and the function ''g'' is [[Monotonic function|monotonic]], then the resulting density function is

: <math>\left| \frac{1}{g'(g^{-1}(y))} \right| \cdot f(g^{-1}(y)).</math>

Here ''g''<sup>&minus;1</sup> denotes the [[inverse function]] and ''g''' denotes the [[derivative]].

For functions which are not monotonic the probability density function for ''y'' is

:<math>\sum_{k}^{n(y)} \left| \frac{1}{g'(g^{-1}_{k}(y))} \right| \cdot f(g^{-1}_{k}(y))</math>

where ''n''(''y'') is the number of solutions in ''x'' for the equation ''g''(''x'') = ''y'', and <math>g^{-1}_{k}(y)</math> are these solutions.

It is tempting to think that in order to find the expected value E(''g''(''X'')) one must first find the probability density of ''g''(''X'').  However, rather than computing

:<math> E(g(X)) = \int_{-\infty}^\infty x f_{g(X)}(x)\,dx, </math>

one may find instead

:<math>E(g(X)) = \int_{-\infty}^\infty g(x) f_X(x)\,dx.</math>

The values of the two integrals are the same in all cases in which both ''X'' and ''g''(''X'') actually have probability density functions.  It is not necessary that ''g'' be a [[one-to-one function]].  In some cases the latter integral is computed much more easily than the former.

=== Multiple variables ===

The above formulas can be generalized to variables (which we will again call ''y'') depending on more than one other variables. ''f''(''x''<sub>0</sub>, ''x''<sub>1</sub>, ..., ''x''<sub>m-1</sub>) shall denote the probability density function of the variables ''y'' depends on, and the dependence shall be ''y'' = ''g''(''x''<sub>0</sub>, ''x''<sub>1</sub>, ..., ''x''<sub>m-1</sub>). Then, the resulting density function is

: <math> \int_{y = g(x_0, x_1, \dots, x_{m-1})} \frac{f(x_0, x_1,\dots, x_{m-1})}\sqrt{\sum_{j=0}^{j<m} (\frac{\partial g}{\partial x_j}(x_0, x_1, \dots , x_{m-1}))^2} \; dV</math>

where the integral is over the entire (m-1)-dimensional solution of the subscripted equation and the symbolic ''dV'' must be replaced by a parametrization of this solution for a particular calculation; the variables ''x''<sub>0</sub>, ''x''<sub>1</sub>, ..., ''x''<sub>m-1</sub> are then of course functions of this parametrization.

== Finding moments and variance ==

In particular, the ''n''th [[moment (mathematics)|moment]] E(''X''<sup>''n''</sup>) of the probability distribution of a random variable ''X'' is given by

:<math> E(X^n) = \int_{-\infty}^\infty x^n f_X(x)\,dx,</math>

and the [[variance]] is

:<math> \operatorname{var}(X) = E((X - E(X))^2 = \int_{-\infty}^\infty (x-E(X))^2 f_X(x)\,dx  </math>

or, expanding, gives: 

:<math> \operatorname{var}(X) = E(X^2) - [E(X)]^2 </math>.

== Bibliography ==
*{{cite book
 | author = Pierre Simon de Laplace
 | year = 1812
 | title = Analytical Theory of Probability}}
:: The first major treatise blending calculus with probability theory, originally in French: ''Th<U+00E9>orie Analytique des Probabilit<U+00E9>s''.

*{{cite book
 | author = Andrei Nikolajevich Kolmogorov
 | year = 1950
 | title = Foundations of the Theory of Probability}}
:: The modern measure-theoretic foundation of probability theory; the original German version (''Grundbegriffe der Wahrscheinlichkeitrechnung'') appeared in 1933.

*{{cite book
 | author = Patrick Billingsley
 | title = Probability and Measure
 | publisher = John Wiley and Sons
 | location = New York, Toronto, London
 | year = 1979}}

*{{cite book
 | author = David Stirzaker
 | year = 2003
 | title = Elementary Probability}}
:: Chapters 7 to 9 are about continuous variables. This books is filled with theory and mathematical proofs.


==See also ==
* [[likelihood function]]
* [[probability distribution]]
* [[probability mass function]]
* [[exponential family]]
* [[density estimation]]
* [[conditional probability density function]]
* [[Probability vector]]
* [[Secondary measure]]

[[Category:Probability theory]]
[[Category:Fundamental physics concepts]]

[[ca:Funci<U+00F3> de densitat de probabilitat]]
[[da:Sandsynlighedst<U+00E6>thedsfunktion]]
[[de:Dichtefunktion]]
[[es:Funci<U+00F3>n de densidad]]
[[fr:Densit<U+00E9> de probabilit<U+00E9>]]
[[id:Fungsi kepekatan probabilitas]]
[[it:Funzione di densit<U+00E0> di probabilit<U+00E0>]]
[[hu:S<U+0171>r<U+0171>s<U+00E9>gf<U+00FC>ggv<U+00E9>ny]]
[[nl:Kansdichtheid]]
[[no:Tetthetsfunksjon]]
[[pl:G<U+0119>sto<U+015B><U+0107> prawdopodobie<U+0144>stwa]]
[[pt:Fun<U+00E7><U+00E3>o densidade]]
[[ru:<U+041F><U+043B><U+043E><U+0442><U+043D><U+043E><U+0441><U+0442><U+044C> <U+0432><U+0435><U+0440><U+043E><U+044F><U+0442><U+043D><U+043E><U+0441><U+0442><U+0438>]]
[[su:Probability density function]]
[[sv:T<U+00E4>thetsfunktion]]
[[vi:H<U+00E0>m m<U+1EAD>t <U+0111><U+1ED9> x<U+00E1>c su<U+1EA5>t]]
[[zh:<U+6A5F><U+7387><U+5BC6><U+5EA6><U+51FD><U+6578>]]
