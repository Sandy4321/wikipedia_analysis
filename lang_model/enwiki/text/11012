[[File:Pompeii - Osteria della Via di Mercurio - Dice Players.jpg|thumb|Ancient [[fresco]] of dice players in [[Pompei]].]]
In ancient '''history''', the concepts of chance and '''[[randomness]]''' were intertwined with that of fate. Many ancient peoples threw dice to determine fate, and this later evolved into games of chance. Most ancient cultures used various methods of [[divination]] to attempt to circumvent randomness and fate.<ref>''Handbook to Life in Ancient Rome'', Lesley Adkins, 1998 ISBN 0195123328 p. 279</ref><ref>''Religions of the Ancient World'', Sarah Iles Johnston, 2004 ISBN 0674015177 p. 370</ref>

The Chinese were perhaps the earliest people to formalize odds and chance 3,000 years ago. The Greek philosophers discussed randomness at length, but only in non-quantitative forms. It was only in the sixteenth century that Italian mathematicians began to formalize the odds associated with various games of chance. The invention of modern [[calculus]] had a positive impact on the formal study of randomness. In the 19th century a proof of the randomness of the digits of the number [[Pi]] was presented.

The early part of the twentieth century saw a rapid growth in the formal analysis of randomness, as various approaches for a mathematical foundation of probability were introduced. In the mid to late twentieth century ideas of [[algorithmic information theory]] introduced new dimensions to the field via the concept of [[algorithmic randomness]]. 

Although randomness had often been viewed as an obstacle and a nuisance for many centuries, in the twentieth century computer scientists began to realize that the ''deliberate'' introduction of randomness into computations can be an effective tool for designing better algorithms. In some cases such [[randomized algorithms]] outperform the best deterministic methods.

==Antiquity to the Middle Ages==
[[File:Fortuna or Fortune.jpg|thumb|Depiction of Roman Goddess [[Fortuna]] who determined fate, by [[Hans Sebald Beham|Hans Beham]], 1541]]

In ancient history, the concepts of chance and randomness were intertwined with that of fate. Pre-Christian people along the [[Mediterranean]] threw dice to determine fate, and this later evolved into games of chance.<ref>''What is Random?: Chance and Order in Mathematics and Life'', Edward J. Beltrami, 1999, Springer ISBN 0387987371 pp. 2-4</ref> There is also evidence of games of chance played by ancient Egyptians, Hindus and
Chinese, dating back to 2100 BC.<ref>''Encyclopedia of Leisure and Outdoor Recreation'', John Michael Jenkins, 2004 ISBN 0415252261 p. 194</ref> The Chinese used dice before the Europeans, and have a long history of playing games of chance.<ref>''Audacious Angles of China'', Elise Mccormick, 2007 ISBN 1406753327 p. 158</ref>

Over 3,000 years ago, the problems concerned with the tossing of several coins were considered in the [[Book of Changes|I Ching]], one of the oldest Chinese mathematical texts, that probably dates to 1150 BC. The two principal elements [[yin and yang]] were combined in the I Ching in various forms to produce ''Heads and Tails'' permutations of the type HH, TH, HT, etc. and the Chinese seem to have been aware of [[Pascal's triangle]] long before the Europeans formalized it in the 17th century.<ref>The Nature and Growth of Modern Mathematics'', Edna Ernestine Kramer, 1983 ISBN p. 313</ref> However, Western philosophy focused on the non-mathematical aspects of chance and randomness until the 16th century.

The development of the concept of chance throughout history has been very gradual. Historians have wondered why progress in the field of randomness was so slow, given that humans have encountered chance since antiquity. Deborah Bennett suggests that ordinary people face an inherent difficulty in understanding randomness, although the concept is often taken as being obvious and self-evident. She cites studies by [[Daniel Kahneman|Kahneman]] and [[Amos Tversky|Tversky]]; these concluded that statistical principles are not learned from everyday experience because people do not attend to the detail necessary to gain such knowledge.<ref>''Randomness'', Deborah J. Bennett, Harvard University Press, 1998. ISBN 0-674-10745-4 pp. 8-9 and 24</ref>

The Greek philosophers were the earliest Western thinkers to address chance and randomness. Around 400 BC, [[Democritus]] presented a view of the world as governed by the unambiguous laws of order and considered randomness as a subjective concept that only originated from the inability of humans to understand the nature of events. He used the example of two men who would send their servants to bring water at the same time to cause them to meet. The servants, unaware of the plan, would view the meeting as random.<ref>''Design and Analysis of Randomized Algorithms'', Juraj Hromkovi<U+010D>, 2005 ISBN 3540239499 p. 1</ref> 

[[Aristotle]] saw chance and necessity as opposite forces. He argued that nature had rich and constant patterns that could not be the result of chance alone, but that these patterns never displayed the machine-like uniformity of necessary determinism. He viewed randomness as a genuine and widespread part of the world, but as subordinate to necessity and order.<ref>''Aristotle's Physics: a Guided Study'', Joe Sachs, 1995 ISBN 0813521920 p. 70</ref>  Aristotle classified events into three types: ''certain'' events that happen necessarily; ''probable'' events that happen in most cases; and ''unknowable'' events that happen by pure chance. He considered the outcome of games of chance as unknowable.<ref>''A History of Probability and Statistics and Their Applications before 1750'', Anders Hald, 2003 ISBN 0471471291 p. 30</ref>

Around 300 BC [[Epicurus]] proposed the concept that randomness exists by itself, independent of human knowledge. He believed that in the atomic world, atoms would ''swerve'' at random along their paths, bringing about randomness at higher levels.<ref>''Epicurus: an Introduction'', John M. Rist, 1972 ISBN 0521084261 p. 52</ref>

[[File:HoteiMusashi.jpg|thumb|left|[[Hotei]], the deity of fortune observing a cock fight in a 16th-century Japanese print]]
For several centuries thereafter, the idea of chance continued to be intertwined with fate. Divination was practiced in many cultures, using diverse methods. The Chinese analyzed the cracks in turtle shells, while the Germans, who according to [[Tacitus]] had the highest regards for lots and omens, utilized strips of bark.<ref>''The Age of Chance'', Gerda Reith, 2000 ISBN 0415179971 p. 15</ref> In the [[Roman Empire]] chance was personified by the Goddess [[Fortuna]]. The Romans would partake in games of chance to simulate what Fortuna would have decided. In [[49 BC]], [[Julius Caesar]] allegedly decided on his fateful decision to cross the [[Rubicon]] after throwing dice.<ref>''What is Random?: Chance and Order in Mathematics and Life'', Edward J. Beltrami, 1999, Springer ISBN 0387987371 pp. 3-4</ref> 

Aristotle's classification of events into the three classes: ''certain'', ''probable'' and ''unknowable'' was adopted by Roman philosophers, but they had to reconcile it with deterministic [[Christian]] teachings in which even events unknowable to man were considered to be predetermined by God. About 960 Bishop Wibold of [[Cambrai]] correctly enumerated the 56 different outcomes (without permutations) of playing with three dice. No reference to playing cards has been found in Europe before 1350. The Church preached against card playing, and card games spread much more slowly than games based on dice.<ref>''A History of Probability and Statistics and Their Applications before 1750'', Anders Hald, 2003 ISBN 0471471291 pp. 29-36</ref>

Over the centuries, many Christian scholars wrestled with the conflict between the belief in [[free will]] and its implied randomness, and the idea that God knows everything that happens. Saints [[Augustine]] and [[Aquinas]] tried to reach an accommodation between foreknowledge and free will, but [[Martin Luther]] argued against randomness and took the position that God's omniscience renders human actions unavoidable and determined.<ref>''The Case for Humanism'', Lewis Vaughn, Austin Dacey, 2003 ISBN 0742513939 p. 81</ref> In the 13th century, [[Thomas Aquinas]] viewed randomness not as the result of a single cause, but of several causes coming together by chance. While he believed in the existence of randomness, he rejected it as an explanation of the end-directedness of nature, for he saw too many patterns in nature to have been obtained by chance.<ref>''The treatise on the divine nature: Summa theologiae I'', 1-13, by Saint Thomas Aquinas, Brian J. Shanley, 2006 ISBN 0872208052 p. 198</ref>

The Greeks and Romans had not noticed the magnitudes of the relative frequencies of the games of chance. For centuries chance was discussed in Europe with no mathematical foundation and it was only in the 16th century that Italian Mathematicians began to discuss the outcomes of games of chance as ratios.<ref>''A History of Probability and Statistics and Their Applications before 1750'', Anders Hald, 2003 ISBN 0471471291 pp. 30-4</ref><ref>''World of Scientific Discovery<U+200E>'', Kimberley A. McGrath and Bridget Traverspage, 1999 ISBN 0787627607 p. 893</ref><ref>''Randomness'', Deborah J. Bennett, Harvard University Press, 1998. ISBN 0-674-10745-4 p. 8</ref> In his 1565 ''Liber de Lude Aleae'' (published after his death) [[Gerolamo Cardano]] wrote one of the first formal tracts to analyze the odds of winning at various games.<ref>''A Dictionary of Scientists'', John Daintith, Derek Gjertsen, 1999 ISBN 0192800868 p. 88</ref>

==17th<U+2013>19th centuries==
[[File:Pascal Pajou Louvre RF2981.jpg|thumb|Statue of [[Blaise Pascal]], [[Louvre]]]]
Around 1620 [[Galileo]] wrote a paper called ''On a discovery concerning dice'' that used an early probabilistic model to address specific questions.<ref>''A History of Probability and Statistics and Their Applications before 1750'', Anders Hald, 2003 ISBN 0471471291 p. 41</ref> In 1654, prompted by [[Chevalier de M<U+00E9>r<U+00E9>]]'s interest in gambling, [[Blaise Pascal]] corresponded with [[Pierre de Fermat]], and much of the groundwork for probability theory was laid. [[Pascal's Wager]] was noted for its early use of the concept of [[infinity]], and the first formal use of decision theory. The work of Pascal and Fermat influenced [[Leibniz]]'s work on the [[infinitesimal calculus]], which in turn provided further momentum for the formal analysis of probability and randomness.

The first known suggestion for viewing randomness in terms of complexity was made by [[Leibniz]] in an obscure 17th-century document discovered after his death. Leibniz asked how one could know if a set of points on a piece of paper were selected at random (e.g. by splattering ink) or not. Given that for any set of finite points there is always a mathematical equation that can describe the points, (e.g. by [[Lagrangian interpolation]]) the question focuses on the way the points are expressed mathematically. Leibniz viewed the points as random if the function describing them had to be extremely complex. Three centuries later, the same concept was formalized as [[algorithmic randomness]] by [[Chaitin]] and [[A. N. Kolmogorov]] as the length of a computer program needed to describe a finite string as random.<ref>''Thinking about G<U+00F6>del and Turing'', Gregory J. Chaitin, 2007 ISBN 9812708960 p. 242</ref>

[[File:Vouet, Simon - The Fortune Teller.jpg|thumb|left|''The Fortune Teller'' by [[Vouet]], 1617]]
While the mathematical elite was making progress in understanding randomness during the 17th century, the public at large continued to rely on practices such as [[fortune telling]] in the hope of taming chance. Fortunes were told in a multitude of ways both in the Orient (where fortune-telling was later termed an addiction) and in Europe by gypsies and others.<ref>''Asia in the Making of Europe'', Volume 3, Donald Frederick Lach, Edwin J. Van Kley, 1998 ISBN 0226467694 p. 1660</ref><ref>''A History of the Gypsies of Eastern Europe and Russia'', David M. Crowe, 1996 ISBN 0312129467 p. 36</ref> English practices such as the reading of eggs dropped in a glass were exported to Puritan communities in North America.<ref>''Events that Changed America through the Seventeenth Century'', John E. Findling, Frank W. Thackeray, 2000 ISBN 0313290830 p. 168</ref>

The [[Frequency probability|frequency theory]] approach to probability was first developed by [[Robert Leslie Ellis|Robert Ellis]] and [[John Venn]] late in the 19th century. In the 1888 edition of his book ''The Logic of Chance'', [[John Venn]] wrote a chapter on "The conception of randomness" and presented a proof of the randomness of the digits of the number [[Pi]] by using them to construct a random walk in two dimensions.<ref>''Annotated Readings in the History of Statistics'', Herbert Aron David, 2001 ISBN 0387988440 p. 115. NB. The 1866 edition of Venn's  book (on Google Books) does not include this chapter.</ref> 

Since the time of [[Isac Newton|Newton]] to about 1890 it was generally believed that if one knows the initial state of a system with great accuracy, and if all the forces acting on the system can be formulated with equal accuracy, it would be possible, in principle, to make predictions of the state of the universe for an infinitely long time. The limits to such predictions in physical systems became clear as early as 1893 when [[Henri Poincar<U+00E9>]] showed that in the [[three body problem]] in astronomy small changes to the initial state could result in large changes in trajectories during the numerical integration of the equations.<ref>''On Limited Predictability'', A. Wiin-Nielsen, 1999 ISBN 8773041858 p. 3</ref>

During the 19th century, as probability theory was formalized and better understood, the attitude towards "randomness as nuisance" began to be be questioned. [[Goethe]] wrote:

<blockquote>
The tissue of the world
is built from necessities and randomness;
the intellect of men places itself between both
and can control them;
it considers the necessity 
and the reason of its existence;
it knows how randomness can be
managed, controlled, and used.
</blockquote>

The words of Goethe proved prophetic, when in the 20th century [[randomized algorithms]] were discovered as powerful tools.<ref>''Design and Analysis of Randomized Algorithms'', Juraj Hromkovi<U+010D>, 2005 ISBN 3540239499 p. 4</ref>

==20th century==
[[File:Antony Gormley Quantum Cloud 2000.jpg|thumb|[[Antony Gormley]]'s ''[[Quantum Cloud]]'' sculpture in [[London]] was designed by a computer using a [[random walk]] algorithm.]]
During the 20th century, the five main [[Probability interpretations|interpretations of probability]] theory, namely ''classical'', ''logical'', ''frequency'', ''propensity'' and ''subjective'' became better understood, were discussed, compared and contrasted.<ref>[http://plato.stanford.edu/entries/probability-interpret/ ''Stanford Encyclopedia of Philosophy'']</ref> A significant number of application areas were developed in this century, from finance to physics. In 1900 [[Louis Bachelier]] applied [[Brownian motion]] to evaluate [[stock options]], effectively launching the fields of [[financial mathematics]] and [[stochastic processes]].

Also early in the 20th century, [[Richard von Mises]] made considerable progress beyond [[John Venn|Venn]] in studying the frequency theory in terms of what he called ''the collective'', i.e. a sample. Von Mises regarded the randomness of a collective as an empirical law, established by experience. He related the "disorder" or randomness of a collective to the lack of success of attempted gambling systems. This approach led him to suggest a definition of randomness which was later refined and made mathematically rigorous by [[Alonso Church]] by using [[computable function]]s in 1940.<ref>''Companion Encyclopedia of the History and Philosophy'' Volume 2, Ivor Grattan-Guinness 0801873975 p. 1412</ref> [[Richard von Mises]] likened the principle of the [[impossibility of a gambling system]] to the principle of the [[conservation of energy]], a law that cannot be proven, but has held true in repeated experiments.<ref>''The Philosophy of Karl Popper'', Herbert Keuth ISBN 0521548306 p. 171</ref>

In his 1940 paper "On the concept of random sequence", [[Alonzo Church]] suggested that the functions used for place settings in the formalism of von Mises be [[recursive function]]s rather than arbitrary functions of the initial segments of the sequence, appealing to the [[Church<U+2013>Turing thesis]] on effectiveness.<ref>[[Alonzo Church]], "On the concept of random sequence," Bull. Amer. Math. Soc., 46 (1940), 254<U+2013>260</ref><ref>J. Alberto Coffa, "Randomness and knowledge," in ''PSA 1972: proceedings of the 1972 Biennial Meeting Philosophy of Science Association'', Volume 20, Springer, 1974 ISBN 9027704082 p. 106</ref>

[[File:Cafe Central Wenen.jpg|thumb|left|[[Caf<U+00E9> Central]], one of the early meeting places of the [[Vienna circle]]]]
By the early 1940s the frequency theory was well accepted within the [[Vienna circle]], but in the 1950s [[Karl Popper]] proposed the [[Propensity probability|propensity theory]].<ref>Karl Popper, 1957, "The propensity interpretation of the calculus of probability and the quantum theory<U+201D>, in S. K<U+00F6>rner (ed.), ''The Colston Papers'', 9: 65<U+2013>70.</ref><ref>Karl Popper, 1959, "The propensity interpretation of probability", ''British Journal of the Philosophy of Science'', 10: 25<U+2013>42.</ref> Given that the frequency approach can not deal with "a single toss" of a coin, and can only address large ensembles or collectives, the single-case probabilities were treated as propensities or chances. The concept of propensity was also driven by the desire to handle single-case probability settings in quantum mechanics, e.g. the probability of decay of a specific atom at a specific moment. In more general terms, the frequency approach can not deal with the probability of the death of a ''specific person'' given that the death can not be repeated multiple times for that person. [[Karl Popper]] echoed the same sentiment as Aristotle in viewing randomness as subordinate to order when he wrote that "the concept of chance is not apposed to the concept of law" in nature, provided one considers the laws of chance.<ref>Karl Popper, ''The Logic of Scientific Discovery'' p. 206</ref><ref>''The Philosophy of Karl Popper'', Herbert Keuth ISBN 0521548306 p. 170</ref>

[[Claude Shannon]]'s development of [[Information theory]] in 1948 gave rise to the ''[[Entropy (information theory)|entropy]] view'' of randomness. In this view, randomness is the opposite of ''determinism'' in a [[stochastic process]]. Hence if a stochastic system has entropy zero it has no randomness and any increase in entropy increases randomness.<ref>''Single Orbit Dynamics'', Benjamin Weiss 1999 ISBN 0821804146 p. 83</ref>

The application of [[Random walk hypothesis]] in [[financial theory]] was first proposed by [[Maurice Kendall]] in 1953.<ref>Kendall, M. G. (1953). "The analysis of economic time-series-part I: prices", ''Journal of the Royal Statistical Society''. A (General) 116 (1): 11<U+2013>34. [http://www.jstor.org/stable/2980947]</ref> It was later promoted by [[Eugene Fama]] and [[Burton Malkiel]]. 

Random strings were first studied by in the 1960s by [[A. N. Kolmogorov]], [[Chaitin]] and [[Martin-L<U+00F6>f]].<ref>''Information and Randomness: an Algorithmic Perspective'', Cristian Calude, 2002 ISBN 3540434666 p. 145</ref> The [[algorithmic randomness]] of a string was defined as the minimum size of a program (e.g. in bits) executed on a universal computer that yields the string.

During the 20th century limits in dealing with randomness were better understood. The best-known example of both theoretical and operational limits on predictability is weather forecasting, simply because models have been used in the field since the 1950s. Predictions of weather and climate are necessarily uncertain. Observations of weather and climate are uncertain and incomplete, and the models into which the data are fed are uncertain.<ref>''Predictability of Weather and Climate'', Tim Palmer, Renate Hagedorn, 2006 ISBN 0521848822 p. 1</ref> In 1961, [[Edward Lorenz]] noticed that a very small change to the initial data submitted to a computer program for weather simulation could result in a completely different weather scenario. This later became known as the [[Butterfly effect]], often paraphrased as the question: "''Does the flap of a butterfly<U+2019>s wings in Brazil set off a tornado in Texas?''".<ref>''Storm Warning: The Story of a Killer Tornado'', Nancy Mathis, 2007 ISBN 0-7432-8053-2 p. x</ref> A key example of serious practical limits on predictability is in geology, where the ability to [[Earthquake prediction|predict earthquakes]] either on an individual or on a statistical basis remains a remote prospect.<ref>L. Knopoff, "Earthquake prediction: the scientific challenge", ''Proceedings of the National Academy of Sciences'', 1999 ISBN 0309058376 p. 3720</ref>

In the late 1970s and early 1980s computer scientist began to realize that the ''deliberate'' introduction of randomness into computations can be an effective tool for designing better algorithms. In some cases such [[randomized algorithms]] outperform the best deterministic methods.<ref>''Design and Analysis of Randomized Algorithms'', Juraj Hromkovi<U+010D> 2005 ISBN 3540239499 p. 4</ref>

==Notes==
{{Reflist|2}}

[[Category:Randomness]]
